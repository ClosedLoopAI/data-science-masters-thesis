<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Critique of an Annual Report Data Product | fastpages</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Critique of an Annual Report Data Product" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Communicating about data is a special art…" />
<meta property="og:description" content="Communicating about data is a special art…" />
<link rel="canonical" href="https://closedloopai.github.io/portfolio/2018/11/07/DataProductCritique.html" />
<meta property="og:url" content="https://closedloopai.github.io/portfolio/2018/11/07/DataProductCritique.html" />
<meta property="og:site_name" content="fastpages" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-11-07T00:00:00-06:00" />
<script type="application/ld+json">
{"url":"https://closedloopai.github.io/portfolio/2018/11/07/DataProductCritique.html","@type":"BlogPosting","headline":"Critique of an Annual Report Data Product","dateModified":"2018-11-07T00:00:00-06:00","datePublished":"2018-11-07T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://closedloopai.github.io/portfolio/2018/11/07/DataProductCritique.html"},"description":"Communicating about data is a special art…","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/portfolio/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://closedloopai.github.io/portfolio/feed.xml" title="fastpages" /><link rel="shortcut icon" type="image/x-icon" href="/portfolio/images/favicon.ico"><link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/portfolio/">fastpages</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/portfolio/about/">About Me</a><a class="page-link" href="/portfolio/search/">Search</a><a class="page-link" href="/portfolio/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Critique of an Annual Report Data Product</h1><p class="page-description">Communicating about data is a special art...</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2018-11-07T00:00:00-06:00" itemprop="datePublished">
        Nov 7, 2018
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      13 min read
    
</span></p>

    

    
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#5.1-TEST-DATASET">5.1 TEST DATASET </a></li>
<li class="toc-entry toc-h2"><a href="#5.2-END-TO-END-INFERENCE-PROCESS">5.2 END-TO-END INFERENCE PROCESS </a>
<ul>
<li class="toc-entry toc-h3"><a href="#5.2.1-Stage-III-Filter-(mod3a)">5.2.1 Stage III Filter (mod3a) </a></li>
<li class="toc-entry toc-h3"><a href="#5.2.2-Stage-II-Filter-(mod2)">5.2.2 Stage II Filter (mod2) </a></li>
<li class="toc-entry toc-h3"><a href="#5.2.3-Stage-I-Filter-(mod1)">5.2.3 Stage I Filter (mod1) </a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#6-CONCLUSIONS-&-RECOMMENDATIONS">6 CONCLUSIONS &amp; RECOMMENDATIONS </a></li>
<li class="toc-entry toc-h1"><a href="#7-SUMMARY">7 SUMMARY </a></li>
<li class="toc-entry toc-h1"><a href="#8-FURTHER-EXPERIMENTATION">8 FURTHER EXPERIMENTATION </a></li>
<li class="toc-entry toc-h1"><a href="#REFERENCES">REFERENCES </a></li>
<li class="toc-entry toc-h1"><a href="#APPENDICES">APPENDICES </a>
<ul>
<li class="toc-entry toc-h2"><a href="#Appendix-A:-Filesystem-Layout">Appendix A: Filesystem Layout </a></li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2018-11-07-DataProductCritique.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="/AnnualReport2018.pdf">AnnualReport2018.pdf</a></p>
<p>This project suggests an automatic way for post-flight analysts to undertake classification of flight profiles into useful versus non-useful classes. Instead of using the traditional algorithms for time-series classification, this work makes use of a relatively new approach: Before classifying, first transform a time-series into an image. This allows for the application of a well-developed set of algorithms from the area of computer vision. In this project, we perform a comparison of a number of these transformation techniques in terms of their associated image classification performance. We apply each transformation technique to the time-series dataset in turn, train a Convolutional Neural Network to do classification, and record the performance. Then we select the most performant transformation technique (a simple line plot that got a 100% F1-score) and use it in the rest of the analysis pipeline.</p>
<p>The pipeline consists of three models. The first model classifies flight profiles into developed (useful) and non-developed (non-useful) profiles. The second model performs multi-label classification on the developed profiles from the first model. The labels reflect whether a profile has canonical climb/cruise/descent segments. The last model classifies flight profiles with canonical cruise segments into classes that have extended cruises (useful) and shorter cruises (non-useful).</p>
<p>Next, we prepare a significant unlabeled test dataset, consisting of data points that have never been seen by any of the models. We construct an end-to-end analytic inference process to simulate a production system, apply it to the test dataset, and obtain impressive results. Finally, we make recommendations to post-flight and other interested analysts.</p>
<p><em>Keywords</em>: Deep learning, Time series, Image Classification, CNN, RNN, Flight path</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/portfolio/images/copied_from_nb/../images/fig5-1.png" alt="Figure 5.1 Analytics pipeline for training" title="Figure 5.1 Analytics pipeline for training"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/portfolio/images/copied_from_nb/../images/fig5-2.png" alt="Figure 5.2 Analytics pipeline for testing/inference" title="Figure 5.2 Analytics pipeline for testing/inference"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="5.1-TEST-DATASET">
<a class="anchor" href="#5.1-TEST-DATASET" aria-hidden="true"><span class="octicon octicon-link"></span></a>5.1 TEST DATASET<a class="anchor-link" href="#5.1-TEST-DATASET"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The dataset for testing/inference was prepared in the same way as the dataset for training. Note that the datapoints are not labeled for inference. The size of the raw dataset is 582 flights (Figure 5.2). After preparation, the datapoints reduced to 514. The same notebook was used to prepare the test dataset (with a few configuration adjustments). The notebook is:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="https://nbviewer.jupyter.org/github/kobus78/dashlink/blob/master/10_mat2csv-2.ipynb">10_mat2csv-2.ipynb</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="5.2-END-TO-END-INFERENCE-PROCESS">
<a class="anchor" href="#5.2-END-TO-END-INFERENCE-PROCESS" aria-hidden="true"><span class="octicon octicon-link"></span></a>5.2 END-TO-END INFERENCE PROCESS<a class="anchor-link" href="#5.2-END-TO-END-INFERENCE-PROCESS"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>No training happens during inference. Instead, the output of each filter is passed to the input of the next filter in the pipeline. The output of the final filter (Stage I with mod1) is a set of developed profiles that have extended canonical cruise segments, ideal for the analyses concerned with in this paper.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Each model behaves as a filter that <em>lets through</em> all the useful profiles for the particular stage. This means</p>
<ul>
<li>Taking prepared raw data, <em>mod3a</em> lets through all profiles that have been developed</li>
<li>Taking mod3a’s output, <em>mod2</em> lets through all profiles with canonical cruise segments</li>
<li>Taking mod2’s output, <em>mod1</em> lets through all profiles with extended cruise segments</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="5.2.1-Stage-III-Filter-(mod3a)">
<a class="anchor" href="#5.2.1-Stage-III-Filter-(mod3a)" aria-hidden="true"><span class="octicon octicon-link"></span></a>5.2.1 Stage III Filter (mod3a)<a class="anchor-link" href="#5.2.1-Stage-III-Filter-(mod3a)"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The initial filter (mod3a) was presented with 514 test data points from the folder:</p>
<ul>
<li>dashlink<ul>
<li>Test<ul>
<li>png3 [contains all the png files]</li>
</ul>
</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The code for the Stage III filter is present in Python notebook:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="https://nbviewer.jupyter.org/github/kobus78/dashlink/blob/master/40_inf3.ipynb">40_inf3.ipynb</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>It is interesting to scroll through the two listings towards the end of the notebook. The first one shows the images for all the useful profiles, the second one shows the same for all the non-useful profiles. These lists of images give an idea of the classification performance.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="5.2.2-Stage-II-Filter-(mod2)">
<a class="anchor" href="#5.2.2-Stage-II-Filter-(mod2)" aria-hidden="true"><span class="octicon octicon-link"></span></a>5.2.2 Stage II Filter (mod2)<a class="anchor-link" href="#5.2.2-Stage-II-Filter-(mod2)"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The second filter (mod2) was presented with 481 test data points from the folder:</p>
<ul>
<li>dashlink<ul>
<li>Test<ul>
<li>png2 [contains all the png files]</li>
</ul>
</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The code for the Stage II filter is present in Python notebook:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="https://nbviewer.jupyter.org/github/kobus78/dashlink/blob/master/40_inf2.ipynb">40_inf2.ipynb</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Once again, the useful/non-useful profiles may be inspected.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="5.2.3-Stage-I-Filter-(mod1)">
<a class="anchor" href="#5.2.3-Stage-I-Filter-(mod1)" aria-hidden="true"><span class="octicon octicon-link"></span></a>5.2.3 Stage I Filter (mod1)<a class="anchor-link" href="#5.2.3-Stage-I-Filter-(mod1)"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The final filter (mod1) was presented with 316 test data points from the folder:</p>
<ul>
<li>dashlink<ul>
<li>Test<ul>
<li>png1 [contains all the png files]</li>
</ul>
</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The code for the Stage I filter is present in Python notebook:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="https://nbviewer.jupyter.org/github/kobus78/dashlink/blob/master/40_inf1.ipynb">40_inf1.ipynb</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Again, the useful/non-useful profiles may be inspected. This being the final results of the complete analytics pipeline, we see that the output consists of 26 useful profiles. They are all developed and have canonical and extended cruise segments, as we set out to find. There were 290 non-useful, or less useful, profiles in the final stage. It is immediately obvious how much time might be saved by using the analytics pipeline rather than having to inspect all 582 raw profiles manually to arrive at the 26 useful ones. There will be, no doubt, some miss-classifications too. This is unavoidable as the error rate of each stage contributes towards the pipeline’s overall error rate.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this section, we looked at how the analytics pipeline developed in this paper might be used in a production setting to provide value for analysts. A test dataset was prepared. None of its data has ever been seen by any of the models during training. This is the normal situation in a production system. By looking at the classification results (in the form of images) from the notebooks, it is interesting to see to what extent the use of this pipeline might be helpful for analysts.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="6-CONCLUSIONS-&amp;-RECOMMENDATIONS">
<a class="anchor" href="#6-CONCLUSIONS-&amp;-RECOMMENDATIONS" aria-hidden="true"><span class="octicon octicon-link"></span></a>6 CONCLUSIONS &amp; RECOMMENDATIONS<a class="anchor-link" href="#6-CONCLUSIONS-&amp;-RECOMMENDATIONS"> </a>
</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We have demonstrated how flight profile time-series can be turned into images for more effective classification. Then we identified the best technique to transform a profile time-series into an image for use by the classification process. Using transfer learning, we showed how quickly a deep learning model could be trained. We conclude that the need for hand classification of flight profiles has been reduced greatly leading to significant time savings potential for post-flight analysts. Making use of a publicly available rich flight data set we hope this work will encourage ordinary data scientists (which do not have access to company flight data) as well as post-flight analysts (that do have access) to undertake studies in this important area.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We recommend that interested analysts take this work as a starting point and adapt it to suit their needs. This may even involve changing the technology stack, for example, making use of other deep learning libraries and a different programming language. Here we used the fastai Python library (built on top of PyTorch) and the Python language. There are a number of other useful technology environments, e.g. Java, TensorFlow, Julia, and MATLAB.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We suggest that analysts need not shy away from the use of deep learning for post-flight analysis. The use of transfer learning makes the training of deep learning models very tractable. In our case, transfer learning was based on the ImageNet model which was trained on over 14 million images to classify them into more than 20,000 categories. There are many cloud providers offering the use of GPUs (Graphical Processing Units), ideal for the training process. GPUs are not necessary for inference. Even without access to a GPU, the training process is still tractable on an ordinary laptop. This is the beauty of transfer learning.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="7-SUMMARY">
<a class="anchor" href="#7-SUMMARY" aria-hidden="true"><span class="octicon octicon-link"></span></a>7 SUMMARY<a class="anchor-link" href="#7-SUMMARY"> </a>
</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this work, we:</p>
<ul>
<li>Performed a comparison of a number of transformation techniques in terms of their associated image classification performance. We applied each transformation technique to the cleaned time-series dataset in turn, trained a CNN to do classification (using supervised learning), and recorded the performance. Then we selected the most performant transformation technique and used it in the rest of the analysis pipeline. The following transformation techniques were considered:<ul>
<li>Altitude line plots transformed into an image</li>
<li>Altitude area plots transformed into an image</li>
<li>Gramian Angular Summation Field (GASF)</li>
<li>Gramian Angular Difference Field (GADF)</li>
<li>Markov Transition Field (MTF)</li>
<li>Recurrence Plot (RP)</li>
</ul>
</li>
<li>Trained a model to classify flight profiles into developed (useful) and non-developed (non-useful) profiles. We also considered the use of anomaly detection by means of an autoencoder (instead of a classification algorithm) due to the significant class imbalance and concluded that the autoencoder did not work as well as the classifier.</li>
<li>Trained a model to do multi-label classification of developed profiles. The labels reflected whether a profile had a canonical climb/cruise/descent segment.</li>
<li>Trained a model to classify flight profiles with canonical cruise segments (regardless of the properties of climb or descent segments) into profiles that have extended cruises (useful) and shorter cruises (non-useful).</li>
<li>Prepared a significant test dataset, consisting of datapoints that have never been seen by any of the models and have not been labeled. We constructed an end-to-end analytic inference process to simulate a production system and applied it to the test dataset. Finally, we made recommendations to post-flight and other interested analysts.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="8-FURTHER-EXPERIMENTATION">
<a class="anchor" href="#8-FURTHER-EXPERIMENTATION" aria-hidden="true"><span class="octicon octicon-link"></span></a>8 FURTHER EXPERIMENTATION<a class="anchor-link" href="#8-FURTHER-EXPERIMENTATION"> </a>
</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>There are a good number of hyper-parameters that may be adjusted leading to further experiments, for example:</p>
<ul>
<li>Fraction of train data dedicated for validation (20% here)</li>
<li>The batch size during training (32 for mod3a and 8 for mod2 and mod1)</li>
<li>Images were resized to 128 x 128. A technique that holds promise is to first down-sample images drastically, say to 32 x 32. Then, after training, transfer learning is used while progressively up-sampling again.</li>
<li>Learning rates. This is arguably the most influential hyper-parameter during training. It may be worthwhile to adjust the used learning rates, (both for the frozen learning rate, <em>lrf</em>, as well as the unfrozen learning rate, <em>lru</em>.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We used data from a single airplane in this study. It may be worthwhile to analyze the datasets available and put together a dataset that represents a number of different aircraft. Care should be taken, however, because the identity and model of aircraft are deliberately omitted. This means the analyst might end up with data from a 747 being mixed with that of a Lear Jet, or even a Cessna, probably not all that useful.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>No <em>data augmentation</em> was used during training. It should be possible to generate more data by flipping images horizontally for mod3a and mod1. This will not work for mod2 due to the implied change of the segment types. A small amount of zooming might also be tried.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The CNN architecture used throughout was ResNet-50. We believe ResNet is the current state-of-the-art but there are other promising architectures, i.e. the Inception network. If an experimenter is challenged in terms of compute-resources, the ResNet-50 can be scaled down, e.g. to ResNet-34 or ResNet-18.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the case of the autoencoder, a <em>linear</em> autoencoder was used. Better results might be obtained if a <em>convolutional</em> autoencoder is used instead. The code for a convolutional autoencoder is included in the anomaly detection notebook.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Finally, we need to mention that, although this paper focused on only using the altitude time-series of a flight, there are many more variables to explore. As mentioned, our data source makes 186 variables available. A few simple explorations are undertaken in the Python notebook:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="https://nbviewer.jupyter.org/github/kobus78/dashlink/blob/master/20_eda1.ipynb">20_eda1.ipynb</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Something interesting that might be tried is to combine a number of normalized variables (to allow for a single vertical scale) on a single line plot, each in a different color. Deep learning may then be used to train for the identification of normal versus anomalous situations.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="REFERENCES">
<a class="anchor" href="#REFERENCES" aria-hidden="true"><span class="octicon octicon-link"></span></a>REFERENCES<a class="anchor-link" href="#REFERENCES"> </a>
</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Autoencoders. (n.d.). Retrieved September 28, 2019, from <a href="http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/">http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Automatic dependent surveillance – broadcast. (n.d.). In Wikipedia. Retrieved September 19, 2019, from   <a href="http://en.wikipedia.org/wiki/https://en.wikipedia.org/wiki/Automatic_dependent_surveillance_%E2%80%93_broadcast">http://en.wikipedia.org/wiki/https://en.wikipedia.org/wiki/Automatic_dependent_surveillance_%E2%80%93_broadcast</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Bagnall, A., Lines, J., Bostrom, A., Large, J., &amp; Keogh, E. (2017). The great time series classification bake off: a review and experimental evaluation of recent algorithmic advances. Data Mining and Knowledge Discovery, 31(3), 606-660.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Bagnall, A., Lines, J., Hills, J., &amp; Bostrom, A. (2016). Time-series classification with COTE: The collective of transformation-based ensembles. International Conference on Data Engineering, pp 1548-1549.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Culurciell, E. (2018). The fall of RNN / LSTM. [Weblog]. Retrieved from 
<a href="https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0">https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Esling, P., &amp; Agon, C. (2012). Time-series data mining. ACM Computing Surveys, 45(1), 12:1-12:34.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Fawaz, H. I., Forestier, G., Weber, J., Idoumghar, L., &amp; Muller, P. (2019). Deep learning for time series classification: a review. Data Mining and Knowledge Discovery, 33, 917. <a href="https://doi.org/10.1007/s10618-019-00619-1">https://doi.org/10.1007/s10618-019-00619-1</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>F1 score. (n.d.). In Wikipedia. Retrieved September 27, 2019, from <a href="https://en.wikipedia.org/wiki/F1_score">https://en.wikipedia.org/wiki/F1_score</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>He, K., Zhang, X., Ren, S., &amp; Sun, J. (2016). Deep Residual Learning for Image Recognition. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, 2016, pp. 770-778. doi: 10.1109/CVPR.2016.90</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Hatami, N., Gavet, Y., &amp; Debayle, J. (2017). Classification of time-series images using deep convolutional neural networks. International Conference on Machine Vision.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Hüsken, M., &amp; Stagge, P. (2003). Recurrent neural networks for time series classification. Neurocomputing, 50, 223-235. Retrieved from <a href="https://www.sciencedirect.com/science/article/pii/S0925231201007068?via=ihub">https://www.sciencedirect.com/science/article/pii/S0925231201007068?via=ihub</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Karpathy, A. (2015). The Unreasonable Effectiveness of Recurrent Neural Networks. [Weblog]. Retrieved from <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">http://karpathy.github.io/2015/05/21/rnn-effectiveness/</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Krizhevsky, A., Sutskever, I., &amp; Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. In NIPS, 2012.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Krizhevsky, A., Sutskever, I., &amp; Hinton, G. E. (2017). ImageNet classification with deep convolutional neural networks. Commun. ACM, 60(6), 84-90. DOI: <a href="https://doi.org/10.1145/3065386">https://doi.org/10.1145/3065386</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Lines, J., Taylor, S., &amp; Bagnall, A. (2016). HIVE-COTE: The hierarchical vote collective of transformation based ensembles for time series classification. IEEE International Conference on Data Mining, pp 1041-1046.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Lines, J., Taylor, S., &amp; Bagnall, A. (2018). Time series classification with HIVE-COTE: The hierarchical vote collective of transformation-based ensembles. ACM Transactions on Knowledge Discovery from Data, 12(5), 52:1-52:35.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Rabinowitz, J. (2017). This Is How Flight Tracking Sites Work. Retrieved from <a href="https://thepointsguy.com/2017/09/how-flight-tracking-sites-work/">https://thepointsguy.com/2017/09/how-flight-tracking-sites-work/</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Skalski, P. (2019). Gentle Dive into Math Behind Convolutional Neural Networks. [Weblog]. Retrieved from <a href="https://towardsdatascience.com/gentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9">https://towardsdatascience.com/gentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Sun, Z., Di, L. &amp; Fang, H. (2019). Using long short-term memory recurrent neural network in land cover classification on Landsat and Cropland data layer time series. International Journal of Remote Sensing, 40(2), 593-614. DOI: 10.1080/01431161.2018.1516313. Retrieved from <a href="https://www.tandfonline.com/doi/abs/10.1080/01431161.2018.1516313">https://www.tandfonline.com/doi/abs/10.1080/01431161.2018.1516313</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Tripathy, R. K., &amp; Acharya, U. R. (2018). Use of features from RR-time series and EEG signals for automated classification of sleep stages in deep neural network framework. Biocybernetics and Biomedical Engineering, 38, 890-902.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Tsang, S. (2018). Review: ResNet — Winner of ILSVRC 2015 (Image Classification, Localization, Detection). [Weblog]. Retrieved from <a href="https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8">https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Wang, Z., Yan, W., &amp; Oates, T. (2017). Time series classification from scratch with deep neural networks: A strong baseline. 2017 International Joint Conference on Neural Networks (IJCNN), 1578-1585.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Wang, Z., &amp; Oates, T. (2015a). Encoding Time Series as Images for Visual Inspection and Classification Using Tiled Convolutional Neural Networks. Trajectory-Based Behavior Analytics: Papers from the 2015 AAAI Workshop. Retrieved from <a href="https://aaai.org/ocs/index.php/WS/AAAIW15/paper/viewFile/10179/10251">https://aaai.org/ocs/index.php/WS/AAAIW15/paper/viewFile/10179/10251</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Wang, Z., &amp; Oates, T. (2015b). Imaging time-series to improve classification and imputation. International Conference on Artificial Intelligence, pp 3939-3945.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Wang, Z., &amp; Oates, T. (2015c). Spatially Encoding Temporal Correlations to Classify Temporal Data Using Convolutional Neural Networks. ArXiv, abs/1509.07481.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., … Bengio, Y. (2015). Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. Proceedings of the 32nd International Conference on Machine Learning, in PMLR, 37, 2048-2057.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Yang, Q., Wu, X. (2006). 10 challenging problems in data mining research. Information Technology &amp; Decision Making, 05(04), 597-604.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="APPENDICES">
<a class="anchor" href="#APPENDICES" aria-hidden="true"><span class="octicon octicon-link"></span></a>APPENDICES<a class="anchor-link" href="#APPENDICES"> </a>
</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Appendix-A:-Filesystem-Layout">
<a class="anchor" href="#Appendix-A:-Filesystem-Layout" aria-hidden="true"><span class="octicon octicon-link"></span></a>Appendix A: Filesystem Layout<a class="anchor-link" href="#Appendix-A:-Filesystem-Layout"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>dashlink<ul>
<li>Train<ul>
<li>1min<ul>
<li>.csv files</li>
</ul>
</li>
<li>png3<ul>
<li>non</li>
<li>typ</li>
</ul>
</li>
<li>png3a<ul>
<li>non</li>
<li>typ</li>
<li>export.pkl file</li>
</ul>
</li>
<li>png3b<ul>
<li>non</li>
<li>typ</li>
<li>export.pkl file</li>
</ul>
</li>
<li>png3c<ul>
<li>non</li>
<li>typ</li>
<li>export.pkl file</li>
</ul>
</li>
<li>png3d<ul>
<li>non</li>
<li>typ</li>
<li>export.pkl file</li>
</ul>
</li>
<li>png3e<ul>
<li>non</li>
<li>typ</li>
<li>export.pkl file</li>
</ul>
</li>
<li>png3f<ul>
<li>non</li>
<li>typ</li>
<li>export.pkl file</li>
</ul>
</li>
<li>png2<ul>
<li>.png files</li>
<li>canonical-segments.csv file</li>
<li>export.pkl file</li>
</ul>
</li>
<li>png1<ul>
<li>non</li>
<li>typ</li>
<li>export.pkl file</li>
</ul>
</li>
</ul>
</li>
<li>Test<ul>
<li>png3<ul>
<li>non</li>
<li>typ</li>
</ul>
</li>
<li>src3<ul>
<li>.png files</li>
</ul>
</li>
<li>png2<ul>
<li>non</li>
<li>typ</li>
</ul>
</li>
<li>png1<ul>
<li>non</li>
<li>typ</li>
</ul>
</li>
</ul>
</li>
<li>10_mat2csv.ipynb</li>
<li>10_mat2csv-2.ipynb</li>
<li>10_csv2png-3.ipynb</li>
<li>20_eda1.ipynb</li>
<li>30_mod3a.ipynb</li>
<li>30_mod3b.ipynb</li>
<li>30_mod3c.ipynb</li>
<li>30_mod3d.ipynb</li>
<li>30_mod3e.ipynb</li>
<li>30_mod3f.ipynb</li>
<li>30_mod4-2.ipynb</li>
<li>30_mod2.ipynb</li>
<li>30_mod1.ipynb</li>
<li>40_inf3.ipynb</li>
<li>40_inf2.ipynb</li>
<li>40_inf1.ipynb</li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>NOTE: The Python notebook files (.ipynb) have a prefix that indicates the following:</p>
<ul>
<li>10_ for data preparation</li>
<li>20_ for exploratory data analysis</li>
<li>30_ for modeling and training</li>
<li>40_ for inference and testing</li>
</ul>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="ClosedLoopAI/portfolio"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/portfolio/2018/11/07/DataProductCritique.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/portfolio/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/portfolio/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/portfolio/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>An easy to use blogging platform with support for Jupyter Notebooks.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/fastai" title="fastai"><svg class="svg-icon grey"><use xlink:href="/portfolio/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/fastdotai" title="fastdotai"><svg class="svg-icon grey"><use xlink:href="/portfolio/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
