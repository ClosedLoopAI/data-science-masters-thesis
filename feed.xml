<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="/portfolio/feed.xml" rel="self" type="application/atom+xml" /><link href="/portfolio/" rel="alternate" type="text/html" /><updated>2020-09-25T14:35:08-05:00</updated><id>/portfolio/feed.xml</id><title type="html">Kobus Esterhuysen</title><subtitle>An easy to use blogging platform with support for Jupyter Notebooks.</subtitle><entry><title type="html">Time-series Classification (Deep Learning to Classify Flight Profiles)</title><link href="/portfolio/airline%20industry/deep%20learning/time%20series/image%20classification/cnn/rnn/python/fastai/matlab/2019/09/15/TimeSeriesClassification.html" rel="alternate" type="text/html" title="Time-series Classification (Deep Learning to Classify Flight Profiles)" /><published>2019-09-15T00:00:00-05:00</published><updated>2019-09-15T00:00:00-05:00</updated><id>/portfolio/airline%20industry/deep%20learning/time%20series/image%20classification/cnn/rnn/python/fastai/matlab/2019/09/15/TimeSeriesClassification</id><content type="html" xml:base="/portfolio/airline%20industry/deep%20learning/time%20series/image%20classification/cnn/rnn/python/fastai/matlab/2019/09/15/TimeSeriesClassification.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2019-09-15-TimeSeriesClassification.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;This project suggests an automatic way for post-flight analysts to undertake classification of flight profiles into useful versus non-useful classes. Instead of using the traditional algorithms for time-series classification, this work makes use of a relatively new approach: Before classifying, first transform a time-series into an image. This allows for the application of a well-developed set of algorithms from the area of computer vision. In this project, we perform a comparison of a number of these transformation techniques in terms of their associated image classification performance. We apply each transformation technique to the time-series dataset in turn, train a Convolutional Neural Network to do classification, and record the performance. Then we select the most performant transformation technique (a simple line plot that got a 100% F1-score) and use it in the rest of the analysis pipeline.&lt;/p&gt;
&lt;p&gt;The pipeline consists of three models. The first model classifies flight profiles into developed (useful) and non-developed (non-useful) profiles. The second model performs multi-label classification on the developed profiles from the first model. The labels reflect whether a profile has canonical climb/cruise/descent segments. The last model classifies flight profiles with canonical cruise segments into classes that have extended cruises (useful) and shorter cruises (non-useful).&lt;/p&gt;
&lt;p&gt;Next, we prepare a significant unlabeled test dataset, consisting of data points that have never been seen by any of the models. We construct an end-to-end analytic inference process to simulate a production system, apply it to the test dataset, and obtain impressive results. Finally, we make recommendations to post-flight and other interested analysts.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Keywords&lt;/em&gt;: Deep learning, Time series, Image Classification, CNN, RNN, Flight path&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;1-BACKGROUND&quot;&gt;1 BACKGROUND&lt;a class=&quot;anchor-link&quot; href=&quot;#1-BACKGROUND&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;One of the important operational characteristics of a commercial airliner is its flight path. This term needs qualification though. The &lt;em&gt;lateral&lt;/em&gt; flight path is the track projected onto a flat earth from above. To visualize the lateral flight path one can plot longitude versus latitude as flight time progresses. The &lt;em&gt;vertical&lt;/em&gt; flight path, on the other hand, is the altitude profile (viewed from the side). This may be visualized by plotting altitude versus flight time. This project will focus on the vertical flight path. When we use the term flight path or flight profile in the rest of the document, we will always refer to the &lt;em&gt;vertical&lt;/em&gt; flight path.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;During normal operation, an airliner has a predictable flight path. After &lt;em&gt;takeoff&lt;/em&gt;, it climbs as quickly as possible (during the phase known as &lt;em&gt;climb&lt;/em&gt;) until it reaches a point called the &lt;em&gt;top of climb&lt;/em&gt;. The pilot then levels off and usually maintains this altitude for most of the flight (straight-and-level flight). This phase of the flight is known as &lt;em&gt;cruise&lt;/em&gt;. When nearing its destination, a point is reached that is known as &lt;em&gt;top of descent&lt;/em&gt;. At this point the pilot enters the &lt;em&gt;descent&lt;/em&gt; phase. Finally, the flight ends during the &lt;em&gt;landing&lt;/em&gt; of the aircraft.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Post-flight data analysts are often interested in separating useful from non-useful (or less useful) flight paths prior to a specific analysis. In this document, and its associated analysis code, useful profiles will be labeled as &lt;em&gt;typical&lt;/em&gt; (abbreviated as “typ”) while less useful, non-useful, or anomalous flight paths (for a specific analysis) will be labeled as &lt;em&gt;non-typical&lt;/em&gt; (abbreviated as “non”). A &lt;em&gt;typical&lt;/em&gt; flight profile, in the context of this document, has a relatively extended cruise section without changes in altitude (see Figure 1.1). This characteristic will make it useful for certain types of analyses, for example, to estimate hard-to-measure variables like drag and exact angle-of-attack, as well as estimations of the positions of vertical flight control surfaces (even though they are measured). Flight paths could also be considered &lt;em&gt;non-typical&lt;/em&gt; due to insignificant (i.e. too short) cruise segments and missing data (see Figure 1.2). Note that our definition of usefulness is by no means universal in post-flight analysis.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/fig1-1.png&quot; alt=&quot;Figure 1.1 Typical and Non-typical vertical flight path examples (the non-typical path on the right has steps during cruise)&quot; title=&quot;Figure 1.1 Typical and Non-typical vertical flight path examples (the non-typical path on the right has steps during cruise)&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;A large airline can operate thousands of flights a day and it is not feasible for the analyst to do this separation/classification in a manual way. What comes to mind next is to construct an algorithm to take on the task. However, it is not straightforward to come up with a traditional algorithm that would discriminate between typical and non-typical flight paths. A promising approach, of course, is to use supervised machine learning and show the model a large enough number of training examples.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/fig1-2.png&quot; alt=&quot;Figure 1.2 Non-typical vertical flight path examples due to insignificant cruise section (left) and missing data (right)&quot; title=&quot;Figure 1.2 Non-typical vertical flight path examples due to insignificant cruise section (left) and missing data (right)&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The &lt;em&gt;predictor points&lt;/em&gt; for this problem are not structured vectors as is common in the case of structured data analysis. Here we have to use a time-series or sequence of scalar-valued predictor points and have the model learn the associated &lt;em&gt;target point&lt;/em&gt; which is a single categorical “scalar” in each case. The values of the target points will be either &lt;em&gt;typical&lt;/em&gt; (“typ”) or &lt;em&gt;non-typical&lt;/em&gt; (“non”). We therefore have a classification problem: Given a flight path (as a scalar-valued time-series), the classifier should choose between typical and non-typical (scalar-valued and categorical).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In the deep learning subfield, it is common to use a &lt;em&gt;Recurrent Neural Network&lt;/em&gt; (RNN) for this kind of problem. See, for example, Hüsken and Stagge (2003), and also Sun, Di, and Fang (2019). However, the training of an RNN can be challenging due to high demands on computing resources including processing power, processing time, and memory. There is also the vanishing/exploding gradients problem, addressed in various ways, but is often still lurking in the background.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Consider how easy it is for the human visual system to handle this problem, and in a fraction of a second. In fact, this is exactly how analysts often do their classifications manually. This suggests that we might benefit from tapping into the biological mechanisms for analyzing visual data (i.e. images). Recently, some researchers started adopting this insight. See, for example, Wang and Oates (2015a). The essence of this line of thought is the following: Instead of analyzing a sequence of 1-D or scalar-valued &lt;em&gt;temporal&lt;/em&gt; data points, we transform them into a single 2-D or matrix-valued &lt;em&gt;spatial&lt;/em&gt; data point. The spatial data point is simply an image which means the time-series signal has been transformed into an image (see Figure 1.3 for an example of a transformation technique). This allows for the application of a large body of relatively well-developed computer vision techniques to the above-stated problem. Most of these techniques center around the &lt;em&gt;Convolutional Neural Network&lt;/em&gt; (CNN). In summary, the &lt;em&gt;time-series classification&lt;/em&gt; problem has been converted to an &lt;em&gt;image classification&lt;/em&gt; problem.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/fig1-3.png&quot; alt=&quot;Figure 1.3 Example of Gramian Angular Summation Field (GASF) transformation of a time-series&quot; title=&quot;Figure 1.3 Example of Gramian Angular Summation Field (GASF) transformation of a time-series&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;This comparative case study will not attempt to compare the difference between using RNNs and CNNs to solve the flight path classification problem. Instead, it will compare the impact of a number of transformation techniques (to transform the time-series into an image) on the image classification performance. After application of each transformation technique to the training dataset of flight path time-series, a CNN will be trained which will serve as a classifier. The performance of the classifiers will be compared. &lt;em&gt;Transfer learning&lt;/em&gt; will be used to speed up the training of the CNNs.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;1.1--VALUE-PROPOSITION&quot;&gt;1.1  VALUE PROPOSITION&lt;a class=&quot;anchor-link&quot; href=&quot;#1.1--VALUE-PROPOSITION&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;This project seeks to provide value in a number of ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Demonstrates how flight profile time-series can be turned into images for more effective classification&lt;/li&gt;
&lt;li&gt;Identifies the best transformation technique for flight path time-series&lt;/li&gt;
&lt;li&gt;Reduces the need for (or does away with) hand classification of flight profiles. This will save significant amounts of time for post-flight analysts.&lt;/li&gt;
&lt;li&gt;Provides an analytic process that can be adopted as a tool by analysts. They can then implement the analytical process in their own preferred technology environment.&lt;/li&gt;
&lt;li&gt;Demonstrates how transfer learning greatly speedup the time to train a CNN neural network for the classification of profiles. This should encourage analysts that might still be skeptical about the use of deep learning for everyday tasks, and save them even more time.&lt;/li&gt;
&lt;li&gt;Demonstrates how post-flight analysis can be undertaken by ordinary analysts. This data is usually considered sensitive by airlines and are not published. A publicly available de-identified source of flight data is used and the project demonstrates how this provides a valuable opportunity for analysts.&lt;/li&gt;
&lt;li&gt;Encourages data scientists to undertake post-flight analyses. This is especially needed in the area of airline safety. In addition, and when allowed by airline policies and pilot unions, post-flight analysis can be a valuable tool in the performance evaluation of pilots. This can have a positive impact on the profitability of an airline.&lt;/li&gt;
&lt;li&gt;Satisfies my personal interest in the analysis of flight data as well as the application of cutting edge analysis techniques in the form of deep learning.&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;1.2-OBJECTIVES&quot;&gt;1.2 OBJECTIVES&lt;a class=&quot;anchor-link&quot; href=&quot;#1.2-OBJECTIVES&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;To setup an analytics pipeline for analysts, the foremost objective is to find the best transformation technique to convert flight path time-series into images. The rest of the objectives provide the components for the construction of the pipeline.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;1.2.1-Transformation-Techniques&quot;&gt;1.2.1 Transformation Techniques&lt;a class=&quot;anchor-link&quot; href=&quot;#1.2.1-Transformation-Techniques&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We will perform a comparison of a number of transformation techniques in terms of their associated image classification performance. To do this we will apply each transformation technique to the cleaned time-series dataset in turn, train a CNN to do classification (using supervised learning), and record the performance. Then we will select the most performant transformation technique and use this technique in the rest of the analysis pipeline. The following transformation techniques will be considered:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Altitude line plots transformed into an image&lt;/li&gt;
&lt;li&gt;Altitude area plots transformed into an image&lt;/li&gt;
&lt;li&gt;Gramian Angular Summation Field (GASF)&lt;/li&gt;
&lt;li&gt;Gramian Angular Difference Field (GADF)&lt;/li&gt;
&lt;li&gt;Markov Transition Field (MTF)&lt;/li&gt;
&lt;li&gt;Recurrence Plot (RP)&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;1.2.2-Developed/Non-developed-Model&quot;&gt;1.2.2 Developed/Non-developed Model&lt;a class=&quot;anchor-link&quot; href=&quot;#1.2.2-Developed/Non-developed-Model&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The first model in the analytics pipeline will classify flight profiles into developed (useful) and non-developed (non-useful) profiles. We will also consider the use of &lt;em&gt;anomaly detection&lt;/em&gt; by means of an &lt;em&gt;auto-encoder&lt;/em&gt; (instead of a classification algorithm) due to a significant class imbalance.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;1.2.3-Canonical-Segments-Model&quot;&gt;1.2.3 Canonical Segments Model&lt;a class=&quot;anchor-link&quot; href=&quot;#1.2.3-Canonical-Segments-Model&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The next model in the pipeline will perform multi-label classification of the developed profiles. The labels used here will reflect whether a profile has &lt;em&gt;canonical&lt;/em&gt; climb, cruise, and descent segments. In this context, canonical means relatively smooth.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;1.2.4-Extended/Short-Cruises-Model&quot;&gt;1.2.4 Extended/Short Cruises Model&lt;a class=&quot;anchor-link&quot; href=&quot;#1.2.4-Extended/Short-Cruises-Model&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The final model in the pipeline will classify flight profiles with canonical &lt;em&gt;cruise&lt;/em&gt; segments (regardless of the properties of climb or descent segments) into profiles that have extended cruises (useful) and shorter cruises (non-useful).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;1.2.3-End-to-end-Inference&quot;&gt;1.2.3 End-to-end Inference&lt;a class=&quot;anchor-link&quot; href=&quot;#1.2.3-End-to-end-Inference&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The final objective will be to prepare a significant &lt;em&gt;test&lt;/em&gt; dataset, consisting of data points that have never been seen by any of the models. We will construct an end-to-end inference process to simulate a production system and apply it to the test dataset. Then we will make recommendations to post-flight analysts.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;2-DATA-SOURCE&quot;&gt;2 DATA SOURCE&lt;a class=&quot;anchor-link&quot; href=&quot;#2-DATA-SOURCE&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;At any moment, there is an average of about 10,000 airplanes in the sky carrying more than a million passengers. Hundreds of variables are usually monitored during a flight which often has a duration of a number of hours. Many of these variables are sampled at a rate of once per second or more frequently. A huge volume of data is generated during a typical flight. This suggests that the analysis of flight data should be of some importance. Moreover, it seems reasonable that flight data should be easily accessible. This is not always the case, however.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Flight data directly reveals how an airline operates its core business and how efficiently pilots perform their duties. This data is considered sensitive. Some of the collected flight data, however, is so basic that it is, in fact, publicly available. Examples are datapoints that contain altitude, latitude, longitude, and heading. This information is considered to be public in the interest of the safe operation of all aircraft.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;2.1-SOURCES-OF-FLIGHT-DATA&quot;&gt;2.1 SOURCES OF FLIGHT DATA&lt;a class=&quot;anchor-link&quot; href=&quot;#2.1-SOURCES-OF-FLIGHT-DATA&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The gradual adoption of &lt;em&gt;Automatic Dependent Surveillance – Broadcast&lt;/em&gt; (ADS–B) by airlines is leading to the wide availability of flight data in the public domain. Wikipedia gives a good overview of this technology (&quot;Automatic dependent surveillance – broadcast,&quot; n.d.). The ADS-B technology allows an aircraft to use satellite navigation to find its position. It then broadcasts this information periodically which enables ground stations to track it. This method is used as a replacement for secondary surveillance radar (SSR) and does not depend on an interrogation signal from the ground. The data that is broadcast can also update the situational awareness of other aircraft in the area.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;2.1.1---Publicly-available-flight-data&quot;&gt;2.1.1   Publicly available flight data&lt;a class=&quot;anchor-link&quot; href=&quot;#2.1.1---Publicly-available-flight-data&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The increasing use of ADS-B has led to many flight tracking sites that publish basic flight data for consumption by the public. See &quot;This Is How Flight Tracking Sites Work&quot; (Rabinowitz, 2017). This data is relatively superficial and usually consists of a dozen or so measured quantities. Some of the more prominent players are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ADS-B Exchange at &lt;a href=&quot;https://www.adsbexchange.com/&quot;&gt;https://www.adsbexchange.com/&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;OPENSKY at &lt;a href=&quot;https://opensky-network.org/&quot;&gt;https://opensky-network.org/&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;FlightAware at &lt;a href=&quot;https://flightaware.com/&quot;&gt;https://flightaware.com/&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;ADSBHub at &lt;a href=&quot;http://www.adsbhub.org/&quot;&gt;http://www.adsbhub.org/&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;planefinder at &lt;a href=&quot;https://planefinder.net/&quot;&gt;https://planefinder.net/&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;Aireon at &lt;a href=&quot;https://aireon.com/&quot;&gt;https://aireon.com/&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;flightradar24 at &lt;a href=&quot;https://www.flightradar24.com/&quot;&gt;https://www.flightradar24.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;RadarBox at &lt;a href=&quot;https://www.radarbox24.com/&quot;&gt;https://www.radarbox24.com/&lt;/a&gt; &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Even though these sources provide the altitude data needed for the analyses described in this document, we chose to not use any of them. Follow-up analyses often require more in-depth flight data that is not provided by any of the ADS-B sources. We also want to provide an example of how to use a substantial flight data source consisting of in-depth data.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;2.1.2-In-depth-flight-data&quot;&gt;2.1.2 In-depth flight data&lt;a class=&quot;anchor-link&quot; href=&quot;#2.1.2-In-depth-flight-data&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Detailed, in-depth flight data is generally unavailable to the public. There are a few sources that make de-indentified data available but usefulness varies. A few sources are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DASHlink at &lt;a href=&quot;https://c3.nasa.gov/dashlink/&quot;&gt;https://c3.nasa.gov/dashlink/&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;IATA at &lt;a href=&quot;https://www.iata.org/services/statistics/gadm/Pages/fdx.aspx&quot;&gt;https://www.iata.org/services/statistics/gadm/Pages/fdx.aspx&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;Data.gov at &lt;a href=&quot;https://www.data.gov/&quot;&gt;https://www.data.gov/&lt;/a&gt; with a search term of “ads-b”&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;2.1.3-Selected-data-source&quot;&gt;2.1.3 Selected data source&lt;a class=&quot;anchor-link&quot; href=&quot;#2.1.3-Selected-data-source&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We selected the DASHlink source. The data is accessible from &lt;a href=&quot;https://c3.nasa.gov/dashlink/projects/85/&quot;&gt;https://c3.nasa.gov/dashlink/projects/85/&lt;/a&gt;.  After clicking on “35 Datasets,” we used the data for “Tail 687.” This is a large amount of data (2,395.4 MB in zipped format) from which we sub-selected the first three datasets: Tail_687_1.zip, Tail_687_2.zip, and Tail_687_3.zip. The data can be downloaded from &lt;a href=&quot;https://c3.nasa.gov/dashlink/resources/664/&quot;&gt;Sample Flight Data&lt;/a&gt;. There are 186 measured quantities (features) in the data.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;3-DATA-PREPARATION&quot;&gt;3 DATA PREPARATION&lt;a class=&quot;anchor-link&quot; href=&quot;#3-DATA-PREPARATION&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The preparation of data involves conversion, cleaning, resampling, and the transformation of time-series data to images.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;3.1-EXPLORATION-OF-DATA-STRUCTURE&quot;&gt;3.1 EXPLORATION OF DATA STRUCTURE&lt;a class=&quot;anchor-link&quot; href=&quot;#3.1-EXPLORATION-OF-DATA-STRUCTURE&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The structure of the raw data files is somewhat complicated. It is in MATLAB format and different variables were sampled at different sample rates. For familiarization, a thorough exploration of the structure of the raw data is provided in the notebook:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://nbviewer.jupyter.org/github/kobus78/dashlink/blob/master/10_mat2csv.ipynb&quot;&gt;10_mat2csv.ipynb&lt;/a&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The actual preparation of the data was divided into two procedures. This first takes care of general conversion and cleaning tasks. The second undertakes the transformation of the time-series in each file to its associated spatial signal or image. Each procedure occurs in its own notebook.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;3.2-CLEANING-PREPARATION-PROCEDURE&quot;&gt;3.2 CLEANING PREPARATION PROCEDURE&lt;a class=&quot;anchor-link&quot; href=&quot;#3.2-CLEANING-PREPARATION-PROCEDURE&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The source data is in MATLAB format (.mat) after downloading and unzipping. The raw data acquired for this project were as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;For training, including validation (data will be labeled)&lt;ul&gt;
&lt;li&gt;Tail_687_1.zip (651 flights)&lt;/li&gt;
&lt;li&gt;Tail_687_2.zip (602 flights)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;For testing, i.e. simulation of production (data will not be labeled)&lt;ul&gt;
&lt;li&gt;Tail_687_3.zip (582 flights)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;After downloading and unzipping, the files in each folder were converted separately (from .mat to .csv) by means of the notebook:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://nbviewer.jupyter.org/github/kobus78/dashlink/blob/master/10_mat2csv-2.ipynb&quot;&gt;10_mat2csv-2.ipynb&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The output .csv files were eventually moved to either the Train (1,253 files) or Test (582 files) folders.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The cleaning preparation procedure can be summarized as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Conversion:&lt;ul&gt;
&lt;li&gt;Using the scipy.io Python package, the data is converted from MATLAB format to a simple .csv format.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Make a dataframe for each sample rate:&lt;ul&gt;
&lt;li&gt;All datapoints for a specific sample rate are collected in a dataframe. The rates available are referred to as 0.25, 1, 2, 4, 8, and 16.&lt;/li&gt;
&lt;li&gt;All dataframes are combined into a single dataframe.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Remove invalid time values:&lt;ul&gt;
&lt;li&gt;Files with invalid values for year, month, day, hour, minute, or second are removed in this step.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Output a csv file from the dataframe&lt;/li&gt;
&lt;li&gt;Build date-time index&lt;ul&gt;
&lt;li&gt;Being a time-series, it is important to index the data in the form of a date-time index. This is done by reading the exported file back into a dataframe.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Down-sample to 1 minute rate:&lt;ul&gt;
&lt;li&gt;The data in each file’s dataframe is down-sampled from a variable’s specific sample rate to a 1 minute rate. This reduces the intensity of the data as well as provides for a more realistic sample rate for the purposes of this study. &lt;/li&gt;
&lt;li&gt;Another csv file is exported using the same name but having a “-1min” appended to the name.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;3.3-TRANFORMATION-PREPARATION-PROCEDURE&quot;&gt;3.3 TRANFORMATION PREPARATION PROCEDURE&lt;a class=&quot;anchor-link&quot; href=&quot;#3.3-TRANFORMATION-PREPARATION-PROCEDURE&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The transformation procedure occurs in the notebook:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://nbviewer.jupyter.org/github/kobus78/dashlink/blob/master/10_csv2png-3.ipynb&quot;&gt;10_csv2png-3.ipynb&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The transformation preparation procedure can be summarized as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Read the csv data into a dataframe&lt;/li&gt;
&lt;li&gt;Select the transformation technique&lt;ul&gt;
&lt;li&gt;Done by commenting in the appropriate section of code. Please see section 4.1.3 for a description of each transformation technique.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Plot the time-series signal&lt;ul&gt;
&lt;li&gt;To convert the time-series signal to a spatial signal it is plotted as a graphic.&lt;/li&gt;
&lt;li&gt;The graphic is stripped of all annotations, e.g. the frame, tick marks, tick labels, axis labels, and heading.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Save the image, using the same name but having an extension of .png.&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;4-MODELING&quot;&gt;4 MODELING&lt;a class=&quot;anchor-link&quot; href=&quot;#4-MODELING&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In this section, we will look at the important concept of time-series classification and how it relates to two of the most important deep learning architectures: Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs). Then we will discuss the classification models for our pipeline in detail, making use of image-transformed flight profile time-series and the CNN architecture.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;4.1-TIME-SERIES-CLASSIFICATION&quot;&gt;4.1 TIME-SERIES CLASSIFICATION&lt;a class=&quot;anchor-link&quot; href=&quot;#4.1-TIME-SERIES-CLASSIFICATION&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Time-series classification (TSC) is an area of active research. Some consider it one of the most challenging problems in data mining (Esling &amp;amp; Agon, 2012). This opinion is supported by Yang and Wu (2006). A large number of techniques have been invented. Many of these approaches are covered by Bagnall, Lines, Bostrom, Large, and Keogh (2017). The most promising approach, they point out, is known as COTE (Collective Of Transformation-based Ensembles) as described by Bagnall, Lines, Hills, and Bostrom (2016). HIVE-COTE is an extension of COTE (Lines, Taylor, &amp;amp; Bagnall, 2016). See also Lines, Taylor, and Bagnall (2018). The extension is in the form of a Hierarchical Vote system. This is considered the state-of-the-art currently. To use HIVE-COTE a large number of classifiers (37) need to be trained. The decisions made by them are not easy to interpret and classification time is excessive.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Given the impressive successes of deep learning in many disciplines lately, the use of it has started to make inroads into the area of time-series classification (Wang, Yan, &amp;amp; Oates, 2017). In their recent paper, “Deep Learning for Time Series Classification: A Review,” Fawaz, Forestier, Weber, Idoumghar, and Muller (2019) point out that they achieved results that are not significantly different from results obtained from HIVE-COTE by making use of deep learning and a residual network. They also provide a handy taxonomy (p. 11) for the use of deep learning algorithms to classify time-series (somewhat abbreviated here):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Deep Learning for Time Series Classification&lt;ul&gt;
&lt;li&gt;Generative Models&lt;ul&gt;
&lt;li&gt;Auto Encoders&lt;ul&gt;
&lt;li&gt;RNNs&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Echo State Networks (simplified RNNs)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Discriminative Models&lt;ul&gt;
&lt;li&gt;Feature Engineering&lt;ul&gt;
&lt;li&gt;Image Transformation&lt;/li&gt;
&lt;li&gt;Domain Specific&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;End-to-End&lt;ul&gt;
&lt;li&gt;Multi-Layer Perceptrons (aka fully-connected or FC networks)&lt;/li&gt;
&lt;li&gt;CNNs&lt;/li&gt;
&lt;li&gt;Hybrid&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The main division is between generative and discriminative models. Generative models generally include an unsupervised training step before the learner fits its classifier. A discriminative model, on the other hand, directly fits the mapping from the raw input of a time-series to the probability distribution over the classification classes.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The literature informally agree that discriminative models are more accurate than generative models. In this report, we will focus on the Image Transformation leaf of this tree, which falls under discriminative models.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;There are significant advantages in the use of deep learning to classify time-series. One specific advantage is the ability to detect time invariant characteristics. This is similar to how spatially invariant filters detect patterns in images.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;4.1.1-Recurrent-Neural-Networks-(RNNs)&quot;&gt;4.1.1 Recurrent Neural Networks (RNNs)&lt;a class=&quot;anchor-link&quot; href=&quot;#4.1.1-Recurrent-Neural-Networks-(RNNs)&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In a fairly old paper, Hüsken and Stagge (2003) promote the use of RNNs for time-series classification. Recurrent layers are described by the equations:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
$$ \Large
\begin{align}
\mathbf{a}^{&amp;lt;t&amp;gt;} &amp;amp;= g(\mathbf{W}_{aa} \mathbf{a}^{&amp;lt;t-1&amp;gt;} + \mathbf{W}_{ax} \mathbf{x}^{&amp;lt;t&amp;gt;} + \mathbf{b}_a) \\
\hat{\mathbf{y}}^{&amp;lt;t&amp;gt;}  &amp;amp;= g(\mathbf W_{ya}\mathbf{a}^{&amp;lt;t&amp;gt;} + \mathbf{b_y})
\end{align}
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The parameters or weights that undergo training are captured in a number of &lt;em&gt;filters&lt;/em&gt; or &lt;em&gt;kernels&lt;/em&gt;. The &lt;em&gt;feedback&lt;/em&gt; filter is $\mathbf{W}_{aa}$, the &lt;em&gt;input&lt;/em&gt; filter $\mathbf{W}_{ax}$, and the &lt;em&gt;output&lt;/em&gt; filter $\mathbf{W}_{ya}$. The &lt;em&gt;signal&lt;/em&gt; is the data that are used as examples during training. The symbols $\mathbf{x}^{&amp;lt;t&amp;gt;}$ and $\mathbf{\hat{y}}^{&amp;lt;t&amp;gt;}$ represent the input and output signals respectively. The hidden state, or internal signal, is given by $\mathbf{a}^{&amp;lt;t&amp;gt;}$. The filters are matrices while the signals are vector-valued. There is often a single layer in an RNN. Note, however, that this architecture is recursive. This means that each time-step could be considered a separate layer in time.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In the context of our classification problem, the input is a sequence of scalar-valued continuous predictor points. The output is a single scalar-valued categorical target point (after being processed by a sigmoid function). The values of a target point are the classes, either &lt;em&gt;typ&lt;/em&gt; or &lt;em&gt;non&lt;/em&gt;. This type of RNN is also known as a &lt;em&gt;many-to-one&lt;/em&gt; RNN because a series of input data points leads to a single output datapoint.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;4.1.2-Disadvantages-of-RNNs&quot;&gt;4.1.2 Disadvantages of RNNs&lt;a class=&quot;anchor-link&quot; href=&quot;#4.1.2-Disadvantages-of-RNNs&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In 2015 RNNs made a dramatic come-back (Karpathy, 2015). A year or two after this the &lt;em&gt;ResNet&lt;/em&gt; (He, Zhang, Ren &amp;amp; Sun, 2016) and the &lt;em&gt;attention&lt;/em&gt; mechanism (Xu et al., 2015) were invented. This provided an expanded context for the evaluation of RNNs and the Long Short Term Memory (LSTM). A further two years later saw the beginning of the decline of the popularity of the RNN and the LSTM in some disciplines.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Culurciell (2018) points out some shortcomings of RNNs in his article “The fall of RNN / LSTM.” In this regard, he mentions the problem of vanishing gradients and that RNNs are not hardware friendly. Fawaz et al. (2019) mostly agree with these sentiments and also mention that an RNN’s architecture was designed for the prediction of the next element in a sequence, not necessarily ideal for the classification task. RNNs are also harder to train and parallelize.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;4.1.3-Time-series-transformation-to-images&quot;&gt;4.1.3 Time-series transformation to images&lt;a class=&quot;anchor-link&quot; href=&quot;#4.1.3-Time-series-transformation-to-images&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Before we look at Convolutional Neural Networks (CNNs), we will discuss how a time-series can be transformed into an image. The purpose of this transformation is to enable computers to “visually” recognize and classify the time-series signal. By doing this transformation we can take advantage of the impressive successes of deep learning architectures (using CNNs) in computer vision. This allows us to identify the structure of a time-series, leading to more effective classification.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We will start by plotting the altitude time-series of a flight. This will be the first (and most simple) transformation. Then we make the image slightly richer by filling the area under the curve. After this, a number of more sophisticated transformation techniques will be used to transform the same time-series into the following representations: Gramian angular summation field, Gramian angular difference field, Markov transition field, and a recurrence plot.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;4.1.3.1-Altitude-line-plots-transformed-into-an-image&quot;&gt;4.1.3.1 Altitude line plots transformed into an image&lt;a class=&quot;anchor-link&quot; href=&quot;#4.1.3.1-Altitude-line-plots-transformed-into-an-image&quot;&gt; &lt;/a&gt;&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;To create this spatial (or image) representation of the time-series, we will use a dark pen on a light background. This black-and-white rendition was deliberately prevented from being binary. A binary image only needs one channel. The pixel values of such an image only have one of two values. To make fair comparisons with more complex renditions (that use coloration), this image was allowed to have three channels as well. This manifests in the form of some blurriness along the black lines which come from the presence of grey pixels.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;No graphical annotations will be included, i.e. graphic frame, tick marks, tick labels, axis labels, and heading. Annotations will make the learning process unnecessarily complex. Figure 4.1 shows an example.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/fig4-1.png&quot; alt=&quot;Figure 4.1 Line plot of altitude, also considered a line-plot transformation of the example time-series&quot; title=&quot;Figure 4.1 Line plot of altitude, also considered a line-plot transformation of the example time-series&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;4.1.3.2-Altitude-area-plots-transformed-into-an-image&quot;&gt;4.1.3.2 Altitude area plots transformed into an image&lt;a class=&quot;anchor-link&quot; href=&quot;#4.1.3.2-Altitude-area-plots-transformed-into-an-image&quot;&gt; &lt;/a&gt;&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;To create the area transformation, we use a color (magenta) that is not confined to a single channel in the red-green-blue (RGB) encoding. There is also an outline in a slightly darker tint. Figure 4.2 shows the rendition of the same time-series.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/fig4-1.png&quot; alt=&quot;Figure 4.2 Area plot of altitude, also considered an area-plot transformation of the example time-series&quot; title=&quot;Figure 4.2 Area plot of altitude, also considered an area-plot transformation of the example time-series&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;4.1.3.3-Gramian-Angular-Field-(GAF)&quot;&gt;4.1.3.3 Gramian Angular Field (GAF)&lt;a class=&quot;anchor-link&quot; href=&quot;#4.1.3.3-Gramian-Angular-Field-(GAF)&quot;&gt; &lt;/a&gt;&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The use of Gramian Angular Fields is described in Wang and Oates (2015a, 2015b, 2015c). Having a data point represented as either a time-series or an image is referred to by them as the &lt;em&gt;duality between time-series and images&lt;/em&gt;. We will first develop the theory of the more generic &lt;em&gt;Gramian Angular Field&lt;/em&gt; before we distinguish between the &lt;em&gt;summation&lt;/em&gt; (GASF) and &lt;em&gt;difference&lt;/em&gt; (GADF) fields.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Consider a time series $X=\{x^{&amp;lt;1&amp;gt;}, x^{&amp;lt;2&amp;gt;}, ..., x^{&amp;lt;m&amp;gt;}\}$ of $m$ scalar-valued and real datapoints. Being a time-series the order of the datapoints is important. The index in angle brackets indicate this order. Next, we scale $X$ so that all values are in the interval [-1, 1]:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
$$ \Large
\begin{align}
\tilde{x}^{&amp;lt;i&amp;gt;} = \frac{(x^{&amp;lt;i&amp;gt;} - \text{max}(X)) + (x^{&amp;lt;i&amp;gt;} - \text{min}(X))}{\text{max}(X)-\text{min}(X)} \qquad(1)
\end{align}
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;where $\tilde{x}^{&amp;lt;i&amp;gt;}$ is a rescaled datapoint and $\tilde{X}$ the rescaled time-series. Equation 1 indicates the value of each of the elements of the time-series. We now represent $\tilde{X}$ in polar coordinates by encoding the &lt;em&gt;value&lt;/em&gt; of a datapoint as the angular cosine and the &lt;em&gt;timestamp&lt;/em&gt; of the datapoint as the radius:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
$$ \Large
\begin{align}
\phi &amp;amp;= \text{arccos}(\tilde{x}^{&amp;lt;i&amp;gt;}), -1 \le \tilde{x}^{&amp;lt;i&amp;gt;} \in \tilde{X} \qquad\qquad\;\,(2a) \\
r &amp;amp;= \frac{t^{&amp;lt;i&amp;gt;}}{N}, t^{&amp;lt;i&amp;gt;} \in \mathbb{N} \qquad\qquad\qquad\qquad\qquad\qquad\quad(2b)
\end{align}
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;where $t^{&amp;lt;i&amp;gt;}$ is the timestamp, $N$ is a constant factor to regularize the span of the polar coordinate system, and $\mathbb{N}$ is the set of natural numbers.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;To understand this transformation, imagine the following: Varying &lt;em&gt;values&lt;/em&gt; of the time-series swing along different angular positions, similar to a weather cock. The progression of &lt;em&gt;time&lt;/em&gt; resembles the way rippling water waves would emanate from the point where a stone was dropped. The transformation equations (Equation 2a, 2b) form a bijection because $\text{cos}\phi$ is monotonic when $\phi \in{[0, \pi]}$ (see Figure 4.3). This means that, for a given time-series, the mapping in Equation 2 produces one and only one transformation in the polar coordinate system with a unique inverse function.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/fig4-3.png&quot; alt=&quot;Figure 4.3 Inverse Cosine (mathisfun.com)&quot; title=&quot;Figure 4.3 Inverse Cosine (mathisfun.com)&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Next, we form the GAF matrix, $G$ where each element is the cosine of the sum of each pair of $\phi$’s. This captures the temporal correlation within different time intervals:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
$$ \Large
G = 
\begin{bmatrix}
\text{cos}(\phi_1+\phi_1) &amp;amp; \cdots &amp;amp; \text{cos}(\phi_1+\phi_m)\\
\text{cos}(\phi_2+\phi_1) &amp;amp; \cdots &amp;amp; \text{cos}(\phi_2+\phi_m)\\
\vdots &amp;amp; \ddots &amp;amp; \vdots \\
\text{cos}(\phi_m+\phi_1) &amp;amp; \cdots &amp;amp; \text{cos}(\phi_m+\phi_m)
\end{bmatrix} \qquad\qquad(3)
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h5 id=&quot;4.1.3.3.1-Gramian-Angular-Summation-Field-(GASF)&quot;&gt;4.1.3.3.1 Gramian Angular Summation Field (GASF)&lt;a class=&quot;anchor-link&quot; href=&quot;#4.1.3.3.1-Gramian-Angular-Summation-Field-(GASF)&quot;&gt; &lt;/a&gt;&lt;/h5&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In this form, we call the GAF matrix, $G$, the Gramian Angular Summation Field (GASF):&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
$$ \Large
G_{GASF} = 
\begin{bmatrix}
\text{cos}(\phi_1+\phi_1) &amp;amp; \cdots &amp;amp; \text{cos}(\phi_1+\phi_m)\\
\text{cos}(\phi_2+\phi_1) &amp;amp; \cdots &amp;amp; \text{cos}(\phi_2+\phi_m)\\
\vdots &amp;amp; \ddots &amp;amp; \vdots \\
\text{cos}(\phi_m+\phi_1) &amp;amp; \cdots &amp;amp; \text{cos}(\phi_m+\phi_m)
\end{bmatrix} \qquad(4)
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;If the same time-series represented in Figure 4.1 is GASF-transformed, we get the image in Figure 4.4.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/fig4-4.png&quot; alt=&quot;Figure 4.4 Gramian Angular Summation Field (GASF) transformation of the example time-series&quot; title=&quot;Figure 4.4 Gramian Angular Summation Field (GASF) transformation of the example time-series&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h5 id=&quot;4.1.3.3.2-Gramian-Angular-Difference-Field-(GADF)&quot;&gt;4.1.3.3.2 Gramian Angular Difference Field (GADF)&lt;a class=&quot;anchor-link&quot; href=&quot;#4.1.3.3.2-Gramian-Angular-Difference-Field-(GADF)&quot;&gt; &lt;/a&gt;&lt;/h5&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;If the summations in the GAF matrix are replaced with differences, and the cosines with sinuses, we get the Gramian Angular Difference Field (GADF). To define the Gramian Angular Difference Field (GADF) we form the sine of the difference of each pair of $\phi$’s:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
$$ \Large
G_{GADF} = 
\begin{bmatrix}
\text{sin}(\phi_1-\phi_1) &amp;amp; \cdots &amp;amp; \text{sin}(\phi_1-\phi_m)\\
\text{sin}(\phi_2-\phi_1) &amp;amp; \cdots &amp;amp; \text{sin}(\phi_2-\phi_m)\\
\vdots &amp;amp; \ddots &amp;amp; \vdots \\
\text{sin}(\phi_m-\phi_1) &amp;amp; \cdots &amp;amp; \text{sin}(\phi_m-\phi_m)
\end{bmatrix} \qquad\qquad(5)
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;When Figure 4.1’s time series is GADF-transformed, we get the image in Figure 4.5.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/fig4-5.png&quot; alt=&quot;Figure 4.5 Gramian Angular Difference Field (GADF) transformation of the example time-series&quot; title=&quot;Figure 4.5 Gramian Angular Difference Field (GADF) transformation of the example time-series&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;4.1.3.4-Markov-Transition-Field-(MTF)&quot;&gt;4.1.3.4 Markov Transition Field (MTF)&lt;a class=&quot;anchor-link&quot; href=&quot;#4.1.3.4-Markov-Transition-Field-(MTF)&quot;&gt; &lt;/a&gt;&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The use of Markov Transition Fields is also described in Wang and Oates (2015a, 2015b, 2015c). Given a times-series $X$, we identify its $Q$ quantile bins by assigning each $x^{&amp;lt;i&amp;gt;}$ to the corresponding bins $q_j$ ($j \in [1, Q]$). This allows us to construct a weighted adjacency matrix $W$ by counting transitions among quantile bins according to a first-order Markov chain along the time axis. The dimensions of $W$ are $Q \times Q$. Each element of $W$, $w_{ij}$, will be the frequency with which a point in quantile $q_j$ is followed by a point in quantile $q_i$.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Next, we normalize $W$ by having $\mathit\Sigma_j w_{ij}=1$. This gives us the Markov transition matrix. This matrix is insensitive to temporal dependency on the time steps $t^{&amp;lt;i&amp;gt;}$. To compensate for the associated information loss (due to the lack of temporal dependency), we define the Markov Transition Field (MTF) in Equation 6:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
$$ \Large
M = 
\begin{bmatrix}
w_{ij|x_1 \in q_i, x_1 \in q_j} &amp;amp; \cdots &amp;amp; w_{ij|x_1 \in q_i, x_m \in q_j} \\
w_{ij|x_2 \in q_i, x_1 \in q_j} &amp;amp; \cdots &amp;amp; w_{ij|x_2 \in q_i, x_m \in q_j} \\
\vdots &amp;amp; \ddots &amp;amp; \vdots \\
w_{ij|x_m \in q_i, x_1 \in q_j} &amp;amp; \cdots &amp;amp; w_{ij|x_m \in q_i, x_m \in q_j}
\end{bmatrix} \qquad\qquad(6)
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The matrix $W$ (Markov transition matrix) is constructed as follows. The magnitude of all the datapoints are separated into $Q$ quantile bins. The bins that contain the datapoints at timestamp $i$ and $j$ are $q_i$ and $q_j$. The associated transition probability of $q_i \to q_j$ in $M$ (Markov Transition Field) is $M_{ij}$. This has the effect of spreading out the matrix $W$ which contains the transition probability on the magnitude axis into the $M$ matrix by taking into account the temporal positions. This leads to the $M$ matrix actually &lt;em&gt;encoding&lt;/em&gt; the multi-span transition probabilities of the time-series, because we assign the probability from the quantile at timestamp $i$ to the quantile at timestamp $j$ at each pixel $M_{ij}$. The main diagonal $M_{ii}$ contains the probability from each quantile to itself (also called the self-transition probability) at timestamp $i$.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;When the time series in Figure 4.1 is MTF-transformed, we get the image in Figure 4.6.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/fig4-6.png&quot; alt=&quot;Figure 4.6 Markov Transition Field (MTF) transformation of the example time-series&quot; title=&quot;Figure 4.6 Markov Transition Field (MTF) transformation of the example time-series&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;4.1.3.5-Recurrence-Plot-(RP)&quot;&gt;4.1.3.5 Recurrence Plot (RP)&lt;a class=&quot;anchor-link&quot; href=&quot;#4.1.3.5-Recurrence-Plot-(RP)&quot;&gt; &lt;/a&gt;&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Hatami, Gavet, and Debayle (2017) describe the recurrence plot transformation. Another source for this technique is Tripath and Acharya (2018).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;A discussion of the recurrence plot (RP) relies on the concept of a &lt;em&gt;phase space&lt;/em&gt;. Often used in the context of formal system theory, a phase space is a multidimensional space in which each axis corresponds to one of the variables necessary to specify the state of a system. Wikipedia offers the following definition:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;blockquote&gt;&lt;p&gt;In dynamical system theory, a &lt;em&gt;phase space&lt;/em&gt; is a space in which all possible states of a system are represented, with each possible state corresponding to one unique point in the phase space. For mechanical systems, the phase space usually consists of all possible values of position and momentum variables. The concept of phase space was developed in the late 19th century by Ludwig Boltzmann, Henri Poincaré, and Josiah Willard Gibbs.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The recurrence of states is a typical phenomenon for dynamic non-linear systems. The time-series that are generated by these systems can be characterized by periodicities and irregular cyclic behavior which is a form of recurrent dynamics. A two-dimensional representation of such recurrence dynamics can be visualized, based on the exploration of an n-dimensional &lt;em&gt;phase space&lt;/em&gt; trajectory. This visualization is provided by the &lt;em&gt;recurrence plot&lt;/em&gt; (RP) transformation. Central to this approach is to show in which points trajectories return to a previous state. If $K$ is the number of states $\pmb{s}$, $H$ is the Heaviside function, and $\epsilon$ is a threshold distance, then the matrix $R$ is given by Equation 7:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
$$ \Large
R_{ij} = H(\epsilon - \Vert \pmb{s}_i-\pmb{s}_j \Vert) \qquad\qquad\qquad(7)
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;where $\pmb{s}\in \mathfrak{R}^n$ and $i,j=1, 2, \ldots, K$. The $\Vert .\Vert$ indicates the norm of the difference between the two state vectors.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;To interpret the $R$ matrix two aspects should be considered: &lt;em&gt;Texture&lt;/em&gt; and &lt;em&gt;typology&lt;/em&gt;. Texture manifests in the form of single dots, vertical lines, horizontal lines, and diagonal lines. Typology is encountered in the forms &lt;em&gt;homogenous&lt;/em&gt;, &lt;em&gt;periodic&lt;/em&gt;, &lt;em&gt;disrupted&lt;/em&gt;, and &lt;em&gt;drift&lt;/em&gt;. For example, if fading happens towards the upper left and lower right corners the dynamics exhibit a drift (commonly referred to as a trend). It is not always straightforward, however, to visually interpret a recurrence plot.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;When the time series in Figure 4.1 is RP-transformed, we get the image in Figure 4.7.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/fig4-7.png&quot; alt=&quot;Figure 4.7 Recurrence Plot (RP) transformation of the example time-series&quot; title=&quot;Figure 4.7 Recurrence Plot (RP) transformation of the example time-series&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;4.1.4-Convolutional-Neural-Networks-(CNNs)&quot;&gt;4.1.4 Convolutional Neural Networks (CNNs)&lt;a class=&quot;anchor-link&quot; href=&quot;#4.1.4-Convolutional-Neural-Networks-(CNNs)&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The first paper that really showed the benefits of using CNNs for image classification was that of Krizhevsky, Sutskever, and Hinton (2012). A follow-up paper provided updates (Krizhevsky, Sutskever &amp;amp; Hinton, 2017). Convolutional Neural Networks are structured in a way that enables them to exploit translational invariance. They extract features through receptive fields. Significant weight sharing drastically reduces the number of parameters involved in training them. They are the state-of-the-art architecture in the handling of computer vision tasks.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;A CNN consists of different types of layers. The most important ones are &lt;em&gt;convolution&lt;/em&gt;, &lt;em&gt;pooling&lt;/em&gt;, and &lt;em&gt;dense&lt;/em&gt; layers. Convolution and pooling layers often alternate during the initial layers. Near the end, a number of dense layers usually occurs which often ends with a sigmoid or softmax layer in the case of a classification CNN.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;4.1.4.1-Convolution-layers&quot;&gt;4.1.4.1 Convolution layers&lt;a class=&quot;anchor-link&quot; href=&quot;#4.1.4.1-Convolution-layers&quot;&gt; &lt;/a&gt;&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Convolution layers are described by the equations:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
$$ \Large
\begin{align}
\mathbf{Z}^{[l]} &amp;amp;= \mathbf{W}^{[l]} \ast \mathbf{A}^{[l-1]} + \mathbf{b}^{[l]} \qquad(8) \\
\mathbf{A}^{[l]} &amp;amp;= g^{[l]}(\mathbf{Z}^{[l]}) \qquad\qquad\qquad\qquad\,(9)
\end{align}
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The &lt;em&gt;filter&lt;/em&gt; (also called &lt;em&gt;kernel&lt;/em&gt;) of a convolution layer is indicated by $\mathbf{W}^{[l]}$ where $l$ is the index of the layer. $\mathbf{W}^{[l]}$ is tensor-valued with each element $w_{ijk}^{[l]} \in \mathbb{R}$. The values of $w_{ijk}^{[l]}$ are learned by the training process. The &lt;em&gt;dimensions&lt;/em&gt; (or &lt;em&gt;shape&lt;/em&gt;) of $\mathbf{W}^{[l]}$ are $n_C^{[l-1]} \times f^{[l]} \times f^{[l]}$ where $n_C^{[l-1]}$ is the &lt;em&gt;number of filters&lt;/em&gt; (also the &lt;em&gt;number of channels&lt;/em&gt;) in the previous layer. The filter size is indicated by $f^{[l]}$. If we have multiple filters in layer $l$, the dimensions of $\mathbf{W}^{[l]}$ expand to $n_C^{[l]} \times n_C^{[l-1]} \times f^{[l]} \times f^{[l]}$ making $\mathbf{W}^{[l]}$ a vector of tensors.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The &lt;em&gt;activations&lt;/em&gt; in layer $l$ are represented by another tensor $\mathbf{A}^{[l]}$. For each layer, we distinguish between the &lt;em&gt;input&lt;/em&gt; activations, $\mathbf{A}^{[l-1]}$, and &lt;em&gt;output&lt;/em&gt; activations, $\mathbf{A}^{[l]}$. The dimensions of $\mathbf{A}^{[l-1]}$ are $n_C^{[l-1]} \times n_H^{[l-1]} \times n_W^{[l-1]}$ where $n_C^{[l-1]}$ is the number of channels in the previous layer, $n_H^{[l-1]}$ the &lt;em&gt;height&lt;/em&gt; of the image in the previous layer, and $n_W^{[l-1]}$ the &lt;em&gt;width&lt;/em&gt; of the image in the previous layer.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The dimensions of $\mathbf{A}^{[l]}$ are $n_C^{[l]} \times n_H^{[l]} \times n_W^{[l]}$  where&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
$$ \Large
n_H^{[l]}=\Bigg \lfloor \frac{n_H^{[l-1]}+2p^{[l]}-f^{[l]}}{s^{[l]}} + 1 \Bigg \rfloor\qquad\qquad\qquad\qquad(10)
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
$$ \Large
n_W^{[l]}=\Bigg \lfloor \frac{n_W^{[l-1]}+2p^{[l]}-f^{[l]}}{s^{[l]}} + 1 \Bigg \rfloor\qquad\qquad\qquad\qquad(11)
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The &lt;em&gt;padding&lt;/em&gt; size in layer $l$ is indicated by $p^{[l]}$. The &lt;em&gt;stride&lt;/em&gt; is represented by $s^{[l]}$. If we make use of mini-batch training, the dimensions of $\mathbf{A}^{[l]}$ will expand to $n_C^{[l]} \times n_H^{[l]} \times n_W^{[l]}$ where $m$ is the mini-batch size. In this case $\mathbf{A}^{[l]}$ becomes a vector of tensors.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The &lt;em&gt;bias&lt;/em&gt; vector in layer $l$ is indicated by $\mathbf{b}^{[l]}$. The &lt;em&gt;convolution operation&lt;/em&gt; is indicated by the symbol $\ast$. Equation 8 describes the &lt;em&gt;linear&lt;/em&gt; part of the convolution. The &lt;em&gt;activation&lt;/em&gt; part is captured by Equation 9. The &lt;em&gt;activation function&lt;/em&gt;, $g^{[l]}$, is often a rectified linear unit (ReLU).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Equations 8 and 9 can be combined into a single equation:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
$$ \Large
\mathbf{A}^{[l]} = g^{[l]}(\mathbf{W}^{[l]} \ast \mathbf{A}^{[l-1]} + \mathbf{b}^{[l]}) \qquad\qquad\qquad\qquad\,(12)
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://towardsdatascience.com/gentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9&quot;&gt;Piotr  Skalski&lt;/a&gt;
 (2019) has a blog that makes the operation of CNNs easy to understand. It includes a number of animations that provide valuable insight.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;4.1.4.2-Pooling-layers&quot;&gt;4.1.4.2 Pooling layers&lt;a class=&quot;anchor-link&quot; href=&quot;#4.1.4.2-Pooling-layers&quot;&gt; &lt;/a&gt;&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The filter of a pooling layer has no weights that need to be trained. It has a filter size $f^{[l]}$, and a stride $s^{[l]}$. The value for padding is almost always zero, $p^{[l]}=0$. The filter performs an aggregation operation as it slides over the activation signal. This operation is performed on each of the input channels independently. Types of aggregation operations are &lt;em&gt;maximum&lt;/em&gt; and &lt;em&gt;average&lt;/em&gt;. The most common type is the &lt;em&gt;max-pool&lt;/em&gt; layer. As the $f \times f$ filter slides across the image, it picks the maximum activation for each position and sends that to the output image, thereby reducing the size of the input image according to equations (10) and (11). There is also no activation function. This means pooling layers are described by the following equation:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
$$ \Large
\mathbf{A}^{[l]}=\text{max}(\mathbf{A}^{[l-1]}) \qquad\qquad\qquad(13)
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;4.1.4.3-Dense-layers&quot;&gt;4.1.4.3 Dense layers&lt;a class=&quot;anchor-link&quot; href=&quot;#4.1.4.3-Dense-layers&quot;&gt; &lt;/a&gt;&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;em&gt;Dense layers&lt;/em&gt; always form the last few layers of a CNN that performs classification. These layers are also called &lt;em&gt;fully connected&lt;/em&gt; layers. Dense layers are described by Equations 14 and 15:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
$$ \Large
\begin{align}
\mathbf{z}^{[l]} &amp;amp;= \mathbf{W}^{[l]}\mathbf{a}^{[l-1]}+\mathbf{b}^{[l]}\qquad\qquad (14)\\
\mathbf{a}^{[l]} &amp;amp;= g^{[l]}(\mathbf{z}^{[l]})\qquad\qquad\qquad\qquad\quad(15)
\end{align}
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Notice the absence of the convolution operator, which have been replaced by matrix multiplication.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The &lt;em&gt;filter&lt;/em&gt; of a dense layer is indicated by $\mathbf{W}^{[l]}$ where $l$ is the index of the layer. $\mathbf{W}^{[l]}$ is matrix-valued with each element $w_{ij}^{[l]}\in\mathbb{R}$. The values of $w_{ij}^{[l]}$ are learned by the training process. The &lt;em&gt;dimensions&lt;/em&gt; of $\mathbf{W}^{[l]}$ are $n^{[l]} \times n^{[l-1]}$ where $n^{[l-1]}$ is the &lt;em&gt;number of input features&lt;/em&gt; (also the number of output features in the previous layer), and $n^{[l]}$ is the &lt;em&gt;number of output features&lt;/em&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The &lt;em&gt;activations&lt;/em&gt; in layer $l$ are represented by a vector $\mathbf{a}^{[l]}$. For each layer, we distinguish between the &lt;em&gt;input&lt;/em&gt; activations, $\mathbf{a}^{[l-1]}$, and &lt;em&gt;output&lt;/em&gt; activations, $\mathbf{a}^{[l]}$. The dimension of $\mathbf{a}^{[l-1]}$ is $n^{[l-1]}$ where $n^{[l-1]}$ is the number of neurons or hidden units in the previous layer. The dimension of $\mathbf{a}^{[l]}$ is $n^{[l]}$ where $n^{[l]}$ is the number of units in the current layer.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;There is no concept of padding nor of stride. The &lt;em&gt;bias&lt;/em&gt; vector in layer $l$ is indicated by $\mathbf{b}^{[l]}$. Equation 14 describes the &lt;em&gt;linear&lt;/em&gt; part of the filter. The &lt;em&gt;activation&lt;/em&gt; part is captured by Equation 15. The &lt;em&gt;activation function&lt;/em&gt;, $g^{[l]}$, is often a ReLU.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;As before, we can combine Equations 14 and 15 into a single equation (see Equation 16).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
$$ \Large
\mathbf{a}^{[l]} = g^{[l]}(\mathbf{W}^{[l]} \mathbf{a}^{[l-1]} + \mathbf{b}^{[l]}) \qquad\qquad\qquad\qquad\,(16)
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;If we make use of mini-batch training, the dimensions of $\mathbf{a}^{[l]}$ will expand to $n^{[l]} \times m$ where $m$ is the mini-batch size. In this case $\mathbf{a}^{[l]}$ becomes a matrix $\mathbf{A}^{[l]}$ so that we have&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
$$ \Large
\mathbf{A}^{[l]} = g^{[l]}(\mathbf{W}^{[l]} \mathbf{A}^{[l-1]} + \mathbf{b}^{[l]}) \qquad\qquad\qquad\qquad(17)
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;4.1.4.4-Residual-Networks&quot;&gt;4.1.4.4 Residual Networks&lt;a class=&quot;anchor-link&quot; href=&quot;#4.1.4.4-Residual-Networks&quot;&gt; &lt;/a&gt;&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;He et al. (2016) provides an impressive and state-of-the-art architecture to improve the performance of CNNs even more. This architecture is called a &lt;em&gt;residual network&lt;/em&gt;, or a &lt;em&gt;ResNet&lt;/em&gt;. &lt;a href=&quot;https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8&quot;&gt;Sik-Ho Tsang&lt;/a&gt; (2018) has a blog that presents some of the details in a more digestible form.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;A ResNet layer or block is developed as follows. As before, for a convolution layer, we have:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
$$ \Large
\begin{align}
\mathbf{Z}^{[l]} &amp;amp;= \mathbf{W}^{[l]} \ast \mathbf{A}^{[l-1]} + \mathbf{b}^{[l]} \\
\mathbf{A}^{[l]} &amp;amp;= g^{[l]}(\mathbf{Z}^{[l]})
\end{align}
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Let’s add 1 to the index values for the purpose of deriving:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
$$ \Large
\begin{align}
\mathbf{Z}^{[l+1]} &amp;amp;= \mathbf{W}^{[l+1]} \ast \mathbf{A}^{[l]} + \mathbf{b}^{[l+1]} \qquad(18) \\
\mathbf{A}^{[l+1]} &amp;amp;= g^{[l+1]}(\mathbf{Z}^{[l+1]}) \qquad\qquad\qquad\qquad\,(19)
\end{align}
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The next layer will then be:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
$$ \Large
\begin{align}
\mathbf{Z}^{[l+2]} &amp;amp;= \mathbf{W}^{[l+2]} \ast \mathbf{A}^{[l+1]} + \mathbf{b}^{[l+2]} \qquad(20) \\
\mathbf{A}^{[l+2]} &amp;amp;= g^{[l+2]}(\mathbf{Z}^{[l+2]}) \qquad\qquad\qquad\qquad\quad(21)
\end{align}
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Now comes the crucial step. We feed the activations $\mathbf{A}^{[l]}$ in Equation 18 &lt;em&gt;forward&lt;/em&gt; by means of a &lt;em&gt;skip-connection&lt;/em&gt; (also called a &lt;em&gt;short-circuit-connection&lt;/em&gt;) and add them to $\mathbf{Z}^{[l+2]}$ in Equation 20. This means Equation 21 now becomes:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
$$ \Large
\mathbf{A}^{[l+2]} = g^{[l+2]}(\mathbf{Z}^{[l+2]} + \mathbf{A}^{[l]})
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The ResNet block is therefore described by the equations:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
$$ \Large
\begin{align}
\mathbf{Z}^{[l+2]} &amp;amp;= \mathbf{W}^{[l+2]} \ast \mathbf{A}^{[l+1]} + \mathbf{b}^{[l+2]} \qquad(22) \\
\mathbf{A}^{[l+2]} &amp;amp;= g^{[l+2]}(\mathbf{Z}^{[l+2]} + \mathbf{A}^{[l]}) \qquad\qquad\quad(23)
\end{align}
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Expressed as a single equation we have:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
$$ \Large
\mathbf{A}^{[l+2]} = g^{[l+2]}(\mathbf{W}^{[l+2]} \ast \mathbf{A}^{[l+1]} + \mathbf{b}^{[l+2]} + \mathbf{A}^{[l]})\quad(24)
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Adjusting the indexes again, we have&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
$$ \Large
\mathbf{A}^{[l]} = g^{[l]}(\mathbf{W}^{[l]} \ast \mathbf{A}^{[l-1]} + \mathbf{b}^{[l]} + \mathbf{A}^{[l-2]})\qquad(25)
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The ResNet architecture allows for much deeper networks than before by stacking ResNet blocks as deep as needed. In theory, as the number of layers in a traditional deep neural network increases, the training error should keep on decreasing. The reality is that when the network gets too deep, the training error actually starts to increase again. ResNets rescue this situation, allowing the training error to keep on falling even with the number of layers approaching one thousand.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Next we will discuss all the classification models used in this paper. All of them make use of a 50-layer ResNet architecture.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;4.2-CLASSIFICATION-MODELS&quot;&gt;4.2 CLASSIFICATION MODELS&lt;a class=&quot;anchor-link&quot; href=&quot;#4.2-CLASSIFICATION-MODELS&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The purpose of the analytics pipeline is to separate useful (called typical) flight paths from non-useful (called non-typical/anomalous) ones. As pointed out earlier, a typical flight profile, in this context, means it is conducive to certain types of analyses. The main requirements for usefulness is that the flight profile will be fully developed, and have a canonical and extended cruise segment (see Figure 1.1). All other flight paths will be considered less useful/non-useful/anomalous for our purposes. Non-usefulness is often due to insignificant (i.e. too short) cruise segments and missing data (see Figure 1.2). Note that in visualizations and in the code, typical (useful) profiles are labeled as &lt;em&gt;typ&lt;/em&gt;, while non-typical (non-useful) profiles will be labeled as &lt;em&gt;non&lt;/em&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We will need a number of classification models to arrive at the objectives described above. Each model will select the useful flight profiles from its input data and pass them on to the next model in the pipeline. The output from the last model will represent a useful dataset for post-flight analysis as described in this paper. The pipeline is defined as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Stage III: Developed/Non-developed Model&lt;/li&gt;
&lt;li&gt;Stage  II: Canonicity of Segments Model&lt;/li&gt;
&lt;li&gt;Stage   I: Extended/Short Cruises Model&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Note that we count &lt;em&gt;down&lt;/em&gt;, starting from Stage III which processes the rawest data. Stage I is the final stage that provides the usable data for the analyst.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;4.2.1-Stage-III:-Developed/Non-developed-Model-(mod3)&quot;&gt;4.2.1 Stage III: Developed/Non-developed Model (mod3)&lt;a class=&quot;anchor-link&quot; href=&quot;#4.2.1-Stage-III:-Developed/Non-developed-Model-(mod3)&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The input datapoints for this model are all the flight profile images that was prepared by the data preparation step. In the software, this model is called &lt;em&gt;mod3&lt;/em&gt;. The purpose of the model is to binary-classify flight profiles into &lt;em&gt;developed&lt;/em&gt; profiles (labeled &lt;em&gt;typ&lt;/em&gt;) and &lt;em&gt;non-developed&lt;/em&gt; profiles (labeled &lt;em&gt;non&lt;/em&gt;). A developed profile is complete in that it contains a &lt;em&gt;climb segment&lt;/em&gt;, a &lt;em&gt;cruise segment&lt;/em&gt; (even if it approaches a duration of zero minutes), and a &lt;em&gt;descent segment&lt;/em&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The Stage III model was chosen to investigate the performance of the range of transformation techniques described earlier. The main reason for this is that this stage has the most data available at its input (1,080 flight profiles, of which 993 are typical and 87 non-typical). This dataset was labeled by hand (using labels &lt;em&gt;typ&lt;/em&gt; and &lt;em&gt;non&lt;/em&gt;), and making use of the altitude line plots produced as described in section 4.1.3.1. On the filesystem, the .png files are in the following folders (Appendix A contains the complete filesystem layout):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;dashlink&lt;ul&gt;
&lt;li&gt;Train&lt;ul&gt;
&lt;li&gt;png3 [‘3’ refers to mod3]&lt;ul&gt;
&lt;li&gt;non [non-typical png files]&lt;/li&gt;
&lt;li&gt;typ [typical png files]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;There is a separate sub-model for each of the transformation techniques (mod3a, mod3b, mod3c, mod3d, mod3e, and mod3f). Each of the sub-models performs the same binary classification between developed and non-developed profiles. The only difference is in the way the input time-series was transformed into an image. Table 4‑1 provides a summary.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;em&gt;Table 4-1 Mapping between transformation technique and Stage III sub-model&lt;/em&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;Transformation Technique&lt;/th&gt;
&lt;th&gt;Stage III sub-model&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Line plots&lt;/td&gt;
&lt;td&gt;mod3a&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Area plots&lt;/td&gt;
&lt;td&gt;mod3f&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Gramian Angular Summation Field (GASF)&lt;/td&gt;
&lt;td&gt;mod3b&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Gramian Angular Difference Field (GADF)&lt;/td&gt;
&lt;td&gt;mod3c&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Markov Transition Field (MTF)&lt;/td&gt;
&lt;td&gt;mod3d&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Recurrence Plot (RP)&lt;/td&gt;
&lt;td&gt;mod3e&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The labeled training data for each sub-model appears in its own folder:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ul&gt;
&lt;li&gt;dashlink&lt;ul&gt;
&lt;li&gt;Train&lt;ul&gt;
&lt;li&gt;png3a&lt;ul&gt;
&lt;li&gt;non&lt;/li&gt;
&lt;li&gt;typ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;png3b&lt;ul&gt;
&lt;li&gt;non&lt;/li&gt;
&lt;li&gt;typ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;png3c&lt;ul&gt;
&lt;li&gt;non&lt;/li&gt;
&lt;li&gt;typ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;png3d&lt;ul&gt;
&lt;li&gt;non&lt;/li&gt;
&lt;li&gt;typ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;png3f&lt;ul&gt;
&lt;li&gt;non&lt;/li&gt;
&lt;li&gt;typ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The code for the sub-models are present in Python notebooks (click on a notebook to access the code):&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ul&gt;
&lt;li&gt;dashlink&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://nbviewer.jupyter.org/github/kobus78/dashlink/blob/master/30_mod3a.ipynb&quot;&gt;30_mod3a.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://nbviewer.jupyter.org/github/kobus78/dashlink/blob/master/30_mod3b.ipynb&quot;&gt;30_mod3b.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://nbviewer.jupyter.org/github/kobus78/dashlink/blob/master/30_mod3c.ipynb&quot;&gt;30_mod3c.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://nbviewer.jupyter.org/github/kobus78/dashlink/blob/master/30_mod3d.ipynb&quot;&gt;30_mod3d.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://nbviewer.jupyter.org/github/kobus78/dashlink/blob/master/30_mod3e.ipynb&quot;&gt;30_mod3e.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://nbviewer.jupyter.org/github/kobus78/dashlink/blob/master/30_mod3f.ipynb&quot;&gt;30_mod3f.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Each sub-model’s notebook performs the following steps:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ul&gt;
&lt;li&gt;Ingest data&lt;ul&gt;
&lt;li&gt;Form item list&lt;/li&gt;
&lt;li&gt;Form train and validation item lists&lt;ul&gt;
&lt;li&gt;Train list is 80% of data&lt;/li&gt;
&lt;li&gt;Validation list is 20% of data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Form label lists&lt;ul&gt;
&lt;li&gt;All data is labeled based on the source file’s location in either the &lt;em&gt;non&lt;/em&gt;, or the &lt;em&gt;typ&lt;/em&gt; folder&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Transform data by resizing the images from 130x133 to 128x128&lt;/li&gt;
&lt;li&gt;Set batch size to 32&lt;/li&gt;
&lt;li&gt;Normalize data using the ImageNet statistics (for transfer learning)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Train model&lt;ul&gt;
&lt;li&gt;Create a learner with the data and a ResNet-50 architecture&lt;/li&gt;
&lt;li&gt;Plot the Loss vs the Learning Rate that allows selection of the frozen learning rate by inspection &lt;/li&gt;
&lt;li&gt;Fit the learner’s model to the data using the frozen learning rate&lt;/li&gt;
&lt;li&gt;Plot the Train and Validation Loss vs the number of Batches&lt;/li&gt;
&lt;li&gt;Plot the metrics vs the number of Batches:&lt;ul&gt;
&lt;li&gt;f_beta&lt;/li&gt;
&lt;li&gt;precision&lt;/li&gt;
&lt;li&gt;recall&lt;/li&gt;
&lt;li&gt;error_rate&lt;/li&gt;
&lt;li&gt;accuracy&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Save the frozen model and iterate if necessary&lt;/li&gt;
&lt;li&gt;Unfreeze the model&lt;/li&gt;
&lt;li&gt;Plot the Loss vs the Learning Rate that allows selection of the unfrozen learning rate by inspection &lt;/li&gt;
&lt;li&gt;Fit the learner’s model to the data using the unfrozen learning rate and 10% of the frozen learning rate&lt;/li&gt;
&lt;li&gt;Plot the Train and Validation Loss vs the number of Batches&lt;/li&gt;
&lt;li&gt;Plot the metrics vs the number of Batches (as before)&lt;/li&gt;
&lt;li&gt;Save the unfrozen model and iterate if necessary&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Interpretation&lt;ul&gt;
&lt;li&gt;Plot the confusion matrix&lt;/li&gt;
&lt;li&gt;Calculate precision, recall, and F1_score&lt;/li&gt;
&lt;li&gt;Plot the top losses, i.e. data points that were predicted incorrectly&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Test Inference/Production (on train data)&lt;ul&gt;
&lt;li&gt;Export the trained, unfrozen model&lt;/li&gt;
&lt;li&gt;Pick a sample file from the “non” folder&lt;/li&gt;
&lt;li&gt;Shows this non-typical image for inspection&lt;/li&gt;
&lt;li&gt;Submit this image to the trained model to demonstrate that the output is the “non” category&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;4.2.1.1----Performance-summary-of-sub-models&quot;&gt;4.2.1.1    Performance summary of sub-models&lt;a class=&quot;anchor-link&quot; href=&quot;#4.2.1.1----Performance-summary-of-sub-models&quot;&gt; &lt;/a&gt;&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Five metrics are reported during training. Each of these is potentially effective for performance comparisons. However, in a situation of skewed classes (also known as class imbalance), precision, recall, error_rate, and accuracy become less useful. As mentioned above, out of 1,080 flight profiles, 993 are typical and 87 non-typical. This is clearly a skewed situation.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;There is also a tradeoff between precision and recall. Furthermore, it is simpler to have a single metric to base performance comparisons on. Fortunately, there is a metric that combines precision and recall in a sensible way: The $F_1$ score (Wikipedia has a good &lt;a href=&quot;https://en.wikipedia.org/wiki/F1_score&quot;&gt;write-up&lt;/a&gt;). The $F_1$ score is a special case of the $F_\beta$ score which is defined as:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
$$ \Large
F_\beta = (1+\beta^2)\cdot\frac{precision \cdot recall}{(\beta^2 \cdot precision) + recall}
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;where $\beta$ indicates how many times more important the value of recall is relative to that of precision.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;With $\beta=1$ we have:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
$$ \Large
F_1 = (1+1^2)\cdot\frac{precision \cdot recall}{(1^2 \cdot precision) + recall}
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Finally,&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
$$ \Large
F_1 = 2\cdot\frac{precision \cdot recall}{precision + recall}
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Next, we will look at the definitions of precision and recall. Table 4‑2 shows how a confusion matrix is constructed for binary classification.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;em&gt;Table 4‑2 Confusion Matrix&lt;/em&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;Confusion Matrix&lt;/th&gt;
&lt;th&gt;Predicted Negatives&lt;/th&gt;
&lt;th&gt;Predicted Positives&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Actual Negatives&lt;/td&gt;
&lt;td&gt;True Negatives (TN)&lt;/td&gt;
&lt;td&gt;False Positives (FP)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Actual Positives&lt;/td&gt;
&lt;td&gt;False Negatives (FN)&lt;/td&gt;
&lt;td&gt;True Positives (TP)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Referring to Table 4-2, the definitions of precision and recall are,&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
$$ \Large
precision = \frac{TP}{TP + FP}
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
$$ \Large
recall = \frac{TP}{TP + FN}
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;$F_1$ is also known as the balanced F-score or the harmonic mean of precision and recall. In each sub-model’s notebook, the confusion matrix is presented and below it the $F_1$ score is calculated. Table 4‑3 provides a performance comparison based on the $F_1$ scores:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;em&gt;Table 4‑3 Performance summary of sub-models&lt;/em&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;Transformation Technique&lt;/th&gt;
&lt;th&gt;Sub-model&lt;/th&gt;
&lt;th&gt;$F_1$ score&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Line plots&lt;/td&gt;
&lt;td&gt;mod3a&lt;/td&gt;
&lt;td&gt;1.000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Area plots&lt;/td&gt;
&lt;td&gt;mod3f&lt;/td&gt;
&lt;td&gt;1.000&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Gramian Angular Summation Field (GASF)&lt;/td&gt;
&lt;td&gt;mod3b&lt;/td&gt;
&lt;td&gt;0.983&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Gramian Angular Difference Field (GADF)&lt;/td&gt;
&lt;td&gt;mod3c&lt;/td&gt;
&lt;td&gt;0.983&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Markov Transition Field (MTF)&lt;/td&gt;
&lt;td&gt;mod3d&lt;/td&gt;
&lt;td&gt;0.964&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Recurrence Plot (RP)&lt;/td&gt;
&lt;td&gt;mod3e&lt;/td&gt;
&lt;td&gt;0.957&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;4.2.1.2----Selection-of-sub-model-with-best-transformation-technique&quot;&gt;4.2.1.2    Selection of sub-model with best transformation technique&lt;a class=&quot;anchor-link&quot; href=&quot;#4.2.1.2----Selection-of-sub-model-with-best-transformation-technique&quot;&gt; &lt;/a&gt;&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;It is somewhat surprising that the simplest transformations, line and area plots, provide the highest scores. It could be a situation where the more elaborate transformations are an “overkill” for the task at hand. At the same time, it is encouraging to see that a simpler transformation technique works better for the present task. But what if we over-fitted the line plot sub-model? Figure 4.8 shows the losses for the unfrozen sub-model and there is no reason to suspect an over-fit. It is also encouraging to receive a perfect score (in the case of line and area plots) which means that all 216 profiles have been classified correctly.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/fig4-8.png&quot; alt=&quot;Figure 4.8 Train &amp;amp; validation losses for the unfrozen line-plot model&quot; title=&quot;Figure 4.8 Train &amp;amp; validation losses for the unfrozen line-plot model&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Thanks to the use of transfer learning, the small training time was also impressive. From the notebook, we see that, for the selected learning rate, the total training time for the frozen model was only  seconds. The frozen model only trains the last few layers that provide customization for our purposes. For the unfrozen model the time was a mere  seconds! The unfrozen model trains the complete architecture, for fine-tuning, making use of the transferred weights from the ImageNet model. In fact, it took much longer to experimentally find a good learning rate.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;For the rest of the analytics pipeline we will &lt;em&gt;only&lt;/em&gt; use the &lt;em&gt;line plot transformation&lt;/em&gt; (the simpler transformation of the two that tied for the top score). This transformation has another advantage compared to the sophisticated techniques – it is easy to visually assess classification results. It will not be necessary to de-transform final results to visually inspect them.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;4.2.2-Anomaly-Detection-Model-(mod4)&quot;&gt;4.2.2 Anomaly Detection Model (mod4)&lt;a class=&quot;anchor-link&quot; href=&quot;#4.2.2-Anomaly-Detection-Model-(mod4)&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In a situation of significant class imbalance, it is natural to use an anomaly detection algorithm. In the area of deep learning a method that is often used is the &lt;em&gt;autoencoder&lt;/em&gt;. An autoencoder neural network is an unsupervised learning algorithm in which the target values are set to be equal to the inputs. This means that $\mathbf{y}^{(i)}=\mathbf{x}^{(i)}$. The neural network consequently learns to reproduce its own input. This arrangement can be exploited by training it only on &lt;em&gt;normal&lt;/em&gt; examples. When, after training, an &lt;em&gt;anomalous&lt;/em&gt; example is submitted, the mean square error (MSE), also called the &lt;em&gt;reconstruction error&lt;/em&gt;, is detectably higher. This higher MSE can then be used to flag anomalous datapoints. The Stanford article “&lt;a href=&quot;http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/&quot;&gt;Autoencoders&lt;/a&gt;” provides a thorough introduction.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In the previous section, we selected the line plot associated sub-model (mod3a) as the best. It will, however, be interesting to see how the performance of an autoencoder compares to that of the selected model, given the current situation of class imbalance. On the filesystem, the relevant .png files are in the folder:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ul&gt;
&lt;li&gt;dashlink&lt;ul&gt;
&lt;li&gt;Train&lt;ul&gt;
&lt;li&gt;png4a [‘4’ refers to mod4]&lt;ul&gt;
&lt;li&gt;non [non-typical/anonymous png files]&lt;/li&gt;
&lt;li&gt;typ [typical/normal png files]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The code for the anomaly detection model, mod4, is present in the Python notebook:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://nbviewer.jupyter.org/github/kobus78/dashlink/blob/master/30_mod4-2.ipynb&quot;&gt;30_mod4-2.ipynb&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Figure 4.9, Figure 4.10, and Figure 4.11 shows reconstruction efforts by the trained model from train, validation, and anomalous data, respectively (it was found that the autoencoder worked better when working with inverted images). It is clear that the reconstruction error is significantly higher in the case of the anomalous data (Figure 4.11).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/fig4-9.png&quot; alt=&quot;Figure 4.9 Reconstructed images (bottom) from train data (top)&quot; title=&quot;Figure 4.9 Reconstructed images (bottom) from train data (top)&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/fig4-10.png&quot; alt=&quot;Figure 4.10 Reconstructed images (bottom) from validation data (top)&quot; title=&quot;Figure 4.10 Reconstructed images (bottom) from validation data (top)&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/fig4-11.png&quot; alt=&quot;Figure 4.11 Reconstructed images (bottom) from anomalous data (top)&quot; title=&quot;Figure 4.11 Reconstructed images (bottom) from anomalous data (top)&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The $F_1$ score for this model (from the notebook) is 0.944. Table 4‑3 reveals that the autoencoder performs worse than all the classification sub-models in the previous section. Consequently, we will still use mod3a (as selected previously) for Stage III rather than the anomaly detection model (mod4).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;4.2.3-Stage-II:-Canonicity-of-Segments-Model-(mod2)&quot;&gt;4.2.3 Stage II: Canonicity of Segments Model (mod2)&lt;a class=&quot;anchor-link&quot; href=&quot;#4.2.3-Stage-II:-Canonicity-of-Segments-Model-(mod2)&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The input for this model (993 data points) are all the useful profile images (from &lt;em&gt;typ&lt;/em&gt;) that was output from the Stage III model (i.e. mod3a). In the software, this model is called &lt;em&gt;mod2&lt;/em&gt;. The purpose of the model is to multi-label-classify developed profiles according to the canonicity of their segments. A segment (climb/cruise/descent) is considered canonical if it is relatively smooth, i.e. does not contain steps or other vertical irregularities. Table 4‑4 shows a fragment from the &lt;em&gt;canonical-segments.csv&lt;/em&gt; file required by this model for training. Each of the 993 images were inspected and the canonicities of its segments were captured in the file under the &lt;em&gt;tags&lt;/em&gt; column. Table 4‑5 shows the encodings used.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;em&gt;Table 4‑4 Fragment from canonical-segments.csv&lt;/em&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;image_name&lt;/th&gt;
&lt;th&gt;tags&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;687200104111158-1min&lt;/td&gt;
&lt;td&gt;cl cr&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;687200104111441-1min&lt;/td&gt;
&lt;td&gt;cl cr&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;687200104120604-1min&lt;/td&gt;
&lt;td&gt;cr&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;687200104121330-1min&lt;/td&gt;
&lt;td&gt;cl cr&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;687200104130953-1min&lt;/td&gt;
&lt;td&gt;cr&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;687200104131343-1min&lt;/td&gt;
&lt;td&gt;cl cr&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;687200104131515-1min&lt;/td&gt;
&lt;td&gt;cl cr&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;687200104111637-1min&lt;/td&gt;
&lt;td&gt;cl&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;687200104120347-1min&lt;/td&gt;
&lt;td&gt;cr&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;687200104140606-1min&lt;/td&gt;
&lt;td&gt;cl cr de&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;687200104190709-1min&lt;/td&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;687200104130631-1min&lt;/td&gt;
&lt;td&gt;cl cr&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;em&gt;Table 4‑5 Codes used for canonicity of segments&lt;/em&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;Encoding&lt;/th&gt;
&lt;th&gt;Meaning&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;cl&lt;/td&gt;
&lt;td&gt;canonical climb&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;cr&lt;/td&gt;
&lt;td&gt;canonical cruise&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;de&lt;/td&gt;
&lt;td&gt;canonical descent&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;no&lt;/td&gt;
&lt;td&gt;no canonical segments&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;This arrangement is flexible for analysts with more general needs. For example, say an analyst wants to use profiles with only smooth climb segments, or say non-smooth cruises and smooth descents, whatever the need, it is easy to tweak the notebook to output the required files.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;On the filesystem, the .png files are in a single folder:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ul&gt;
&lt;li&gt;dashlink&lt;ul&gt;
&lt;li&gt;Train&lt;ul&gt;
&lt;li&gt;png2 [contains all the png files]&lt;/li&gt;
&lt;li&gt;canonical-segments.csv file&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The code for the Stage II model is present in the Python notebook:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://nbviewer.jupyter.org/github/kobus78/dashlink/blob/master/30_mod2.ipynb&quot;&gt;30_mod2.ipynb&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The model’s notebook performs similar steps to what was described in Stage III. The main difference is due to training that supports multi-label classification rather than binary classification.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Figure 4.12 shows some examples of multi-label classification from the notebook.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The best $F_1$ score that could be obtained for mod2 during training is 0.937 (compare with Table 4‑3). This is much lower than the score for mod3a (1.000). Firstly, it was trained on fewer data points (Figure 5.1). Secondly, the labeling process introduced some noise. It is somewhat subjective to decide whether, for example, a discontinuity towards the end of the cruise segment is associated with cruise itself, or, alternatively, with the beginning of descent. The model finds the classification task a bit more challenging due to this noise.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/fig4-12.png&quot; alt=&quot;Figure 4.12 Examples of multi-label classification&quot; title=&quot;Figure 4.12 Examples of multi-label classification&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;4.2.4-Stage-I:-Extended/Short-Cruises-Model-(mod1)&quot;&gt;4.2.4 Stage I: Extended/Short Cruises Model (mod1)&lt;a class=&quot;anchor-link&quot; href=&quot;#4.2.4-Stage-I:-Extended/Short-Cruises-Model-(mod1)&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The input for this model (674 data points) are all the profiles that were output from the Stage II model (i.e. mod2). These are the images of all profiles that have canonical cruise segments, regardless of the canonicity of climb or descent segments. In the software, this model is called &lt;em&gt;mod1&lt;/em&gt;. The purpose of the model is to binary-classify between extended cruise segments (labeled &lt;em&gt;typ&lt;/em&gt;) and segments with shorter cruises (labeled &lt;em&gt;non&lt;/em&gt;). Each of the 674 images were inspected and manually placed in the appropriate folder (&lt;em&gt;typ&lt;/em&gt; or &lt;em&gt;non&lt;/em&gt;).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;On the filesystem, the relevant .png files are in folders:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ul&gt;
&lt;li&gt;dashlink&lt;ul&gt;
&lt;li&gt;Train&lt;ul&gt;
&lt;li&gt;png1 [‘1’ refers to mod1]&lt;ul&gt;
&lt;li&gt;non [short cruises]&lt;/li&gt;
&lt;li&gt;typ [extended cruises]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The code for the Stage I model is present in the Python notebook:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://nbviewer.jupyter.org/github/kobus78/dashlink/blob/master/30_mod1.ipynb&quot;&gt;30_mod1.ipynb&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The model’s notebook performs the same steps as was described in Stage III.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The best $F_1$ score that could be obtained for mod1 during training is 0.947 (compare with Table 4‑3). The current model was trained on even fewer data points (Figure 5.1). In spite of that an encouraging 0.947 $F_1$ score was obtained. The labeling process again introduced some noise. It is not easy to always label the images accurately enough by making use of a purely visual inspection process. The discriminating decision is based on whether a cruise segment is extended or shorter. This is a subjective decision and an image &lt;em&gt;regression&lt;/em&gt; algorithm would have been more suitable. Labeling would have been much more laborious if regression was used, however. It would have required a visual measurement (maybe by using an electronic ruler) of the length of the cruise segment (say to the nearest millimeter on an image) to use for the label. The model finds the classification task more challenging due to this noise.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;This brings us to the end of the modeling section. We looked at the concept of time-series classification and the two major deep learning architectures used for this. Convolutional neural networks were identified as the superior architecture. To make use of CNNs, and to tap into the power of visual methods, we looked at a number of transformation techniques. The selected technique was a simple line plot of the time-series, giving us perfect classification performance on validation data. Finally, we covered the training processes of the analysis pipeline. It begins with Stage III (mod3a), then flows through Stage II (mod2), and ends with Stage I (mod1). In each stage the useful profiles are filtered out and passed on to the next stage.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;5-INFERENCE&quot;&gt;5 INFERENCE&lt;a class=&quot;anchor-link&quot; href=&quot;#5-INFERENCE&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We have finally come to the application of the models developed above. During inference (also called testing in this paper) the trained models are presented with previously unseen data points. This is the situation in production when the developed models are called upon to provide value for users.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Figure 5.1 summarizes the analytics pipeline in the &lt;em&gt;training&lt;/em&gt; mode. Compare this with Figure 5.2 which gives the same summary but in the testing or &lt;em&gt;inference&lt;/em&gt; mode. The text in each box indicates the useful profiles filtered out by that stage.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/fig5-1.png&quot; alt=&quot;Figure 5.1 Analytics pipeline for training&quot; title=&quot;Figure 5.1 Analytics pipeline for training&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/fig5-2.png&quot; alt=&quot;Figure 5.2 Analytics pipeline for testing/inference&quot; title=&quot;Figure 5.2 Analytics pipeline for testing/inference&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;5.1-TEST-DATASET&quot;&gt;5.1 TEST DATASET&lt;a class=&quot;anchor-link&quot; href=&quot;#5.1-TEST-DATASET&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The dataset for testing/inference was prepared in the same way as the dataset for training. Note that the datapoints are not labeled for inference. The size of the raw dataset is 582 flights (Figure 5.2). After preparation, the datapoints reduced to 514. The same notebook was used to prepare the test dataset (with a few configuration adjustments). The notebook is:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://nbviewer.jupyter.org/github/kobus78/dashlink/blob/master/10_mat2csv-2.ipynb&quot;&gt;10_mat2csv-2.ipynb&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;5.2-END-TO-END-INFERENCE-PROCESS&quot;&gt;5.2 END-TO-END INFERENCE PROCESS&lt;a class=&quot;anchor-link&quot; href=&quot;#5.2-END-TO-END-INFERENCE-PROCESS&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;No training happens during inference. Instead, the output of each filter is passed to the input of the next filter in the pipeline. The output of the final filter (Stage I with mod1) is a set of developed profiles that have extended canonical cruise segments, ideal for the analyses concerned with in this paper.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Each model behaves as a filter that &lt;em&gt;lets through&lt;/em&gt; all the useful profiles for the particular stage. This means&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Taking prepared raw data, &lt;em&gt;mod3a&lt;/em&gt; lets through all profiles that have been developed&lt;/li&gt;
&lt;li&gt;Taking mod3a’s output, &lt;em&gt;mod2&lt;/em&gt; lets through all profiles with canonical cruise segments&lt;/li&gt;
&lt;li&gt;Taking mod2’s output, &lt;em&gt;mod1&lt;/em&gt; lets through all profiles with extended cruise segments&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;5.2.1-Stage-III-Filter-(mod3a)&quot;&gt;5.2.1 Stage III Filter (mod3a)&lt;a class=&quot;anchor-link&quot; href=&quot;#5.2.1-Stage-III-Filter-(mod3a)&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The initial filter (mod3a) was presented with 514 test data points from the folder:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;dashlink&lt;ul&gt;
&lt;li&gt;Test&lt;ul&gt;
&lt;li&gt;png3 [contains all the png files]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The code for the Stage III filter is present in Python notebook:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://nbviewer.jupyter.org/github/kobus78/dashlink/blob/master/40_inf3.ipynb&quot;&gt;40_inf3.ipynb&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;It is interesting to scroll through the two listings towards the end of the notebook. The first one shows the images for all the useful profiles, the second one shows the same for all the non-useful profiles. These lists of images give an idea of the classification performance.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;5.2.2-Stage-II-Filter-(mod2)&quot;&gt;5.2.2 Stage II Filter (mod2)&lt;a class=&quot;anchor-link&quot; href=&quot;#5.2.2-Stage-II-Filter-(mod2)&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The second filter (mod2) was presented with 481 test data points from the folder:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;dashlink&lt;ul&gt;
&lt;li&gt;Test&lt;ul&gt;
&lt;li&gt;png2 [contains all the png files]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The code for the Stage II filter is present in Python notebook:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://nbviewer.jupyter.org/github/kobus78/dashlink/blob/master/40_inf2.ipynb&quot;&gt;40_inf2.ipynb&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Once again, the useful/non-useful profiles may be inspected.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;5.2.3-Stage-I-Filter-(mod1)&quot;&gt;5.2.3 Stage I Filter (mod1)&lt;a class=&quot;anchor-link&quot; href=&quot;#5.2.3-Stage-I-Filter-(mod1)&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The final filter (mod1) was presented with 316 test data points from the folder:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;dashlink&lt;ul&gt;
&lt;li&gt;Test&lt;ul&gt;
&lt;li&gt;png1 [contains all the png files]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The code for the Stage I filter is present in Python notebook:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://nbviewer.jupyter.org/github/kobus78/dashlink/blob/master/40_inf1.ipynb&quot;&gt;40_inf1.ipynb&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Again, the useful/non-useful profiles may be inspected. This being the final results of the complete analytics pipeline, we see that the output consists of 26 useful profiles. They are all developed and have canonical and extended cruise segments, as we set out to find. There were 290 non-useful, or less useful, profiles in the final stage. It is immediately obvious how much time might be saved by using the analytics pipeline rather than having to inspect all 582 raw profiles manually to arrive at the 26 useful ones. There will be, no doubt, some miss-classifications too. This is unavoidable as the error rate of each stage contributes towards the pipeline’s overall error rate.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In this section, we looked at how the analytics pipeline developed in this paper might be used in a production setting to provide value for analysts. A test dataset was prepared. None of its data has ever been seen by any of the models during training. This is the normal situation in a production system. By looking at the classification results (in the form of images) from the notebooks, it is interesting to see to what extent the use of this pipeline might be helpful for analysts.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;6-CONCLUSIONS-&amp;amp;-RECOMMENDATIONS&quot;&gt;6 CONCLUSIONS &amp;amp; RECOMMENDATIONS&lt;a class=&quot;anchor-link&quot; href=&quot;#6-CONCLUSIONS-&amp;amp;-RECOMMENDATIONS&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We have demonstrated how flight profile time-series can be turned into images for more effective classification. Then we identified the best technique to transform a profile time-series into an image for use by the classification process. Using transfer learning, we showed how quickly a deep learning model could be trained. We conclude that the need for hand classification of flight profiles has been reduced greatly leading to significant time savings potential for post-flight analysts. Making use of a publicly available rich flight data set we hope this work will encourage ordinary data scientists (which do not have access to company flight data) as well as post-flight analysts (that do have access) to undertake studies in this important area.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We recommend that interested analysts take this work as a starting point and adapt it to suit their needs. This may even involve changing the technology stack, for example, making use of other deep learning libraries and a different programming language. Here we used the fastai Python library (built on top of PyTorch) and the Python language. There are a number of other useful technology environments, e.g. Java, TensorFlow, Julia, and MATLAB.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We suggest that analysts need not shy away from the use of deep learning for post-flight analysis. The use of transfer learning makes the training of deep learning models very tractable. In our case, transfer learning was based on the ImageNet model which was trained on over 14 million images to classify them into more than 20,000 categories. There are many cloud providers offering the use of GPUs (Graphical Processing Units), ideal for the training process. GPUs are not necessary for inference. Even without access to a GPU, the training process is still tractable on an ordinary laptop. This is the beauty of transfer learning.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;7-SUMMARY&quot;&gt;7 SUMMARY&lt;a class=&quot;anchor-link&quot; href=&quot;#7-SUMMARY&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In this work, we:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Performed a comparison of a number of transformation techniques in terms of their associated image classification performance. We applied each transformation technique to the cleaned time-series dataset in turn, trained a CNN to do classification (using supervised learning), and recorded the performance. Then we selected the most performant transformation technique and used it in the rest of the analysis pipeline. The following transformation techniques were considered:&lt;ul&gt;
&lt;li&gt;Altitude line plots transformed into an image&lt;/li&gt;
&lt;li&gt;Altitude area plots transformed into an image&lt;/li&gt;
&lt;li&gt;Gramian Angular Summation Field (GASF)&lt;/li&gt;
&lt;li&gt;Gramian Angular Difference Field (GADF)&lt;/li&gt;
&lt;li&gt;Markov Transition Field (MTF)&lt;/li&gt;
&lt;li&gt;Recurrence Plot (RP)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Trained a model to classify flight profiles into developed (useful) and non-developed (non-useful) profiles. We also considered the use of anomaly detection by means of an autoencoder (instead of a classification algorithm) due to the significant class imbalance and concluded that the autoencoder did not work as well as the classifier.&lt;/li&gt;
&lt;li&gt;Trained a model to do multi-label classification of developed profiles. The labels reflected whether a profile had a canonical climb/cruise/descent segment.&lt;/li&gt;
&lt;li&gt;Trained a model to classify flight profiles with canonical cruise segments (regardless of the properties of climb or descent segments) into profiles that have extended cruises (useful) and shorter cruises (non-useful).&lt;/li&gt;
&lt;li&gt;Prepared a significant test dataset, consisting of datapoints that have never been seen by any of the models and have not been labeled. We constructed an end-to-end analytic inference process to simulate a production system and applied it to the test dataset. Finally, we made recommendations to post-flight and other interested analysts.&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;8-FURTHER-EXPERIMENTATION&quot;&gt;8 FURTHER EXPERIMENTATION&lt;a class=&quot;anchor-link&quot; href=&quot;#8-FURTHER-EXPERIMENTATION&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;There are a good number of hyper-parameters that may be adjusted leading to further experiments, for example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fraction of train data dedicated for validation (20% here)&lt;/li&gt;
&lt;li&gt;The batch size during training (32 for mod3a and 8 for mod2 and mod1)&lt;/li&gt;
&lt;li&gt;Images were resized to 128 x 128. A technique that holds promise is to first down-sample images drastically, say to 32 x 32. Then, after training, transfer learning is used while progressively up-sampling again.&lt;/li&gt;
&lt;li&gt;Learning rates. This is arguably the most influential hyper-parameter during training. It may be worthwhile to adjust the used learning rates, (both for the frozen learning rate, &lt;em&gt;lrf&lt;/em&gt;, as well as the unfrozen learning rate, &lt;em&gt;lru&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We used data from a single airplane in this study. It may be worthwhile to analyze the datasets available and put together a dataset that represents a number of different aircraft. Care should be taken, however, because the identity and model of aircraft are deliberately omitted. This means the analyst might end up with data from a 747 being mixed with that of a Lear Jet, or even a Cessna, probably not all that useful.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;No &lt;em&gt;data augmentation&lt;/em&gt; was used during training. It should be possible to generate more data by flipping images horizontally for mod3a and mod1. This will not work for mod2 due to the implied change of the segment types. A small amount of zooming might also be tried.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The CNN architecture used throughout was ResNet-50. We believe ResNet is the current state-of-the-art but there are other promising architectures, i.e. the Inception network. If an experimenter is challenged in terms of compute-resources, the ResNet-50 can be scaled down, e.g. to ResNet-34 or ResNet-18.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In the case of the autoencoder, a &lt;em&gt;linear&lt;/em&gt; autoencoder was used. Better results might be obtained if a &lt;em&gt;convolutional&lt;/em&gt; autoencoder is used instead. The code for a convolutional autoencoder is included in the anomaly detection notebook.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Finally, we need to mention that, although this paper focused on only using the altitude time-series of a flight, there are many more variables to explore. As mentioned, our data source makes 186 variables available. A few simple explorations are undertaken in the Python notebook:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://nbviewer.jupyter.org/github/kobus78/dashlink/blob/master/20_eda1.ipynb&quot;&gt;20_eda1.ipynb&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Something interesting that might be tried is to combine a number of normalized variables (to allow for a single vertical scale) on a single line plot, each in a different color. Deep learning may then be used to train for the identification of normal versus anomalous situations.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;REFERENCES&quot;&gt;REFERENCES&lt;a class=&quot;anchor-link&quot; href=&quot;#REFERENCES&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Autoencoders. (n.d.). Retrieved September 28, 2019, from &lt;a href=&quot;http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/&quot;&gt;http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Automatic dependent surveillance – broadcast. (n.d.). In Wikipedia. Retrieved September 19, 2019, from   &lt;a href=&quot;http://en.wikipedia.org/wiki/https://en.wikipedia.org/wiki/Automatic_dependent_surveillance_%E2%80%93_broadcast&quot;&gt;http://en.wikipedia.org/wiki/https://en.wikipedia.org/wiki/Automatic_dependent_surveillance_%E2%80%93_broadcast&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Bagnall, A., Lines, J., Bostrom, A., Large, J., &amp;amp; Keogh, E. (2017). The great time series classification bake off: a review and experimental evaluation of recent algorithmic advances. Data Mining and Knowledge Discovery, 31(3), 606-660.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Bagnall, A., Lines, J., Hills, J., &amp;amp; Bostrom, A. (2016). Time-series classification with COTE: The collective of transformation-based ensembles. International Conference on Data Engineering, pp 1548-1549.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Culurciell, E. (2018). The fall of RNN / LSTM. [Weblog]. Retrieved from 
&lt;a href=&quot;https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0&quot;&gt;https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Esling, P., &amp;amp; Agon, C. (2012). Time-series data mining. ACM Computing Surveys, 45(1), 12:1-12:34.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Fawaz, H. I., Forestier, G., Weber, J., Idoumghar, L., &amp;amp; Muller, P. (2019). Deep learning for time series classification: a review. Data Mining and Knowledge Discovery, 33, 917. &lt;a href=&quot;https://doi.org/10.1007/s10618-019-00619-1&quot;&gt;https://doi.org/10.1007/s10618-019-00619-1&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;F1 score. (n.d.). In Wikipedia. Retrieved September 27, 2019, from &lt;a href=&quot;https://en.wikipedia.org/wiki/F1_score&quot;&gt;https://en.wikipedia.org/wiki/F1_score&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;He, K., Zhang, X., Ren, S., &amp;amp; Sun, J. (2016). Deep Residual Learning for Image Recognition. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, 2016, pp. 770-778. doi: 10.1109/CVPR.2016.90&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Hatami, N., Gavet, Y., &amp;amp; Debayle, J. (2017). Classification of time-series images using deep convolutional neural networks. International Conference on Machine Vision.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Hüsken, M., &amp;amp; Stagge, P. (2003). Recurrent neural networks for time series classification. Neurocomputing, 50, 223-235. Retrieved from &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0925231201007068?via=ihub&quot;&gt;https://www.sciencedirect.com/science/article/pii/S0925231201007068?via=ihub&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Karpathy, A. (2015). The Unreasonable Effectiveness of Recurrent Neural Networks. [Weblog]. Retrieved from &lt;a href=&quot;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&quot;&gt;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Krizhevsky, A., Sutskever, I., &amp;amp; Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. In NIPS, 2012.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Krizhevsky, A., Sutskever, I., &amp;amp; Hinton, G. E. (2017). ImageNet classification with deep convolutional neural networks. Commun. ACM, 60(6), 84-90. DOI: &lt;a href=&quot;https://doi.org/10.1145/3065386&quot;&gt;https://doi.org/10.1145/3065386&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Lines, J., Taylor, S., &amp;amp; Bagnall, A. (2016). HIVE-COTE: The hierarchical vote collective of transformation based ensembles for time series classification. IEEE International Conference on Data Mining, pp 1041-1046.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Lines, J., Taylor, S., &amp;amp; Bagnall, A. (2018). Time series classification with HIVE-COTE: The hierarchical vote collective of transformation-based ensembles. ACM Transactions on Knowledge Discovery from Data, 12(5), 52:1-52:35.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Rabinowitz, J. (2017). This Is How Flight Tracking Sites Work. Retrieved from &lt;a href=&quot;https://thepointsguy.com/2017/09/how-flight-tracking-sites-work/&quot;&gt;https://thepointsguy.com/2017/09/how-flight-tracking-sites-work/&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Skalski, P. (2019). Gentle Dive into Math Behind Convolutional Neural Networks. [Weblog]. Retrieved from &lt;a href=&quot;https://towardsdatascience.com/gentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9&quot;&gt;https://towardsdatascience.com/gentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Sun, Z., Di, L. &amp;amp; Fang, H. (2019). Using long short-term memory recurrent neural network in land cover classification on Landsat and Cropland data layer time series. International Journal of Remote Sensing, 40(2), 593-614. DOI: 10.1080/01431161.2018.1516313. Retrieved from &lt;a href=&quot;https://www.tandfonline.com/doi/abs/10.1080/01431161.2018.1516313&quot;&gt;https://www.tandfonline.com/doi/abs/10.1080/01431161.2018.1516313&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Tripathy, R. K., &amp;amp; Acharya, U. R. (2018). Use of features from RR-time series and EEG signals for automated classification of sleep stages in deep neural network framework. Biocybernetics and Biomedical Engineering, 38, 890-902.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Tsang, S. (2018). Review: ResNet — Winner of ILSVRC 2015 (Image Classification, Localization, Detection). [Weblog]. Retrieved from &lt;a href=&quot;https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8&quot;&gt;https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Wang, Z., Yan, W., &amp;amp; Oates, T. (2017). Time series classification from scratch with deep neural networks: A strong baseline. 2017 International Joint Conference on Neural Networks (IJCNN), 1578-1585.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Wang, Z., &amp;amp; Oates, T. (2015a). Encoding Time Series as Images for Visual Inspection and Classification Using Tiled Convolutional Neural Networks. Trajectory-Based Behavior Analytics: Papers from the 2015 AAAI Workshop. Retrieved from &lt;a href=&quot;https://aaai.org/ocs/index.php/WS/AAAIW15/paper/viewFile/10179/10251&quot;&gt;https://aaai.org/ocs/index.php/WS/AAAIW15/paper/viewFile/10179/10251&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Wang, Z., &amp;amp; Oates, T. (2015b). Imaging time-series to improve classification and imputation. International Conference on Artificial Intelligence, pp 3939-3945.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Wang, Z., &amp;amp; Oates, T. (2015c). Spatially Encoding Temporal Correlations to Classify Temporal Data Using Convolutional Neural Networks. ArXiv, abs/1509.07481.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., … Bengio, Y. (2015). Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. Proceedings of the 32nd International Conference on Machine Learning, in PMLR, 37, 2048-2057.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Yang, Q., Wu, X. (2006). 10 challenging problems in data mining research. Information Technology &amp;amp; Decision Making, 05(04), 597-604.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;APPENDICES&quot;&gt;APPENDICES&lt;a class=&quot;anchor-link&quot; href=&quot;#APPENDICES&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Appendix-A:-Filesystem-Layout&quot;&gt;Appendix A: Filesystem Layout&lt;a class=&quot;anchor-link&quot; href=&quot;#Appendix-A:-Filesystem-Layout&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ul&gt;
&lt;li&gt;dashlink&lt;ul&gt;
&lt;li&gt;Train&lt;ul&gt;
&lt;li&gt;1min&lt;ul&gt;
&lt;li&gt;.csv files&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;png3&lt;ul&gt;
&lt;li&gt;non&lt;/li&gt;
&lt;li&gt;typ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;png3a&lt;ul&gt;
&lt;li&gt;non&lt;/li&gt;
&lt;li&gt;typ&lt;/li&gt;
&lt;li&gt;export.pkl file&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;png3b&lt;ul&gt;
&lt;li&gt;non&lt;/li&gt;
&lt;li&gt;typ&lt;/li&gt;
&lt;li&gt;export.pkl file&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;png3c&lt;ul&gt;
&lt;li&gt;non&lt;/li&gt;
&lt;li&gt;typ&lt;/li&gt;
&lt;li&gt;export.pkl file&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;png3d&lt;ul&gt;
&lt;li&gt;non&lt;/li&gt;
&lt;li&gt;typ&lt;/li&gt;
&lt;li&gt;export.pkl file&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;png3e&lt;ul&gt;
&lt;li&gt;non&lt;/li&gt;
&lt;li&gt;typ&lt;/li&gt;
&lt;li&gt;export.pkl file&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;png3f&lt;ul&gt;
&lt;li&gt;non&lt;/li&gt;
&lt;li&gt;typ&lt;/li&gt;
&lt;li&gt;export.pkl file&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;png2&lt;ul&gt;
&lt;li&gt;.png files&lt;/li&gt;
&lt;li&gt;canonical-segments.csv file&lt;/li&gt;
&lt;li&gt;export.pkl file&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;png1&lt;ul&gt;
&lt;li&gt;non&lt;/li&gt;
&lt;li&gt;typ&lt;/li&gt;
&lt;li&gt;export.pkl file&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Test&lt;ul&gt;
&lt;li&gt;png3&lt;ul&gt;
&lt;li&gt;non&lt;/li&gt;
&lt;li&gt;typ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;src3&lt;ul&gt;
&lt;li&gt;.png files&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;png2&lt;ul&gt;
&lt;li&gt;non&lt;/li&gt;
&lt;li&gt;typ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;png1&lt;ul&gt;
&lt;li&gt;non&lt;/li&gt;
&lt;li&gt;typ&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;10_mat2csv.ipynb&lt;/li&gt;
&lt;li&gt;10_mat2csv-2.ipynb&lt;/li&gt;
&lt;li&gt;10_csv2png-3.ipynb&lt;/li&gt;
&lt;li&gt;20_eda1.ipynb&lt;/li&gt;
&lt;li&gt;30_mod3a.ipynb&lt;/li&gt;
&lt;li&gt;30_mod3b.ipynb&lt;/li&gt;
&lt;li&gt;30_mod3c.ipynb&lt;/li&gt;
&lt;li&gt;30_mod3d.ipynb&lt;/li&gt;
&lt;li&gt;30_mod3e.ipynb&lt;/li&gt;
&lt;li&gt;30_mod3f.ipynb&lt;/li&gt;
&lt;li&gt;30_mod4-2.ipynb&lt;/li&gt;
&lt;li&gt;30_mod2.ipynb&lt;/li&gt;
&lt;li&gt;30_mod1.ipynb&lt;/li&gt;
&lt;li&gt;40_inf3.ipynb&lt;/li&gt;
&lt;li&gt;40_inf2.ipynb&lt;/li&gt;
&lt;li&gt;40_inf1.ipynb&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;NOTE: The Python notebook files (.ipynb) have a prefix that indicates the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;10_ for data preparation&lt;/li&gt;
&lt;li&gt;20_ for exploratory data analysis&lt;/li&gt;
&lt;li&gt;30_ for modeling and training&lt;/li&gt;
&lt;li&gt;40_ for inference and testing&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><category term="Airline Industry" /><category term="Deep Learning" /><category term="Time series" /><category term="Image Classification" /><category term="CNN" /><category term="RNN" /><category term="Python" /><category term="fastai" /><category term="Matlab" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/portfolio/images/fig5-1.png" /><media:content medium="image" url="/portfolio/images/fig5-1.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Real-time Capture of Airline Data from OpenSky</title><link href="/portfolio/airline%20industry/ads-b/api/python/urllib/2019/04/22/OpenSky.html" rel="alternate" type="text/html" title="Real-time Capture of Airline Data from OpenSky" /><published>2019-04-22T00:00:00-05:00</published><updated>2019-04-22T00:00:00-05:00</updated><id>/portfolio/airline%20industry/ads-b/api/python/urllib/2019/04/22/OpenSky</id><content type="html" xml:base="/portfolio/airline%20industry/ads-b/api/python/urllib/2019/04/22/OpenSky.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2019-04-22-OpenSky.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Purpose&quot;&gt;Purpose&lt;a class=&quot;anchor-link&quot; href=&quot;#Purpose&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The gradual adoption of &lt;em&gt;Automatic Dependent Surveillance – Broadcast&lt;/em&gt; (ADS–B) by airlines is leading to the wide availability of flight data in the public domain. Wikipedia gives a good overview of this technology (&quot;Automatic dependent surveillance – broadcast,&quot; n.d.). The ADS-B technology allows an aircraft to use satellite navigation to find its position. It then broadcasts this information periodically which enables ground stations to track it. This method is used as a replacement for secondary surveillance radar (SSR) and does not depend on an interrogation signal from the ground. The data that is broadcast can also update the situational awareness of other aircraft in the area.&lt;/p&gt;
&lt;p&gt;The increasing use of ADS-B has led to many flight tracking sites that publish basic flight data for consumption by the public. See &quot;This Is How Flight Tracking Sites Work&quot; (Rabinowitz, 2017). This data is relatively superficial and usually consists of a dozen or so measured quantities. Some of the more prominent players are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ADS-B Exchange at &lt;a href=&quot;https://www.adsbexchange.com/&quot;&gt;https://www.adsbexchange.com/&lt;/a&gt;  &lt;/li&gt;
&lt;li&gt;OpenSky at &lt;a href=&quot;https://opensky-network.org/&quot;&gt;https://opensky-network.org/&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;FlightAware at &lt;a href=&quot;https://flightaware.com/&quot;&gt;https://flightaware.com/&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;ADSBHub at &lt;a href=&quot;http://www.adsbhub.org/&quot;&gt;http://www.adsbhub.org/&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;planefinder at &lt;a href=&quot;https://planefinder.net/&quot;&gt;https://planefinder.net/&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;Aireon at &lt;a href=&quot;https://aireon.com/&quot;&gt;https://aireon.com/&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;flightradar24 at &lt;a href=&quot;https://www.flightradar24.com/&quot;&gt;https://www.flightradar24.com/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;RadarBox at &lt;a href=&quot;https://www.radarbox24.com/&quot;&gt;https://www.radarbox24.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This project makes use of the &lt;a href=&quot;https://opensky-network.org/&quot;&gt;OpenSky&lt;/a&gt; API to assess the usefulness of real-time capture of airline data.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Dataset-and-Variables&quot;&gt;Dataset and Variables&lt;a class=&quot;anchor-link&quot; href=&quot;#Dataset-and-Variables&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;A process was setup to capture the real-time flight data of a large number of flights from many carriers, once every 60 seconds. This activity continued for about 12 hours.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The data consists of 3,252,478 rows, each capturing 17 features. The data is prepared in the notebooks:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://nbviewer.jupyter.org/github/kobus78/OpenSky/blob/master/1_CollectRawData.ipynb&quot;&gt;1_CollectRawData.ipynb&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://nbviewer.jupyter.org/github/kobus78/OpenSky/blob/master/2_Combine.ipynb&quot;&gt;2_Combine.ipynb&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The features are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;datetime&lt;/li&gt;
&lt;li&gt;icao24&lt;/li&gt;
&lt;li&gt;callsign&lt;/li&gt;
&lt;li&gt;origin_country&lt;/li&gt;
&lt;li&gt;time_position&lt;/li&gt;
&lt;li&gt;last_contact&lt;/li&gt;
&lt;li&gt;longitude&lt;/li&gt;
&lt;li&gt;latitude&lt;/li&gt;
&lt;li&gt;baro_altitude&lt;/li&gt;
&lt;li&gt;on_ground&lt;/li&gt;
&lt;li&gt;velocity&lt;/li&gt;
&lt;li&gt;true_track&lt;/li&gt;
&lt;li&gt;vertical_rate&lt;/li&gt;
&lt;li&gt;sensors&lt;/li&gt;
&lt;li&gt;geo_altitude&lt;/li&gt;
&lt;li&gt;squawk&lt;/li&gt;
&lt;li&gt;spi&lt;/li&gt;
&lt;li&gt;position_source&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Exploratory-Data-Analysis&quot;&gt;Exploratory Data Analysis&lt;a class=&quot;anchor-link&quot; href=&quot;#Exploratory-Data-Analysis&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Some exploratory data analysis is undertaken in the notebook:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://nbviewer.jupyter.org/github/kobus78/OpenSky/blob/master/3_EDA.ipynb&quot;&gt;3_EDA.ipynb&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Conclusion&quot;&gt;Conclusion&lt;a class=&quot;anchor-link&quot; href=&quot;#Conclusion&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Even though the OpenSky API provides quite useful data, there is often a need to have more in-depth flight data for serious post-flight analyses. Such data are not provided by any of the ADS-B sources listed above.&lt;/p&gt;
&lt;p&gt;Detailed, in-depth flight data is generally unavailable to the public. There are a few sources that make de-indentified data available but usefulness varies. A few sources are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DASHlink at &lt;a href=&quot;https://c3.nasa.gov/dashlink/&quot;&gt;https://c3.nasa.gov/dashlink/&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;IATA at &lt;a href=&quot;https://www.iata.org/services/statistics/gadm/Pages/fdx.aspx&quot;&gt;https://www.iata.org/services/statistics/gadm/Pages/fdx.aspx&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;Data.gov at &lt;a href=&quot;https://www.data.gov/&quot;&gt;https://www.data.gov/&lt;/a&gt; with a search term of “ads-b”&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that these sources do not provide real-time data. An example project that makes use of the DASHlink source is:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://closedloopai.github.io/portfolio/2019/09/15/TimeSeriesClassification.html&quot;&gt;https://closedloopai.github.io/portfolio/2019/09/15/TimeSeriesClassification.html&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;References/Bibliography&quot;&gt;References/Bibliography&lt;a class=&quot;anchor-link&quot; href=&quot;#References/Bibliography&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Automatic dependent surveillance – broadcast. (n.d.). In Wikipedia. Retrieved September 19, 2019, from   &lt;a href=&quot;http://en.wikipedia.org/wiki/https://en.wikipedia.org/wiki/Automatic_dependent_surveillance_%E2%80%93_broadcast&quot;&gt;http://en.wikipedia.org/wiki/https://en.wikipedia.org/wiki/Automatic_dependent_surveillance_%E2%80%93_broadcast&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Rabinowitz, J. (2017). This Is How Flight Tracking Sites Work. Retrieved from &lt;a href=&quot;https://thepointsguy.com/2017/09/how-flight-tracking-sites-work/&quot;&gt;https://thepointsguy.com/2017/09/how-flight-tracking-sites-work/&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><category term="Airline Industry" /><category term="ADS-B" /><category term="API" /><category term="Python" /><category term="urllib" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/portfolio/images/opensky_logo.png" /><media:content medium="image" url="/portfolio/images/opensky_logo.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">NLP: Topic Modeling using LSI, LDA, and HDP</title><link href="/portfolio/nlp/topic%20modeling/network%20analysis/python/gensim/spacy/pyldavis/igraph/2019/04/06/TextMiningProject.html" rel="alternate" type="text/html" title="NLP: Topic Modeling using LSI, LDA, and HDP" /><published>2019-04-06T00:00:00-05:00</published><updated>2019-04-06T00:00:00-05:00</updated><id>/portfolio/nlp/topic%20modeling/network%20analysis/python/gensim/spacy/pyldavis/igraph/2019/04/06/TextMiningProject</id><content type="html" xml:base="/portfolio/nlp/topic%20modeling/network%20analysis/python/gensim/spacy/pyldavis/igraph/2019/04/06/TextMiningProject.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2019-04-06-TextMiningProject.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Purpose&quot;&gt;Purpose&lt;a class=&quot;anchor-link&quot; href=&quot;#Purpose&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The purpose of this pilot project is to investigate the usefulness of &lt;em&gt;topic modeling&lt;/em&gt; on a large sermon set. This set forms the basis of outreach initiatives for a Canadian non-profit. Topic modeling can be used to organize this substantial body of text. A second application is as a basis for a more intelligent form of searching.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Dataset-and-Variables&quot;&gt;Dataset and Variables&lt;a class=&quot;anchor-link&quot; href=&quot;#Dataset-and-Variables&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The complete dataset consists of about 1200 sermons which have been preached by Reverend William Branham over a time period from 1947 to 1965. Since then, all of these have been transcribed and is available in text format (see &lt;a href=&quot;http://www.messagehub.info/en/messages.do?show_en=true&quot;&gt;http://www.messagehub.info/en/messages.do?show_en=true&lt;/a&gt;)&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;From this dataset, I selected a sample of 13 sermons. This was not meant to be a representative sample. It is simply a small subset that I could lay my hands on easily and that I could familiarize myself with. I deliberately started small as this is a proof-of-concept pilot project.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Each sermon comes in its own text file with each line containing a single sentence. This was my source data and I had to take it in this form. A &lt;em&gt;descriptor&lt;/em&gt; identifies each sentence. An example of a descriptor is 2.1.c. Separated by periods the &lt;em&gt;descriptor&lt;/em&gt; consists of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Running sub-block number&lt;/li&gt;
&lt;li&gt;Sentence number&lt;/li&gt;
&lt;li&gt;Sentence type&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The sentence &lt;em&gt;type&lt;/em&gt; may be one of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;h (heading)&lt;/li&gt;
&lt;li&gt;n (normal)&lt;/li&gt;
&lt;li&gt;c (conversation)&lt;/li&gt;
&lt;li&gt;p (first line of poetry/song/hymn)&lt;/li&gt;
&lt;li&gt;q (non-first line of poetry/song/hymn)&lt;/li&gt;
&lt;li&gt;s (scripture)&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;This format is not suitable for topic analysis. Consequently, I discarded the descriptors and merged all sentences in a sermon into a single line of text. All these lines were further merged into a single .csv file called &lt;strong&gt;all_titles.csv&lt;/strong&gt;. So, each sermon comes from this file as a single text line. This allows for the potential contribution of &lt;em&gt;n-grams&lt;/em&gt; more efficiently. This means the all_titles.csv file consists of 13 lines. The file contains 223,843 words which means each sermon contains an average of 17,219 words.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Problem-Description-/-Objective&quot;&gt;Problem Description / Objective&lt;a class=&quot;anchor-link&quot; href=&quot;#Problem-Description-/-Objective&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;As noted above, the complete sermon set consists of about 1200 sermons. This is a substantial body of text to search through for specific areas of interest. The human searcher can scan the titles of course, but this is laborious and far from ideal. The central principle of &lt;em&gt;LDA topic analysis&lt;/em&gt; is that a document (sermon in this case) consists of a &lt;em&gt;distribution of topics&lt;/em&gt;. At the lower level, each topic consists of a &lt;em&gt;distribution of words&lt;/em&gt;. This is an ideal situation for my purpose. It is common for a minister to dwell on a variety of topics during a sermon. If everything works out as I envision, each sermon could be “tagged” with its top 5 topics, for example.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;If this approach works well, these higher probability topics could become the basis for categorizing and locating material. Another area of expansion could be to investigate the “evolution” of a topic over time (1947 to 1965). This is known as dynamic topic modeling.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Methodology&quot;&gt;Methodology&lt;a class=&quot;anchor-link&quot; href=&quot;#Methodology&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;I will first describe the tooling environment. Next, the pre-processing will be covered. Lastly, the topic modeling will be described.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Tooling&quot;&gt;Tooling&lt;a class=&quot;anchor-link&quot; href=&quot;#Tooling&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;I used the python language for writing the analysis code. An initial python script was prototyped for the merging of the sermon files into the final .csv file. Later, this python script was moved over to a IPython notebook called &lt;strong&gt;TextMiningProject-pre.ipynb&lt;/strong&gt;. The remainder of the analysis was performed in two more notebooks called &lt;strong&gt;TextMiningProject.ipynb&lt;/strong&gt; and &lt;strong&gt;TextMiningProject-graph.ipynb&lt;/strong&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The three notebooks can be accessed here:&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://nbviewer.jupyter.org/github/kobus78/TextMiningProject/blob/master/TextMiningProject-pre.ipynb&quot; title=&quot;TextMiningProject-pre.ipynb&quot;&gt;TextMiningProject-pre.ipynb&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://nbviewer.jupyter.org/github/kobus78/TextMiningProject/blob/master/TextMiningProject.ipynb&quot; title=&quot;TextMiningProject.ipynb&quot;&gt;TextMiningProject.ipynb&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://nbviewer.jupyter.org/github/kobus78/TextMiningProject/blob/master/TextMiningProject-graph.ipynb&quot; title=&quot;TextMiningProject-graph.ipynb&quot;&gt;TextMiningProject-graph.ipynb&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The following python packages were used:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;matplotlib&lt;/li&gt;
&lt;li&gt;gensim (for topic modeling)&lt;/li&gt;
&lt;li&gt;numpy&lt;/li&gt;
&lt;li&gt;spacy (for text pre-processing)&lt;/li&gt;
&lt;li&gt;pyLDAvis (for visualization of LDA topic model)&lt;/li&gt;
&lt;li&gt;pandas&lt;/li&gt;
&lt;li&gt;python-igraph (for network analysis)&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The pre-processing notebook executed on a local “mac-mini” computer. The topic modeling and graph modeling notebooks executed on an Ubuntu 16.01 virtual machine on a remote server. I tried to have these notebooks combined but was unsuccessful. I could not get all the topic modeling packages to execute locally. My apologies for having two notebooks for the main analysis.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Pre-processing&quot;&gt;Pre-processing&lt;a class=&quot;anchor-link&quot; href=&quot;#Pre-processing&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;As Lev Konstantinovskiy said, &quot;NLP is 80% preprocessing.&quot; (&lt;a href=&quot;https://www.linkedin.com/in/levkonst/?originalSubdomain=uk&quot;&gt;https://www.linkedin.com/in/levkonst/?originalSubdomain=uk&lt;/a&gt;)&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;Part-1&quot;&gt;Part 1&lt;a class=&quot;anchor-link&quot; href=&quot;#Part-1&quot;&gt; &lt;/a&gt;&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The first part of pre-processing is about transforming the source files (one per sermon) into a merged .csv file. This is taken care of in the notebook &lt;strong&gt;TextMiningProject-pre.ipynb&lt;/strong&gt;. Because I used only 13 sermon files, I did not think it worth the effort to batch process the files. I merely filled in one file name at a time in the notebook (variable MAIN). That input file was then ingested from the &lt;em&gt;input&lt;/em&gt; folder, processed into a single string/line, and output to the &lt;em&gt;output&lt;/em&gt; folder as a “-top” file (“top” is for topic analysis). Here is a list of the input files:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Mac-mini:input kobus$ ls
1965-0919_Thirst_ENG_15-1102-B123E1R-x.txt
1965-1031y_Leadership_ENG_17-0901-B123E1R-x.txt
1965-1121_WhatHouseWillYouBuildMe_ENG_15-1102-B123E1R-x.txt
1965-1125_TheInvisibleUnionOfTheBrideOfChrist_ENG_17-0502-B123E1R-x.txt
1965-1127b_TryingToDoGodAServiceWithoutItBeingGodsWill_ENG_15-1002-B123-x.txt
1965-1127z_IHaveHeardButNowISee_ENG_14-1102-B123E1R-x.txt
1965-1128x_GodsOnlyProvidedPlaceOfWorship_ENG_14-1101-t.txt
1965-1128z_OnTheWingsOfASnowWhiteDove_ENG_17-0501-B123-x.txt
1965-1204_TheRapture_ENG_16-1102-B123-x.txt
1965-1205_ThingsThatAreToBe_ENG_17-0203-B123E1R-x.txt
1965-1206_ModernEventsAreMadeClearByProphecy_ENG_14-0901-B123-x.txt
1965-1207_Leadership_ENG_15-0402-B123E1R-x.txt
1965-1212_Communion_ENG_12-1201-B123E1R-x.txt&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;These are the output files:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Mac-mini:output kobus$ ls
1965-0919_Thirst_ENG_15-1102-B123E1R-x-top.txt
1965-1031y_Leadership_ENG_17-0901-B123E1R-x-top.txt
1965-1121_WhatHouseWillYouBuildMe_ENG_15-1102-B123E1R-x-top.txt
1965-1125_TheInvisibleUnionOfTheBrideOfChrist_ENG_17-0502-B123E1R-x-top.txt
1965-1127b_TryingToDoGodAServiceWithoutItBeingGodsWill_ENG_15-1002-B123-x-top.txt
1965-1127z_IHaveHeardButNowISee_ENG_14-1102-B123E1R-x-top.txt
1965-1128x_GodsOnlyProvidedPlaceOfWorship_ENG_14-1101-t-top.txt
1965-1128z_OnTheWingsOfASnowWhiteDove_ENG_17-0501-B123-x-top.txt
1965-1204_TheRapture_ENG_16-1102-B123-x-top.txt
1965-1205_ThingsThatAreToBe_ENG_17-0203-B123E1R-x-top.txt
1965-1206_ModernEventsAreMadeClearByProphecy_ENG_14-0901-B123-x-top.txt
1965-1207_Leadership_ENG_15-0402-B123E1R-x-top.txt
1965-1212_Communion_ENG_12-1201-B123E1R-x-top.txt
all_titles.csv&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;With the “-top” files ready in the output folder, the next step was to merge them together with the following command:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat `ls *.txt` &amp;gt; all_titles.csv&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Next, the all_titles.csv was copied to the Ubuntu virtual machine with:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scp output/all_titles.csv proj@192.168.1.208:~/TextMiningProject/&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;Part-2&quot;&gt;Part 2&lt;a class=&quot;anchor-link&quot; href=&quot;#Part-2&quot;&gt; &lt;/a&gt;&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The second part of the pre-processing was executed from the notebook &lt;strong&gt;TextMiningProject.ipynb&lt;/strong&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h5 id=&quot;English-language-model&quot;&gt;English language model&lt;a class=&quot;anchor-link&quot; href=&quot;#English-language-model&quot;&gt; &lt;/a&gt;&lt;/h5&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The first step was to load the &lt;em&gt;spacy&lt;/em&gt; package’s English language model. The language model includes a set of common &lt;em&gt;stopwords&lt;/em&gt;. I had to tweak the default set due to a bug in the package. This allows for variations in capitalization of the default stopwords.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h5 id=&quot;Stopwords&quot;&gt;Stopwords&lt;a class=&quot;anchor-link&quot; href=&quot;#Stopwords&quot;&gt; &lt;/a&gt;&lt;/h5&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Next was the addition of my own stopwords, discovered by a process of trial-and-error. When multiple topics ended up having the same high-probability words, I sometimes added these to the stopwords. These words do not contribute to the individuality of topics and might be considered as noise. I added the following stopwords:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;my_stop_words = 
['said','Said','saying','Saying','thing','things','Thing','Things','man','day','church','Church','people','People','time', 'way','ways','Way','Ways','place','places','Place','Places','hand','age','ages','world','worlds','tonight','Tonight',
'day','days','Day','Days','brother','brothers','Brother','Brothers','sister','sisters','Sister','Sisters',
'woman','women','year','years','chapter','chapters','verse','verses','today','Today','mammy','Mammy','hand','Hand',
'prophet','prophets','Prophet','Prophets','life','Life','heart','hearts','message']&lt;/code&gt;&lt;/pre&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h5 id=&quot;Load-dataset&quot;&gt;Load dataset&lt;a class=&quot;anchor-link&quot; href=&quot;#Load-dataset&quot;&gt; &lt;/a&gt;&lt;/h5&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The dataset was loaded next. It consists of 13 text lines. Each sermon is in the form of a single line of text.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h5 id=&quot;Clean-dataset&quot;&gt;Clean dataset&lt;a class=&quot;anchor-link&quot; href=&quot;#Clean-dataset&quot;&gt; &lt;/a&gt;&lt;/h5&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;To clean the dataset the following steps were taken:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;remove stopwords&lt;/li&gt;
&lt;li&gt;keep alphanumeric tokens&lt;/li&gt;
&lt;li&gt;remove punctuation tokens&lt;/li&gt;
&lt;li&gt;remove numbers&lt;/li&gt;
&lt;li&gt;keep tokens with more than 2 characters&lt;/li&gt;
&lt;li&gt;keep tokens that are nouns&lt;/li&gt;
&lt;li&gt;keep lemmas of remaining tokens&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The cleaning took about 2 minutes.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h5 id=&quot;Bigrams&quot;&gt;Bigrams&lt;a class=&quot;anchor-link&quot; href=&quot;#Bigrams&quot;&gt; &lt;/a&gt;&lt;/h5&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;I initially thought that the use of bigrams, even trigrams and higher n-grams might be beneficial. As it turned out, I rarely noticed a bigram high-probability word coming up in a topic. I decided to drop the use of bigrams. As it is, I settled on using nouns only in the end.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h5 id=&quot;Dictionary&quot;&gt;Dictionary&lt;a class=&quot;anchor-link&quot; href=&quot;#Dictionary&quot;&gt; &lt;/a&gt;&lt;/h5&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;A dictionary was created next from the cleaned corpus. It consists of 2486 unique tokens.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h5 id=&quot;Document-Term-Matrix&quot;&gt;Document-Term-Matrix&lt;a class=&quot;anchor-link&quot; href=&quot;#Document-Term-Matrix&quot;&gt; &lt;/a&gt;&lt;/h5&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The dictionary was used to form a &lt;em&gt;document-term-matrix&lt;/em&gt; (DTM) to hold the word vectors for a &lt;em&gt;bag-of-words&lt;/em&gt; representation. I used a sparse representation consisting of a list of lists. The outer list contains the complete corpus. Each inner list contains the matrix entries for a document (sermon in this case) consisting of multiple tuples. The first entry in each tuple is the ID of the token in the dictionary. The second entry is the count of this token in the document represented by the specific inner list.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;This brings us to the end of the pre-processing. Next, I will discuss the topic modeling.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Topic-Modeling&quot;&gt;Topic Modeling&lt;a class=&quot;anchor-link&quot; href=&quot;#Topic-Modeling&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Three topic models were evaluated:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Latent Semantic Indexing (LSI) Model&lt;/li&gt;
&lt;li&gt;Latent Dirichlet Allocation (LDA) Model&lt;/li&gt;
&lt;li&gt;Hierarchical Dirichlet Process (HDP) Model&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Finding a set of topics was an intensely interactive process. The practical reason for this is that these techniques are based on unsupervised learning principles and it is usually up to the analyst to decide on how to cluster or group the data – in this case into groups of topics and groups of words. Each grouping comes in the form of a discrete probability distribution, also called topic proportions and word proportions.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Fortunately, the Hierarchical Dirichlet Process Model provides a suggestion of the optimal number of topics. I used this value (which was 20 topics) as the initial value for the other two models for the number of topics. In the end, I ended up with 13 topics for the LSI model, 14 topics for the LDA model, and 20 topics for the HDP model.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The LSI model is an older algorithm and the LDA model was developed for fix some issues with it (Blei 2012).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Findings-and-Discussion&quot;&gt;Findings and Discussion&lt;a class=&quot;anchor-link&quot; href=&quot;#Findings-and-Discussion&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Here are the titles of the sermons again to help with the placement of topics. They have been color-coded to show the association with topics (I have some familiarity with some of the sermons).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/txt_top1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The main theme/topic of the 1965-0919_Thirst sermon came out clearly in each of the models (&lt;strong&gt;highlighted in yellow&lt;/strong&gt;). It is about how a wounded &lt;em&gt;dear&lt;/em&gt; that has been chased by the &lt;em&gt;dogs&lt;/em&gt;, &lt;em&gt;thirsts&lt;/em&gt; for water and has a strong &lt;em&gt;desire&lt;/em&gt; to reach it, else it will surely die. The spiritual application is for the human &lt;em&gt;soul&lt;/em&gt; to reach out for the &lt;em&gt;water&lt;/em&gt; of life and be &lt;em&gt;enlightened&lt;/em&gt; by the &lt;em&gt;Word&lt;/em&gt; of life. I have italicized the high-probability tokens identified by the models.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The main topic of the 1965-1212_Communion sermon was also identified accurately (&lt;strong&gt;highlighted in light green&lt;/strong&gt;). The high-probability tokens were: &lt;em&gt;communion&lt;/em&gt;, &lt;em&gt;ordinance&lt;/em&gt;, &lt;em&gt;water&lt;/em&gt;, &lt;em&gt;order&lt;/em&gt;, &lt;em&gt;bread&lt;/em&gt;, &lt;em&gt;blood&lt;/em&gt;, &lt;em&gt;supper&lt;/em&gt;, &lt;em&gt;lamb&lt;/em&gt;, &lt;em&gt;sacrifice&lt;/em&gt;, &lt;em&gt;body&lt;/em&gt;, &lt;em&gt;sin&lt;/em&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The theme of the 1965-1031y_Leadership sermon is how a child grows up by being submitted to a series of leadership roles that shapes his/her life (&lt;strong&gt;highlighted in purple&lt;/strong&gt;).  The &lt;em&gt;child&lt;/em&gt; will progressively hear the &lt;em&gt;leadership voices&lt;/em&gt; of &lt;em&gt;mother&lt;/em&gt;, its &lt;em&gt;teacher&lt;/em&gt;, &lt;em&gt;nature&lt;/em&gt;, father (who happens to be a &lt;em&gt;business&lt;/em&gt; man), and eventually God’s voice in the form of &lt;em&gt;revelation&lt;/em&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The 1965-1127b_TryingToDoGodAServiceWithoutItBeingGodsWill sermon (&lt;strong&gt;highlighted in cyan&lt;/strong&gt;) relates how &lt;em&gt;king&lt;/em&gt; David had &lt;em&gt;faith&lt;/em&gt; and &lt;em&gt;inspiration&lt;/em&gt; to recover the &lt;em&gt;ark&lt;/em&gt; of the covenant from the Philistines. This gave rise to a great &lt;em&gt;revival&lt;/em&gt; among the Israelites. In the end, it turns out that he made the wrong &lt;em&gt;choice&lt;/em&gt;. It is compared with making the wrong &lt;em&gt;choice&lt;/em&gt; by trying to do God a service without it being His will. This often happens by relying on the &lt;em&gt;denominational&lt;/em&gt; church system and providing service by means of it for salvation, instead of relying on God himself.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The 1965-1128z_OnTheWingsOfASnowWhiteDove sermon tells the story of Noah (&lt;strong&gt;highlighted in grey&lt;/strong&gt;). A &lt;em&gt;dove&lt;/em&gt; was released by Noah after the flood in order to find land; it came back carrying a freshly plucked olive leaf, a &lt;em&gt;sign&lt;/em&gt; of life and &lt;em&gt;love&lt;/em&gt; after the long &lt;em&gt;night&lt;/em&gt; of the &lt;em&gt;water&lt;/em&gt; of the Flood. This &lt;em&gt;sign&lt;/em&gt; of love was given to Noah on the &lt;em&gt;wings&lt;/em&gt; of a &lt;em&gt;snow&lt;/em&gt;-white dove.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Latent-Semantic-Indexing-Model-(13-topics-across-corpus)&quot;&gt;Latent Semantic Indexing Model (13 topics across corpus)&lt;a class=&quot;anchor-link&quot; href=&quot;#Latent-Semantic-Indexing-Model-(13-topics-across-corpus)&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/txt_top2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Latent-Dirichlet-Allocation-Model-(14-topics-across-corpus)&quot;&gt;Latent Dirichlet Allocation Model (14 topics across corpus)&lt;a class=&quot;anchor-link&quot; href=&quot;#Latent-Dirichlet-Allocation-Model-(14-topics-across-corpus)&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/txt_top3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Hierarchical-Dirichlet-Process-Model-(20-topics-across-corpus)&quot;&gt;Hierarchical Dirichlet Process Model (20 topics across corpus)&lt;a class=&quot;anchor-link&quot; href=&quot;#Hierarchical-Dirichlet-Process-Model-(20-topics-across-corpus)&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/txt_top4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Visualization-of-Latent-Dirichlet-Allocation-Model&quot;&gt;Visualization of Latent Dirichlet Allocation Model&lt;a class=&quot;anchor-link&quot; href=&quot;#Visualization-of-Latent-Dirichlet-Allocation-Model&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Figure 1 shows a visualization of the findings of the Latent Dirichlet Allocation Model.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/txt_fig1.png&quot; alt=&quot;Figure 1  Visualization of the LDA Model&quot; title=&quot;Figure 1  Visualization of the LDA Model&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Visualization was performed using the python pyLDAvis package, &lt;a href=&quot;https://pypi.org/project/pyLDAvis/1.0.0/&quot;&gt;https://pypi.org/project/pyLDAvis/1.0.0/&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The main topic associated with &lt;em&gt;1965-1212_Communion&lt;/em&gt; is highlighted in red. Note that the topic numbers in the visualization are different from those where the models are printed out (I have noticed this on the web too). There are some overlaps among topics (which require further work). There is one strongest topic in the north-west and a number of tiny topics towards the east.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The main topic associated with &lt;em&gt;1965-1207_Leadership&lt;/em&gt; and &lt;em&gt;1965-1031y_Leadership&lt;/em&gt; is shown in Figure 2.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/txt_fig2.png&quot; alt=&quot;Figure 2  Main topic associated with Leadership&quot; title=&quot;Figure 2  Main topic associated with Leadership&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The main topic associated with &lt;em&gt;1965-0919_Thirst&lt;/em&gt; is shown in Figure 3.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/txt_fig3.png&quot; alt=&quot;Figure 3  Main topic associated with Thirst&quot; title=&quot;Figure 3  Main topic associated with Thirst&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The main topic associated with Noah’s dove (&lt;em&gt;1965-1128z_OnTheWingsOfASnowWhiteDove&lt;/em&gt;) is shown in Figure 4.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/txt_fig4.png&quot; alt=&quot;Figure 4  Main topic associated with Noah&amp;#39;s dove&quot; title=&quot;Figure 4  Main topic associated with Noah&amp;#39;s dove&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Comparison-of-the-3-models-using-Topic-Coherence&quot;&gt;Comparison of the 3 models using Topic Coherence&lt;a class=&quot;anchor-link&quot; href=&quot;#Comparison-of-the-3-models-using-Topic-Coherence&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;I used the technique of topic coherence to draw a comparison between the three models. This technique only works when comparing models based on the same dataset. As is evident from Figure 5, the LDA (Latent Dirichlet Allocation) model fared the best. After it came the HDP (Hierarchical Dirichlet Process) and then the LSI (Latent Semantic Indexing) model.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/txt_fig5.png&quot; alt=&quot;Figure 5  Comparison of the 3 models using Topic Coherence&quot; title=&quot;Figure 5  Comparison of the 3 models using Topic Coherence&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Network-Analysis-of-the-LSI-Topic-Model&quot;&gt;Network Analysis of the LSI Topic Model&lt;a class=&quot;anchor-link&quot; href=&quot;#Network-Analysis-of-the-LSI-Topic-Model&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;I have visualized the topic model of the LDA model above. For the visualization of a network model I will use the LSI topic model. Although this is the weakest model, it was easiest to construct its incidence matrix because it has the fewest number of topics (13). For the connection of each of the topics the top 10 words were used from the topic model. The incidence matrix is in the file &lt;strong&gt;IncidenceMatrixLSI.csv&lt;/strong&gt;. The network analysis of the LSI topic model is in the file &lt;strong&gt;TextMiningProject-graph.ipynb&lt;/strong&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;After reading in the incidence matrix, the NaN’s are replaced with zeros and the data is prepared for consumption by the python-igraph package. A bipartite graph is created with 88 vertices and 142 edges. Names are provided for the vertices from the row and column names of the incidence matrix.  Figure 6 shows the bipartite graph of the LSI topic model.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/txt_fig6.png&quot; alt=&quot;Figure 6  Bipartite graph of the LSI topic model&quot; title=&quot;Figure 6  Bipartite graph of the LSI topic model&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The red vertices represent the &lt;em&gt;topics&lt;/em&gt; and the blue vertices the &lt;em&gt;words&lt;/em&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Next, the bipartite graph was projected into a graph for topics and a graph for words. Figure 7 shows the &lt;strong&gt;topic&lt;/strong&gt; graph for the LSI topic model.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/txt_fig7.png&quot; alt=&quot;Figure 7  Topic graph for the LSI topic model&quot; title=&quot;Figure 7  Topic graph for the LSI topic model&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Figure 8 shows the &lt;strong&gt;word&lt;/strong&gt; graph for the LSI topic model.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/txt_fig8.png&quot; alt=&quot;Figure 8  Word graph for the LSI topic model&quot; title=&quot;Figure 8  Word graph for the LSI topic model&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Conclusion-and-Further-Work&quot;&gt;Conclusion and Further Work&lt;a class=&quot;anchor-link&quot; href=&quot;#Conclusion-and-Further-Work&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Overall, I am impressed by the possibilities of topic modeling. Although the models I derived were not adequate enough for my purposes, I think it might not require too much more work to make them usable. I would like to pursue this idea further and apply it in the areas of semantic search, tagging of each sermon with its highest-probability topics, and tracking of topics over time by means of dynamic topic models (Blei 2012).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;References/Bibliography&quot;&gt;References/Bibliography&lt;a class=&quot;anchor-link&quot; href=&quot;#References/Bibliography&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Cloverdale Bible Way: 
&lt;a href=&quot;https://bibleway.org/&quot;&gt;https://bibleway.org/&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Message Hub:
&lt;a href=&quot;http://www.messagehub.info/en/messages.do?show_en=true&quot;&gt;http://www.messagehub.info/en/messages.do?show_en=true&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Lev Konstantinovskiy:
&lt;a href=&quot;https://www.linkedin.com/in/levkonst/?originalSubdomain=uk&quot;&gt;https://www.linkedin.com/in/levkonst/?originalSubdomain=uk&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Blei, D.M. Probabilistic Topic Models, 2012.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;pyLDAvis python package:
&lt;a href=&quot;https://pypi.org/project/pyLDAvis/1.0.0/&quot;&gt;https://pypi.org/project/pyLDAvis/1.0.0/&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><category term="NLP" /><category term="Topic Modeling" /><category term="Network Analysis" /><category term="Python" /><category term="gensim" /><category term="spacy" /><category term="pyLDAvis" /><category term="igraph" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/portfolio/images/txt_fig6.png" /><media:content medium="image" url="/portfolio/images/txt_fig6.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Network Analysis on the Bali network</title><link href="/portfolio/network%20analysis/python/igraph/2019/03/15/NetworkProject.html" rel="alternate" type="text/html" title="Network Analysis on the Bali network" /><published>2019-03-15T00:00:00-05:00</published><updated>2019-03-15T00:00:00-05:00</updated><id>/portfolio/network%20analysis/python/igraph/2019/03/15/NetworkProject</id><content type="html" xml:base="/portfolio/network%20analysis/python/igraph/2019/03/15/NetworkProject.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2019-03-15-NetworkProject.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Purpose&quot;&gt;Purpose&lt;a class=&quot;anchor-link&quot; href=&quot;#Purpose&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The purpose of this project is to perform a &lt;em&gt;network analysis&lt;/em&gt; on the Bali terrorist network data set for the purpose of assessing the capabilities of the Python igraph package. The data set includes connection data between various terrorists (see &lt;a href=&quot;https://www.researchgate.net/publication/249035940_Connecting_Terrorist_Networks&quot;&gt;https://www.researchgate.net/publication/249035940_Connecting_Terrorist_Networks&lt;/a&gt; for background).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Analysis&quot;&gt;Analysis&lt;a class=&quot;anchor-link&quot; href=&quot;#Analysis&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The notebook for the analysis can be accessed here:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://nbviewer.jupyter.org/github/kobus78/NetworkProject/blob/master/NetworkingProject.ipynb&quot; title=&quot;NetworkingProject.ipynb&quot;&gt;NetworkingProject.ipynb&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><category term="Network Analysis" /><category term="Python" /><category term="igraph" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/portfolio/images/net_figa.png" /><media:content medium="image" url="/portfolio/images/net_figa.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Network Analysis on USA Airports</title><link href="/portfolio/airline%20industry/network%20modeling/community%20detection/visualization/r/2019/03/14/NetworkProject_R.html" rel="alternate" type="text/html" title="Network Analysis on USA Airports" /><published>2019-03-14T00:00:00-05:00</published><updated>2019-03-14T00:00:00-05:00</updated><id>/portfolio/airline%20industry/network%20modeling/community%20detection/visualization/r/2019/03/14/NetworkProject_R</id><content type="html" xml:base="/portfolio/airline%20industry/network%20modeling/community%20detection/visualization/r/2019/03/14/NetworkProject_R.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2019-03-14-NetworkProject_R.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Purpose&quot;&gt;Purpose&lt;a class=&quot;anchor-link&quot; href=&quot;#Purpose&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The purpose of this project is to gain insights from a &lt;em&gt;network analysis&lt;/em&gt; of USA airports. This network consists of passenger flights between airports in the United States that occurred during December of 2010.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Dataset&quot;&gt;Dataset&lt;a class=&quot;anchor-link&quot; href=&quot;#Dataset&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The data comes from the Research and Innovative Technology Administration (RITA). See &lt;a href=&quot;http://www.rita.dot.gov/about_rita/&quot;&gt;http://www.rita.dot.gov/about_rita/&lt;/a&gt; for details. The airport position information was collected from Wikipedia (by Gabor) and other public online sources.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The dataset is a directed network with edge directions aligning with flight directions. The network contains multiple edges between many vertices. Each edge is associated with a single carrier aircraft type. Multiple carriers between the same pair of airports will have multiple edges between the vertices for the airports. This kind of network is sometimes called a &lt;em&gt;multigraph&lt;/em&gt;. In total the network has 755 vertices and 23,473 edges.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The analysis was performed in R using the igraphdata package, written by Gabor Csardi (Gabor, 2015). He also maintains the package. The package is available in the CRAN repository (&lt;a href=&quot;https://cran.r-project.org/web/packages/igraphdata/igraphdata.pdf&quot;&gt;https://cran.r-project.org/web/packages/igraphdata/igraphdata.pdf&lt;/a&gt;). This package consists of a collection of network datasets to use with the igraph package. Examples of these datasets are: The Enron email network, various food webs, interactions in the immunoglobulin protein, the karate club network, Koenigsberg's bridges, visuotactile brain areas of the macaque monkey, UK faculty friendship network, and the domestic US flights network.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;I will start by working with the complete dataset. The dataset will be slimmed down progressively until I arrive at a suitable subset for my purposes. I have a general interest in the operations of airlines. In addition, the network seemed rich in terms of the number of attributes and the multiplicity of edges.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Next, I will describe the attributes included with the network. The network has a ‘name’ graph attribute. It also has several vertex and edge attributes.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Vertex-attributes&quot;&gt;Vertex attributes&lt;a class=&quot;anchor-link&quot; href=&quot;#Vertex-attributes&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;name:&lt;/strong&gt; Symbolic vertex name, this is the three letter IATA airport code.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;City:&lt;/strong&gt; City and state, where the airport is located.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Position:&lt;/strong&gt; Position of the airport, in WGS coordinates.&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Edge-attributes&quot;&gt;Edge attributes&lt;a class=&quot;anchor-link&quot; href=&quot;#Edge-attributes&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Carrier:&lt;/strong&gt; Name of the airline. The network includes both domestic and international carriers that performed at least one flight in December of 2010.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Departures:&lt;/strong&gt; The number of departures (for a given airline and aircraft type.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Seats:&lt;/strong&gt; The total number of seats available on the flights carried out by a given airline, using a given aircraft type.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Passengers:&lt;/strong&gt; The total number of passengers on the flights carried out by a given airline, using a given aircraft type.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Aircraft:&lt;/strong&gt; Type of the aircraft.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Distance:&lt;/strong&gt; The distance between the two airports, in miles.&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Visualization-and-Network-Summary&quot;&gt;Visualization and Network Summary&lt;a class=&quot;anchor-link&quot; href=&quot;#Visualization-and-Network-Summary&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Although the terms “node” and “link” (rather than &lt;em&gt;vertex&lt;/em&gt; and &lt;em&gt;edge&lt;/em&gt;) are more digestible to the general reader, I have decided, for the sake of preciseness, to use the more technical &lt;em&gt;vertex&lt;/em&gt; and &lt;em&gt;edge&lt;/em&gt;. This usage will be extended to the visualizations as well.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;A summary of this network in R confirms that it is &lt;em&gt;directed&lt;/em&gt; and &lt;em&gt;named&lt;/em&gt;. It is not &lt;em&gt;bipartite&lt;/em&gt;; neither is it &lt;em&gt;weighted&lt;/em&gt;. There are, however, edge attributes that can be used to make it weighted.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;usa&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;IGRAPH&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bf6202d&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DN&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;755&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;23473&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;US&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;airports&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;attr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;name &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;name &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;City &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Position &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Carrier &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Departures &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Seats &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Passengers &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Aircraft &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Distance &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image.png&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;](&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;attachment&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image.png&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Figure 1 shows a visualization of the network. Note that the size of vertices has been made proportional to the log of the degree due to the large variation of the degree of vertices. The symbolic vertex names (airport codes) are not shown to improve clarity. There are 755 vertices and 23,473 edges (although this is not apparent from the visualization). It is quite clear that, as we might expect, that this is a &lt;em&gt;scale-free&lt;/em&gt; (rather than a random) network. A number of prominent, highly connected vertices are easy to spot.  Another characteristic of this network is its apparent partitioning of vertices into a number of cohesive subgroups. There seems to be two main subgroups and several smaller ones. There are even small “islands” of a few connected vertices. The visualization reveals the presence of some &lt;em&gt;self-loops&lt;/em&gt; in the network.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/net_fig1.png&quot; alt=&quot;Figure 1  Network with vertex size proportional to the log of the number of connected edges&quot; title=&quot;Figure 1  Network with vertex size proportional to the log of the number of connected edges&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The R-code finds 53 self-loops:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;which_loop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;usa&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;53&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Figure 2 shows the network again, this time without taking the log of the degree. This provides some additional insights.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/net_fig2.png&quot; alt=&quot;Figure 2  Network with vertex size proportional to the number of connected edges&quot; title=&quot;Figure 2  Network with vertex size proportional to the number of connected edges&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Figure 3 is the same as Figure 2 except that the layout of the vertices is along a circle. This visualization emphasizes how highly-connected the hub vertices are in this scale-free network.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/net_fig3.png&quot; alt=&quot;Figure 3  Network with vertex size proportional to the number of connected edges&quot; title=&quot;Figure 3  Network with vertex size proportional to the number of connected edges&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Community-Detection&quot;&gt;Community Detection&lt;a class=&quot;anchor-link&quot; href=&quot;#Community-Detection&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We can approach the presence of cohesion in a network from a number of angles. One approach is to consider &lt;em&gt;cliques&lt;/em&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Cliques&quot;&gt;Cliques&lt;a class=&quot;anchor-link&quot; href=&quot;#Cliques&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;A clique is a maximally complete subgraph, in other words, a subset of vertices that have all possible ties among them. Cliques are usually considered in the context of undirected networks. It is, however, somewhat interesting to pretend for a moment that our network is undirected and consider the presence of cliques.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The R code reveals that there are cliques present and that the size of the largest maximally complete subgraph is 27:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;clique.number&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;usa&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;27&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;To proceed toward proper community detection, we will remove all the self-loops in the network. The occurrence of these are somewhat abnormal. They could be data errors or simply the fact that an airplane had to turn back to its takeoff airport due to problems that were experienced. As mentioned above, there are 53 self-loops in the network.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;After removal, we have this network:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;usa2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;IGRAPH&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b11c16a&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DN&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;755&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;23420&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;US&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;airports&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;attr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;name &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;name &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;City &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Position &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Carrier &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Departures &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Seats &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Passengers &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Aircraft &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Distance &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Next, I will simplify the network by collapsing all the multi-edges. In other words, I will turn the multigraph into a simple graph. Doing this will simplify the analysis. It does not make sense to combine the values for &lt;em&gt;Carrier&lt;/em&gt; and &lt;em&gt;Aircraft&lt;/em&gt; from multiple edges into a single value for my purposes. I will ignore these edge attributes. For the other edge attributes I will combine them by summing. This means I will sum all the &lt;em&gt;Departures&lt;/em&gt;, &lt;em&gt;Seats&lt;/em&gt;, and &lt;em&gt;Passengers&lt;/em&gt; for all the similarly-directed edges between a given vertex pair. The network will still be directional. The &lt;em&gt;Distance&lt;/em&gt; edge attribute values will be combined by taking the max of sets of equal values. After simplification, we have:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;usa3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;IGRAPH&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ddbc561&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DN&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;755&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;8228&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;US&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;airports&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;attr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;name &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;name &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;City &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Position &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Departures &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Seats &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Passengers &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Distance &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;is_simple&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;usa3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#confirm that usa3 is now a simple graph&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Next, I will turn the graph into a &lt;em&gt;weighted&lt;/em&gt; network. I will use the &lt;em&gt;Distance&lt;/em&gt; edge attribute to weight each edge.  After weighting, we have:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;usa4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;IGRAPH&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ddbc561&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DNW&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;755&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;8228&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;US&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;airports&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;attr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;name &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;name &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;City &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Position &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Departures &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Seats &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Passengers &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Distance &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;weight &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The first ten edges now look like this:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;E&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;usa4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;8228&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;edges&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;ddbc561 &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vertex&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
   &lt;span class=&quot;n&quot;&gt;tail&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;head&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tid&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hid&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Departures&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Seats&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Passengers&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Distance&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;BGR&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;BOS&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;          &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;    &lt;span class=&quot;m&quot;&gt;34&lt;/span&gt;          &lt;span class=&quot;m&quot;&gt;6&lt;/span&gt;      &lt;span class=&quot;m&quot;&gt;201&lt;/span&gt;    &lt;span class=&quot;m&quot;&gt;201&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;BGR&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;JFK&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;          &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;525&lt;/span&gt;        &lt;span class=&quot;m&quot;&gt;446&lt;/span&gt;      &lt;span class=&quot;m&quot;&gt;382&lt;/span&gt;    &lt;span class=&quot;m&quot;&gt;382&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;BGR&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;MIA&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;6&lt;/span&gt;          &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;    &lt;span class=&quot;m&quot;&gt;12&lt;/span&gt;          &lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;     &lt;span class=&quot;m&quot;&gt;1459&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;1459&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;BGR&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;EWR&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;7&lt;/span&gt;          &lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;758&lt;/span&gt;        &lt;span class=&quot;m&quot;&gt;680&lt;/span&gt;      &lt;span class=&quot;m&quot;&gt;393&lt;/span&gt;    &lt;span class=&quot;m&quot;&gt;393&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;BGR&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;DCA&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;43&lt;/span&gt;          &lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;200&lt;/span&gt;        &lt;span class=&quot;m&quot;&gt;116&lt;/span&gt;      &lt;span class=&quot;m&quot;&gt;590&lt;/span&gt;    &lt;span class=&quot;m&quot;&gt;590&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;6&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;BGR&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;DTW&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;44&lt;/span&gt;         &lt;span class=&quot;m&quot;&gt;55&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;2750&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;1955&lt;/span&gt;      &lt;span class=&quot;m&quot;&gt;750&lt;/span&gt;    &lt;span class=&quot;m&quot;&gt;750&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;7&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;BGR&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;LGA&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;57&lt;/span&gt;        &lt;span class=&quot;m&quot;&gt;130&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;6266&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;3640&lt;/span&gt;      &lt;span class=&quot;m&quot;&gt;378&lt;/span&gt;    &lt;span class=&quot;m&quot;&gt;378&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;8&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;BGR&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;PHL&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;71&lt;/span&gt;        &lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;6274&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;4953&lt;/span&gt;      &lt;span class=&quot;m&quot;&gt;473&lt;/span&gt;    &lt;span class=&quot;m&quot;&gt;473&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;9&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;BGR&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;PIE&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;157&lt;/span&gt;          &lt;span class=&quot;m&quot;&gt;9&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;1350&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;1198&lt;/span&gt;     &lt;span class=&quot;m&quot;&gt;1394&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;1394&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;BGR&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;SFB&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;370&lt;/span&gt;         &lt;span class=&quot;m&quot;&gt;11&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;1650&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;1491&lt;/span&gt;     &lt;span class=&quot;m&quot;&gt;1299&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;1299&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;After inspecting a much larger listing, we find that there are many flights with only a few passengers. This is also evident from the histogram in Figure 4. For example, there are 128 flights with a single passenger, 256 with 1 or 2 passengers, 522 with 5 or less, and 756 with 10 or fewer passengers:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;E&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;usa4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Passengers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Passengers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;128&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;E&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;usa4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Passengers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Passengers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;256&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;E&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;usa4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Passengers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Passengers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;522&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;E&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;usa4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Passengers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Passengers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;756&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/net_fig4.png&quot; alt=&quot;Figure 4  Distribution of number of passengers&quot; title=&quot;Figure 4  Distribution of number of passengers&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Next, I will remove all these edges. In fact, I will remove all edges that have 50,000 or less passengers. After this, we have the network:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;usa5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;IGRAPH&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d89fa3&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DNW&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;755&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;131&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;US&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;airports&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;attr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;name &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;name &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;City &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Position &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Departures &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Seats &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Passengers &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Distance &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;weight &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Next, I will remove all the isolates (vertices without any connected edges). After this step, we have:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;usa5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;IGRAPH&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;679&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f9d1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DNW&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;38&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;131&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;US&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;airports&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;attr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;name &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;name &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;City &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Position &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Departures &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Seats &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Passengers &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Distance &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;weight &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;After this, we can inspect the distribution of the number of flight legs over all geodesics. This is shown in Figure 5. Most flights (in the current dataset) consists of 2 legs (i.e. one transfer). Analyzing this further could open up opportunities to establish new direct flights, or reduced-leg flights. But, this is not our purpose currently.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/net_fig5.png&quot; alt=&quot;Figure 5  Distribution of number of flight legs over all geodesics&quot; title=&quot;Figure 5  Distribution of number of flight legs over all geodesics&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The next step is to convert the network to an undirected, unweighted graph. This will allow us more algorithms for the detection of communities. In converting to an undirected network, I will sum the &lt;em&gt;Departures&lt;/em&gt; for both directions between a vertex pair. This will also be done for the edge attributes &lt;em&gt;Seats&lt;/em&gt; and &lt;em&gt;Passengers&lt;/em&gt;. For distance, I will take the max (of two identical numbers). The &lt;em&gt;weight&lt;/em&gt; edge attribute will be ignored. After these operations, we have:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;usa6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;IGRAPH&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;999&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ad18&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;UN&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;38&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;72&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;US&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;airports&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;attr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;name &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;name &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;City &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Position &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Departures &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Seats &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Passengers &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Distance &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;This network consists of two clusters. We extract the largest component and then have:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;usa7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;IGRAPH&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;6881&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ea4&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;UN&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;--&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;36&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;71&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;US&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;airports&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;attr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;name &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;g&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;name &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;City &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Position &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Departures &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Seats &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Passengers &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Distance &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;This largest component is shown in Figure 6.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/net_fig6.png&quot; alt=&quot;Figure 6  Largest component of the network&quot; title=&quot;Figure 6  Largest component of the network&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We are finally ready for community analysis. &lt;em&gt;Modularity&lt;/em&gt; is an internal quality measure of clustering. This is an ideal measure when the ground truth for the 'correctness' of a cluster is not known. I will use this measure. Next, i will submit the network to a number of algorithms to identify communities.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Edge-betweenness-algorithm&quot;&gt;Edge-betweenness algorithm&lt;a class=&quot;anchor-link&quot; href=&quot;#Edge-betweenness-algorithm&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Figure 7 shows the edge-betweenness communities. This algorithm identifies three communities with a modularity of 0.31. One is centralized around Atlanta, GA. Another consists of the airports on the Hawaiian Islands. The third is a conglomeration of the rest of the airports.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/net_fig7.png&quot; alt=&quot;Figure 7  Edge-betweenness communities&quot; title=&quot;Figure 7  Edge-betweenness communities&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Leading-eigenvector-algorithm&quot;&gt;Leading eigenvector algorithm&lt;a class=&quot;anchor-link&quot; href=&quot;#Leading-eigenvector-algorithm&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In Figure 8 the communities identified by the leading eigenvector algorithm can be seen. These communities have a modularity of 0.33. This time we have four communities – Austin, TX and Dallas, TX form a separate community on their own. Los Angeles, CA is now part of the “Hawaii community” and so are Houston, TX and Washington, DC.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/net_fig8.png&quot; alt=&quot;Figure 8  Leading eigenvector communities&quot; title=&quot;Figure 8  Leading eigenvector communities&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Fast-greedy-algorithm&quot;&gt;Fast-greedy algorithm&lt;a class=&quot;anchor-link&quot; href=&quot;#Fast-greedy-algorithm&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Figure 9 shows the communities identified by the Fast-greedy algorithm. The modularity is 0.34 and we now have five communities. A fifth community seems to straddle parts of the “Atlanta”, “Dallas”, and “Other” communities.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/net_fig9.png&quot; alt=&quot;Figure 9  Fast-greedy communities&quot; title=&quot;Figure 9  Fast-greedy communities&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Louvain-algorithm&quot;&gt;Louvain algorithm&lt;a class=&quot;anchor-link&quot; href=&quot;#Louvain-algorithm&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The Louvain algorithm finds four communities with a modularity of 0.35. This is shown in Figure 10. It outlines a community centralized around Chicago, IL. The Atlanta and Hawaii communities are still present as well as the “Other” more “mesh-like” community.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/net_fig10.png&quot; alt=&quot;Figure 10  Louvain communities&quot; title=&quot;Figure 10  Louvain communities&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Walktrap-algorithm&quot;&gt;Walktrap algorithm&lt;a class=&quot;anchor-link&quot; href=&quot;#Walktrap-algorithm&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In Figure 11 we see the communities outlined by the Walktrap algorithm. The algorithm calculates a modularity of 0.31. We are back to three communities, much like the edge-betweenness algorithm’s result. There is a community centered around Atlanta, one centered around Honolulu, and a “other” community that contains the rest of the airports.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/net_fig11.png&quot; alt=&quot;Figure 11  Walktrap communities&quot; title=&quot;Figure 11  Walktrap communities&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Label-propagation-algorithm&quot;&gt;Label propagation algorithm&lt;a class=&quot;anchor-link&quot; href=&quot;#Label-propagation-algorithm&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Figure 12 shows the communities identified by the label propagation algorithm with a modularity of 0.10. This time we only have two communities: One centered around Honolulu and a second one that contains the rest of the airports. The crudeness of this partitioning is reflected in the relatively low modularity score of 0.10.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/net_fig12.png&quot; alt=&quot;Figure 12  Label propagation communities&quot; title=&quot;Figure 12  Label propagation communities&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;InfoMAP-algorithm&quot;&gt;InfoMAP algorithm&lt;a class=&quot;anchor-link&quot; href=&quot;#InfoMAP-algorithm&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In Figure 13 we see the communities outlined by the InfoMAP algorithm. Again, we have a relatively low modularity of 0.10. The two communities identified resembles those from the previous label propagation algorithm.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/net_fig13.png&quot; alt=&quot;Figure 13  InfoMAP communities&quot; title=&quot;Figure 13  InfoMAP communities&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Spinglass-algorithm&quot;&gt;Spinglass algorithm&lt;a class=&quot;anchor-link&quot; href=&quot;#Spinglass-algorithm&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The communities identified by the Spinglass algorithm appears in Figure 14. Here we have a modularity of 0.34. Except for the “Hawaii” cluster, we have a somewhat novel partitioning this time.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/net_fig14.png&quot; alt=&quot;Figure 14  Spinglass communities&quot; title=&quot;Figure 14  Spinglass communities&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Most-interesting-community-structure&quot;&gt;Most interesting community structure&lt;a class=&quot;anchor-link&quot; href=&quot;#Most-interesting-community-structure&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;To me, the most interesting community structure was provided by the edge-betweenness algorithm. This structure is repeated in Figure 15 and has a modularity of 0.31. It cleanly partitions the vertices into three communities. One community is strongly centered around Atlanta, GA. It has a particularly central and strong hub (Atlanta) from where a number of “spokes” emanates to smaller airports. This is not a surprise. Most Americans know that Atlanta is a world-wide flight hub. Due to its star-like structure, we expect the density of the subgraph associated with this “Atlanta” community to be low. Indeed, when we form an induced subgraph and calculate its density we get a density of only 0.16:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;community_density&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;usa7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;V&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;usa7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ceb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]])&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#Atlanta&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0.1648352&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/net_fig15.png&quot; alt=&quot;Figure 15  Most interesting community structure&quot; title=&quot;Figure 15  Most interesting community structure&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The second community is centered around Honolulu, HI and also has a star structure. This is interesting because the algorithm considers this tiny cluster of only 5 vertices as a “peer” of the other two communities. Once again, this is in line with our expectation. The flight data was collected in December of 2010 which means it is winter in the USA. The fact that the Hawaii cluster is so prominent reflects the substantial holiday traffic associated with this tropical island during the mainland’s winter season. Add to this the fact that the Hawaiian Islands can only (reasonably) be visited by flying there (i.e. no driving) and you have a very “worthy” airport community.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Due to its star-like structure, we also expect the density of the subgraph associated with the “Hawaii” community to be low. However, it has a very small number of vertices and there are not many more links that can be made. So, its density turns out to be 0.4:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;community_density&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;usa7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;V&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;usa7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ceb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]])&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#Hawaii&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0.4&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The third community is interesting in the sense that it almost resembles a random (Erdos-Renyi) network (Erdos, Renyi, 1959). It seems like the relatively large airports contained within it compete with each other to be larger hubs and in the process, ends up more-or-less as “peers.” This random appearance suggests that the subgraph associated with this “Other” community might be denser, and indeed it is when compared with the “Atlanta” community which has a density of only 0.16. This community has a density of 0.28:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;community_density&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;usa7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;V&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;usa7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ceb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]])&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#Others&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0.2794118&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;For a final visualization of our most interesting community structure we size the edge width according to the total number of passengers flowing back and forth along each edge. See Figure 16. Indeed, we see how relatively strong the flow towards and within the Hawaii community is.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/net_fig16.png&quot; alt=&quot;Figure 16  Most interesting community structure (2)&quot; title=&quot;Figure 16  Most interesting community structure (2)&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Network-Modeling&quot;&gt;Network Modeling&lt;a class=&quot;anchor-link&quot; href=&quot;#Network-Modeling&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;I will now add two &lt;em&gt;additional&lt;/em&gt; vertex attributes to the network to make analysis more interesting. I suspect that the population of an airport’s city might have an effect on the network’s model. Accordingly, I obtained population information from the 2010 US Census,  &lt;a href=&quot;https://www.census.gov/programs-surveys/decennial-census/decade.2010.html&quot;&gt;https://www.census.gov/programs-surveys/decennial-census/decade.2010.html&lt;/a&gt;. I stored this data in a new numeric vertex attribute called &lt;em&gt;Population&lt;/em&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Another variable I thought might play a role is whether an airport’s city is a coastal city. For this, I added a new boolean vertex variable called &lt;em&gt;Coastal&lt;/em&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;I will now use ERGM to build progressively richer and more accurate models for the network. The first model is the &lt;em&gt;null&lt;/em&gt; model.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Null-model&quot;&gt;Null model&lt;a class=&quot;anchor-link&quot; href=&quot;#Null-model&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Using only the edge feature, I got this model (called null.mod in the R code):&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==========================&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Summary&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;==========================&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Formula&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;usa.n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;~&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;edges&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Iterations&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;5&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;20&lt;/span&gt; 

&lt;span class=&quot;n&quot;&gt;Monte&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Carlo&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MLE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Results&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;Estimate&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Std.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Error&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MCMC&lt;/span&gt; % &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Pr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;    
&lt;span class=&quot;n&quot;&gt;edges&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;-2.063&lt;/span&gt;      &lt;span class=&quot;m&quot;&gt;0.126&lt;/span&gt;      &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;-16.38&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1e-04&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;***&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Signif.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;codes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt; ‘&lt;span class=&quot;o&quot;&gt;***&lt;/span&gt;’ &lt;span class=&quot;m&quot;&gt;0.001&lt;/span&gt; ‘&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;’ &lt;span class=&quot;m&quot;&gt;0.01&lt;/span&gt; ‘&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;’ &lt;span class=&quot;m&quot;&gt;0.05&lt;/span&gt; ‘&lt;span class=&quot;n&quot;&gt;.&lt;/span&gt;’ &lt;span class=&quot;m&quot;&gt;0.1&lt;/span&gt; ‘ ’ &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;

     &lt;span class=&quot;n&quot;&gt;Null&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Deviance&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;873.4&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;630&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;degrees&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;freedom&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;Residual&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Deviance&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;443.7&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;629&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;degrees&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;freedom&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;AIC&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;445.7&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;BIC&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;450.1&lt;/span&gt;    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Smaller&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;better.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The coefficient of edges is significant and is negative (-2.063) which shows that the density of the network is less than 50% (a coefficient of 0 indicates a 50% density). It is typical of real networks to have a density less than 50%. Indeed, the R code reveals that the density of the network is 0.1126984. This value is also the probability of creating an additional edge by adding one more node. The null model gives an AIC value of 445.7.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Population-effect&quot;&gt;Population effect&lt;a class=&quot;anchor-link&quot; href=&quot;#Population-effect&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Adding the population data to the existing model, I get the following model (called pop.mod in the R code):&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==========================&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Summary&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;==========================&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Formula&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;usa.n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;~&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;edges&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;nodecov&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Population&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Iterations&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;20&lt;/span&gt; 

&lt;span class=&quot;n&quot;&gt;Monte&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Carlo&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MLE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Results&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
                     &lt;span class=&quot;n&quot;&gt;Estimate&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Std.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Error&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MCMC&lt;/span&gt; % &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Pr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;    
&lt;span class=&quot;n&quot;&gt;edges&lt;/span&gt;              &lt;span class=&quot;m&quot;&gt;-2.332e+00&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;1.743e-01&lt;/span&gt;      &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;-13.375&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1e-04&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;***&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;nodecov.Population&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;1.023e-07&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;4.097e-08&lt;/span&gt;      &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;2.496&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0.0126&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;  
&lt;span class=&quot;o&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Signif.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;codes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt; ‘&lt;span class=&quot;o&quot;&gt;***&lt;/span&gt;’ &lt;span class=&quot;m&quot;&gt;0.001&lt;/span&gt; ‘&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;’ &lt;span class=&quot;m&quot;&gt;0.01&lt;/span&gt; ‘&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;’ &lt;span class=&quot;m&quot;&gt;0.05&lt;/span&gt; ‘&lt;span class=&quot;n&quot;&gt;.&lt;/span&gt;’ &lt;span class=&quot;m&quot;&gt;0.1&lt;/span&gt; ‘ ’ &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;

     &lt;span class=&quot;n&quot;&gt;Null&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Deviance&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;873.4&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;630&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;degrees&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;freedom&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;Residual&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Deviance&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;438.0&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;628&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;degrees&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;freedom&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;AIC&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;442&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;BIC&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;450.9&lt;/span&gt;    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Smaller&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;better.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The low p value of nodecov.Population shows that the coefficient of nodecov.Population is somewhat significant in predicting the connection between two airport nodes. For example, the 
likelihood of a flight connection between two cities with population of 1 and 2 million respectively, is:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;plogis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;-2.332&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1.023e-07&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1.023e-07&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0.08850721&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;This value is smaller than the density of the network (0.1126984) which indicates that the role of the population in the present model is not very helpful in the prediction of a connection. Even so, the AIC value of this model is a little lower than that of the null model.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Homophily-effect&quot;&gt;Homophily effect&lt;a class=&quot;anchor-link&quot; href=&quot;#Homophily-effect&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Next, I will use homophily to see if matches between Coastal values of vertices might help. Doing this, I got the following model (called hom.mod in the R code):&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==========================&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Summary&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;==========================&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Formula&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;usa.n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;~&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;edges&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;nodecov&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Population&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
                         &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;nodematch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Coastal&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;diff&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Iterations&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;5&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;20&lt;/span&gt; 

&lt;span class=&quot;n&quot;&gt;Monte&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Carlo&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MLE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Results&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
                     &lt;span class=&quot;n&quot;&gt;Estimate&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Std.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Error&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MCMC&lt;/span&gt; % &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Pr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;    
&lt;span class=&quot;n&quot;&gt;edges&lt;/span&gt;              &lt;span class=&quot;m&quot;&gt;-2.821e+00&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;2.524e-01&lt;/span&gt;      &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;-11.176&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1e-04&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;***&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;nodecov.Population&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;1.103e-07&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;4.188e-08&lt;/span&gt;      &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;2.634&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;0.00843&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;nodematch.Coastal&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;8.241e-01&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;2.671e-01&lt;/span&gt;      &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;3.085&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;0.00203&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; 
&lt;span class=&quot;o&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Signif.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;codes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt; ‘&lt;span class=&quot;o&quot;&gt;***&lt;/span&gt;’ &lt;span class=&quot;m&quot;&gt;0.001&lt;/span&gt; ‘&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;’ &lt;span class=&quot;m&quot;&gt;0.01&lt;/span&gt; ‘&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;’ &lt;span class=&quot;m&quot;&gt;0.05&lt;/span&gt; ‘&lt;span class=&quot;n&quot;&gt;.&lt;/span&gt;’ &lt;span class=&quot;m&quot;&gt;0.1&lt;/span&gt; ‘ ’ &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;

     &lt;span class=&quot;n&quot;&gt;Null&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Deviance&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;873.4&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;630&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;degrees&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;freedom&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;Residual&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Deviance&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;428.0&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;627&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;degrees&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;freedom&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;AIC&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;434&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;BIC&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;447.3&lt;/span&gt;    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Smaller&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;better.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The low p value of nodematch.Coastal shows that its coefficient is significant in predicting the connection between two airport vertices. (We notice that the significance of the nodecov.Population parameter has improved too.) For example, the likelihood of a flight connection between two cities with population of 1 and 2 million respectively, and matching &lt;em&gt;Coastal&lt;/em&gt; values is:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;plogis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;-2.821&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1.103e-07&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1.103e-07&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;8.241e-01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.1195288&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;This value is larger than the density of the network (0.1126984) which indicates that the effect of whether an airport is at the coast or not is indeed helpful in the prediction of a connection. In addition, the AIC has improved too, now at 434.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Differential-Homophily-effect&quot;&gt;Differential Homophily effect&lt;a class=&quot;anchor-link&quot; href=&quot;#Differential-Homophily-effect&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;I will now use homophily of the Coastal vertex attribute in a more in-depth way, known as differential homophily. This means we will match for both No-No as well as Yes-Yes. Doing so gave me the following model (called difhom1.mod in the R code):&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==========================&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Summary&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;==========================&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Formula&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;usa.n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;~&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;edges&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;nodecov&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Population&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
                         &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;nodematch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Coastal&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;diff&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Iterations&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;5&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;20&lt;/span&gt; 

&lt;span class=&quot;n&quot;&gt;Monte&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Carlo&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MLE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Results&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
                          &lt;span class=&quot;n&quot;&gt;Estimate&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Std.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Error&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MCMC&lt;/span&gt; % &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Pr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;    
&lt;span class=&quot;n&quot;&gt;edges&lt;/span&gt;                   &lt;span class=&quot;m&quot;&gt;-2.905e+00&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;2.642e-01&lt;/span&gt;      &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;-10.994&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1e-04&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;***&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;nodecov.Population&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;1.355e-07&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;4.513e-08&lt;/span&gt;      &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;3.002&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0.002686&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;nodematch.Coastal.FALSE&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;1.070e+00&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;2.982e-01&lt;/span&gt;      &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;3.587&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0.000334&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;***&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;nodematch.Coastal.TRUE&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;4.444e-01&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;3.567e-01&lt;/span&gt;      &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;1.246&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0.212748&lt;/span&gt;    
&lt;span class=&quot;o&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Signif.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;codes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt; ‘&lt;span class=&quot;o&quot;&gt;***&lt;/span&gt;’ &lt;span class=&quot;m&quot;&gt;0.001&lt;/span&gt; ‘&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;’ &lt;span class=&quot;m&quot;&gt;0.01&lt;/span&gt; ‘&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;’ &lt;span class=&quot;m&quot;&gt;0.05&lt;/span&gt; ‘&lt;span class=&quot;n&quot;&gt;.&lt;/span&gt;’ &lt;span class=&quot;m&quot;&gt;0.1&lt;/span&gt; ‘ ’ &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;

     &lt;span class=&quot;n&quot;&gt;Null&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Deviance&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;873.4&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;630&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;degrees&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;freedom&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;Residual&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Deviance&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;424.8&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;626&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;degrees&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;freedom&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;AIC&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;432.8&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;BIC&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;450.6&lt;/span&gt;    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Smaller&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;better.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We see that the significance of nodematch.Coastal.FALSE is higher than the significance of nodematch.Coastal in the previous model. In our example, the 
likelihood of a flight connection between two cities with population of 1 and 2 million respectively, and miss-matching &lt;em&gt;Coastal&lt;/em&gt; values is:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;plogis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;-2.905&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1.335e-07&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1.335e-07&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1.070&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0.1376438&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The AIC value is even better (lower), now at 432.8.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Differential-Homophily-effect-2&quot;&gt;Differential Homophily effect 2&lt;a class=&quot;anchor-link&quot; href=&quot;#Differential-Homophily-effect-2&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;I will now use differential homophily of the Coastal vertex attribute to only match for No-No. This gave me the following model (called difhom2.mod in the R code):&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==========================&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Summary&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;==========================&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Formula&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;usa.n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;~&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;edges&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;nodecov&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Population&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
                       &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;nodematch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Coastal&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;diff&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keep&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Iterations&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;5&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;20&lt;/span&gt; 

&lt;span class=&quot;n&quot;&gt;Monte&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Carlo&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MLE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Results&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
                          &lt;span class=&quot;n&quot;&gt;Estimate&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Std.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Error&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MCMC&lt;/span&gt; % &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Pr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;    
&lt;span class=&quot;n&quot;&gt;edges&lt;/span&gt;                   &lt;span class=&quot;m&quot;&gt;-2.790e+00&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;2.418e-01&lt;/span&gt;      &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;-11.535&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1e-04&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;***&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;nodecov.Population&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;1.435e-07&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;4.477e-08&lt;/span&gt;      &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;3.206&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0.001347&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; 
&lt;span class=&quot;n&quot;&gt;nodematch.Coastal.FALSE&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;9.397e-01&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;2.737e-01&lt;/span&gt;      &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;3.433&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0.000597&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;***&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Signif.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;codes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt; ‘&lt;span class=&quot;o&quot;&gt;***&lt;/span&gt;’ &lt;span class=&quot;m&quot;&gt;0.001&lt;/span&gt; ‘&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;’ &lt;span class=&quot;m&quot;&gt;0.01&lt;/span&gt; ‘&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;’ &lt;span class=&quot;m&quot;&gt;0.05&lt;/span&gt; ‘&lt;span class=&quot;n&quot;&gt;.&lt;/span&gt;’ &lt;span class=&quot;m&quot;&gt;0.1&lt;/span&gt; ‘ ’ &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;

     &lt;span class=&quot;n&quot;&gt;Null&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Deviance&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;873.4&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;630&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;degrees&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;freedom&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;Residual&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Deviance&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;426.3&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;627&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;degrees&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;freedom&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;AIC&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;432.3&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;BIC&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;445.7&lt;/span&gt;    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Smaller&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;better.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We see that the significance of all the previous parameters have remained. In addition, the AIC value is slightly lower to 432.3.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Differential-Homophily-effect-3&quot;&gt;Differential Homophily effect 3&lt;a class=&quot;anchor-link&quot; href=&quot;#Differential-Homophily-effect-3&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;This time I will have the Coastal vertex attribute to only match for Yes-Yes. This gave me the following model (called difhom2.mod in the R code):&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==========================&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Summary&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;==========================&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Formula&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;usa.n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;~&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;edges&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;nodecov&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Population&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
                       &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;nodematch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Coastal&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;diff&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keep&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Iterations&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;20&lt;/span&gt; 

&lt;span class=&quot;n&quot;&gt;Monte&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Carlo&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MLE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Results&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
                         &lt;span class=&quot;n&quot;&gt;Estimate&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Std.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Error&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MCMC&lt;/span&gt; % &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Pr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;    
&lt;span class=&quot;n&quot;&gt;edges&lt;/span&gt;                  &lt;span class=&quot;m&quot;&gt;-2.334e+00&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;1.791e-01&lt;/span&gt;      &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;-13.035&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1e-04&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;***&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;nodecov.Population&lt;/span&gt;      &lt;span class=&quot;m&quot;&gt;1.017e-07&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;4.183e-08&lt;/span&gt;      &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;2.432&lt;/span&gt;    &lt;span class=&quot;m&quot;&gt;0.015&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;  
&lt;span class=&quot;n&quot;&gt;nodematch.Coastal.TRUE&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;1.992e-02&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;3.196e-01&lt;/span&gt;      &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0.062&lt;/span&gt;    &lt;span class=&quot;m&quot;&gt;0.950&lt;/span&gt;    
&lt;span class=&quot;o&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Signif.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;codes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt; ‘&lt;span class=&quot;o&quot;&gt;***&lt;/span&gt;’ &lt;span class=&quot;m&quot;&gt;0.001&lt;/span&gt; ‘&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;’ &lt;span class=&quot;m&quot;&gt;0.01&lt;/span&gt; ‘&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;’ &lt;span class=&quot;m&quot;&gt;0.05&lt;/span&gt; ‘&lt;span class=&quot;n&quot;&gt;.&lt;/span&gt;’ &lt;span class=&quot;m&quot;&gt;0.1&lt;/span&gt; ‘ ’ &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;

     &lt;span class=&quot;n&quot;&gt;Null&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Deviance&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;873.4&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;630&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;degrees&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;freedom&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;Residual&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Deviance&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;438.0&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;627&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;degrees&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;freedom&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;AIC&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;444&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;BIC&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;457.3&lt;/span&gt;    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Smaller&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;better.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;This time we have a &lt;em&gt;weaker&lt;/em&gt; model in terms of the significance of the parameter estimates. In addition, the AIC value is up to 444.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Relational-terms-(network-predictors)&quot;&gt;Relational terms (network predictors)&lt;a class=&quot;anchor-link&quot; href=&quot;#Relational-terms-(network-predictors)&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;To try and improve the model further I will add some relational predictors. This means information about ties among the network members is used to predict the likelihood of the dependent variable tie. No matter how I combined edge attributes, I always got a model that did not converge:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==========================&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Summary&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;==========================&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Formula&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;usa.n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;~&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;edges&lt;/span&gt; 
               &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;nodecov&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Population&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;nodematch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Coastal&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;diff&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keep&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;edgecov&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;usa.n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;attr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;Departures&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;edgecov&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;usa.n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;attr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;Seats&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;edgecov&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;usa.n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;attr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;Passengers&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;edgecov&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;usa.n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;attr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;Distance&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Iterations&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;25&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;20&lt;/span&gt; 

&lt;span class=&quot;n&quot;&gt;Monte&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Carlo&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MLE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Results&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
                          &lt;span class=&quot;n&quot;&gt;Estimate&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Std.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Error&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MCMC&lt;/span&gt; % &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Pr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;edges&lt;/span&gt;                   &lt;span class=&quot;m&quot;&gt;-2.684e+01&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;2.559e+04&lt;/span&gt;      &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;-0.001&lt;/span&gt;    &lt;span class=&quot;m&quot;&gt;0.999&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;nodecov.Population&lt;/span&gt;      &lt;span class=&quot;m&quot;&gt;-6.178e-09&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;6.083e-03&lt;/span&gt;      &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0.000&lt;/span&gt;    &lt;span class=&quot;m&quot;&gt;1.000&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;nodematch.Coastal.FALSE&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;1.447e+00&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;2.587e+04&lt;/span&gt;      &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0.000&lt;/span&gt;    &lt;span class=&quot;m&quot;&gt;1.000&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;edgecov.Departures&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;3.049e-02&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;1.231e+03&lt;/span&gt;      &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0.000&lt;/span&gt;    &lt;span class=&quot;m&quot;&gt;1.000&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;edgecov.Seats&lt;/span&gt;           &lt;span class=&quot;m&quot;&gt;-8.836e-04&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;2.166e+01&lt;/span&gt;      &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0.000&lt;/span&gt;    &lt;span class=&quot;m&quot;&gt;1.000&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;edgecov.Passengers&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;1.720e-03&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;1.664e+01&lt;/span&gt;      &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0.000&lt;/span&gt;    &lt;span class=&quot;m&quot;&gt;1.000&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;edgecov.Distance&lt;/span&gt;         &lt;span class=&quot;m&quot;&gt;3.731e-03&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;7.526e+01&lt;/span&gt;      &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0.000&lt;/span&gt;    &lt;span class=&quot;m&quot;&gt;1.000&lt;/span&gt;

     &lt;span class=&quot;n&quot;&gt;Null&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Deviance&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;8.734e+02&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;630&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;degrees&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;freedom&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;Residual&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Deviance&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;7.314e-09&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;623&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;degrees&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;freedom&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;AIC&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;14&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;BIC&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;45.12&lt;/span&gt;    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Smaller&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;better.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Local-structural-predictors-(dyad-dependency)&quot;&gt;Local structural predictors (dyad dependency)&lt;a class=&quot;anchor-link&quot; href=&quot;#Local-structural-predictors-(dyad-dependency)&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Lastly, I tried to include a predictor that uses local structural properties. This gave me the following model (called dyad.mod in the R code):&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==========================&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Summary&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;==========================&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Formula&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;usa.n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;~&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;edges&lt;/span&gt; 
                   &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;nodecov&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Population&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
                   &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;nodematch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Coastal&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;diff&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keep&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
                   &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;gwesp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fixed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Iterations&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;20&lt;/span&gt; 

&lt;span class=&quot;n&quot;&gt;Monte&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Carlo&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MLE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Results&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
                          &lt;span class=&quot;n&quot;&gt;Estimate&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Std.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Error&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MCMC&lt;/span&gt; %  &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Pr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;|&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;|&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;    
&lt;span class=&quot;n&quot;&gt;edges&lt;/span&gt;                   &lt;span class=&quot;m&quot;&gt;-3.533e+00&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;4.262e-03&lt;/span&gt;    &lt;span class=&quot;m&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;-828.828&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1e-04&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;***&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;nodecov.Population&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;7.585e-08&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;1.472e-08&lt;/span&gt;      &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;    &lt;span class=&quot;m&quot;&gt;5.153&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1e-04&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;***&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;nodematch.Coastal.FALSE&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;5.196e-01&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;3.694e-03&lt;/span&gt;    &lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;140.663&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1e-04&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;***&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;gwesp.fixed.0.7&lt;/span&gt;          &lt;span class=&quot;m&quot;&gt;6.979e-01&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;2.173e-03&lt;/span&gt;    &lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;321.149&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1e-04&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;***&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Signif.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;codes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt; ‘&lt;span class=&quot;o&quot;&gt;***&lt;/span&gt;’ &lt;span class=&quot;m&quot;&gt;0.001&lt;/span&gt; ‘&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;’ &lt;span class=&quot;m&quot;&gt;0.01&lt;/span&gt; ‘&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;’ &lt;span class=&quot;m&quot;&gt;0.05&lt;/span&gt; ‘&lt;span class=&quot;n&quot;&gt;.&lt;/span&gt;’ &lt;span class=&quot;m&quot;&gt;0.1&lt;/span&gt; ‘ ’ &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;

     &lt;span class=&quot;n&quot;&gt;Null&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Deviance&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;873.4&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;630&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;degrees&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;freedom&lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;Residual&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Deviance&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;397.6&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;on&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;626&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;degrees&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;freedom&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;AIC&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;405.6&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;BIC&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;423.3&lt;/span&gt;    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Smaller&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;better.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Not only did this approach have a wholesome effect on the significance of all the parameters I used, but it also lowered the AIC value to 405.6.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;I will next investigate the goodness-of-fit of the above models.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Goodness-of-fit&quot;&gt;Goodness-of-fit&lt;a class=&quot;anchor-link&quot; href=&quot;#Goodness-of-fit&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The simulation statistics show that the dyad.mod (the last model) seems to perform better than the other models, in particular in the case of triangles:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;edges&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;degree0&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;degree1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;degree2&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;degree3&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;degree4&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;degree5&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;triangle&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;usa.n&lt;/span&gt;          &lt;span class=&quot;m&quot;&gt;71&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;      &lt;span class=&quot;m&quot;&gt;14&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;47&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;null.mod&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;83&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;6&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;6&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;9&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;15&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;hom.mod&lt;/span&gt;        &lt;span class=&quot;m&quot;&gt;69&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;6&lt;/span&gt;      &lt;span class=&quot;m&quot;&gt;12&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;        &lt;span class=&quot;m&quot;&gt;9&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;difhom1.mod&lt;/span&gt;    &lt;span class=&quot;m&quot;&gt;69&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;6&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;8&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;8&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;        &lt;span class=&quot;m&quot;&gt;8&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;difhom2.mod&lt;/span&gt;    &lt;span class=&quot;m&quot;&gt;74&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;8&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;12&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;difhom3.mod&lt;/span&gt;    &lt;span class=&quot;m&quot;&gt;79&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;8&lt;/span&gt;      &lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;7&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;13&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dyad.mod&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;84&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;50&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Next I will plot some charts that will indicate the goodness-of-fit of the best model: dyad.mod. See Figure 17. When the dark line falls within the gray lines it indicates a good fit. The gray lines indicate the confidence interval. As is evident from Figure 17 and Figure 18, most elements have been fitted well.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/net_fig17.png&quot; alt=&quot;Figure 17  Goodness-of-fit diagnostics (1)&quot; title=&quot;Figure 17  Goodness-of-fit diagnostics (1)&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Figure 18 shows the same information except for the vertical axes that are calibrated to display log-odds values.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/net_fig18.png&quot; alt=&quot;Figure 18  Goodness-of-fit diagnostics (2)&quot; title=&quot;Figure 18  Goodness-of-fit diagnostics (2)&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Figure 19 shows the MCMC goodness-of-fit diagnostics.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/net_fig19.png&quot; alt=&quot;Figure 19  Goodness-of-fit diagnostics: MCMC&quot; title=&quot;Figure 19  Goodness-of-fit diagnostics: MCMC&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Lastly, we will look at the &lt;em&gt;p-values&lt;/em&gt; of the goodness-of-fit simulations. When p-values are high it shows that there are no significant differences between the simulated networks and the original network. The problematic p-values have been highlighted. In terms of &lt;em&gt;minimum geodesic distance&lt;/em&gt;, we have a good fit. For degree, we have a good fit except for degree values of 1 and 19.  In the case of &lt;em&gt;edgewise shared partner&lt;/em&gt;, we have a good fit. For &lt;em&gt;dyadwise shared partner&lt;/em&gt;, the only problem is at dsp1. &lt;em&gt;Triad census&lt;/em&gt; shows a good fit too. The same is true for the &lt;em&gt;model statistics&lt;/em&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dyad.mod.gof&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#goodness-of-fit for the dyad.mod network&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Goodness&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;of&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;minimum&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;geodesic&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;distance&lt;/span&gt; 

    &lt;span class=&quot;n&quot;&gt;obs&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;min&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MC&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;    &lt;span class=&quot;m&quot;&gt;71&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;24&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;71.33&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;123&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;1.00&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;261&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;28&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;178.31&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;333&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0.18&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;198&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;31&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;152.75&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;225&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0.24&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;    &lt;span class=&quot;m&quot;&gt;95&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;62.22&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;147&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0.22&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;     &lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;18.87&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;69&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0.52&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;6&lt;/span&gt;     &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;5.26&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;45&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0.98&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;7&lt;/span&gt;     &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;1.43&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;27&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;1.00&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;8&lt;/span&gt;     &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0.34&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;12&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;1.00&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;9&lt;/span&gt;     &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0.09&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;1.00&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;    &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0.03&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;1.00&lt;/span&gt;
&lt;span class=&quot;kc&quot;&gt;Inf&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;139.37&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;454&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0.06&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Goodness&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;of&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;degree&lt;/span&gt; 

   &lt;span class=&quot;n&quot;&gt;obs&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;min&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MC&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;    &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;3.71&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0.06&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;14&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;4.76&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;16&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0.02&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;    &lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;5.11&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;11&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;1.00&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;    &lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;4.72&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;13&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0.64&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;    &lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;4.16&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0.80&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;    &lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;3.69&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;8&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0.98&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;6&lt;/span&gt;    &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;3.03&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;9&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0.48&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;7&lt;/span&gt;    &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2.21&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;6&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;1.00&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;8&lt;/span&gt;    &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1.52&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;7&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;1.00&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;9&lt;/span&gt;    &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1.13&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0.78&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0.79&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0.90&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;11&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0.42&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;1.00&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;12&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0.31&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0.50&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;13&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0.21&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0.34&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;14&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0.11&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;1.00&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;15&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0.05&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;1.00&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;16&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0.03&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;1.00&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;17&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0.01&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;1.00&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;18&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0.03&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;1.00&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;19&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0.00&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0.00&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Goodness&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;of&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;edgewise&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shared&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;partner&lt;/span&gt; 

     &lt;span class=&quot;n&quot;&gt;obs&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;min&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MC&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;esp0&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;20&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;14.90&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;33&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0.34&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;esp1&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;17&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;23.98&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;41&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0.26&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;esp2&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;12&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;17.16&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;38&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0.52&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;esp3&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;9.25&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;26&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0.52&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;esp4&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;11&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;3.92&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;22&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0.22&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;esp5&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;1.46&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;9&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0.66&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;esp6&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;0.46&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0.08&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;esp7&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;0.17&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0.22&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;esp9&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;0.03&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0.06&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Goodness&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;of&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dyadwise&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shared&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;partner&lt;/span&gt; 

     &lt;span class=&quot;n&quot;&gt;obs&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;min&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MC&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dsp0&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;318&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;182&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;395.26&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;597&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0.32&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dsp1&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;206&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;31&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;142.54&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;201&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0.00&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dsp2&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;66&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;59.01&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;140&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0.74&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dsp3&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;21.69&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;71&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0.52&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dsp4&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;20&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;7.90&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;46&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0.18&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dsp5&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;2.62&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;15&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0.40&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dsp6&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0.73&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;8&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0.22&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dsp7&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0.21&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0.24&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dsp8&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0.01&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;1.00&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dsp9&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;0.03&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0.06&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Goodness&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;of&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;triad&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;census&lt;/span&gt; 

   &lt;span class=&quot;n&quot;&gt;obs&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;min&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;max&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MC&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;5186&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;3732&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;5053.56&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;6357&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0.84&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1541&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;752&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1785.41&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2728&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0.48&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;366&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;29&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;263.28&lt;/span&gt;  &lt;span class=&quot;m&quot;&gt;592&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0.40&lt;/span&gt;
&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;47&lt;/span&gt;    &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;37.75&lt;/span&gt;   &lt;span class=&quot;m&quot;&gt;97&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0.54&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Goodness&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;of&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;statistics&lt;/span&gt; 

                                 &lt;span class=&quot;n&quot;&gt;obs&lt;/span&gt;          &lt;span class=&quot;n&quot;&gt;min&lt;/span&gt;         &lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;          &lt;span class=&quot;n&quot;&gt;max&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MC&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;edges&lt;/span&gt;                   &lt;span class=&quot;m&quot;&gt;7.100000e+01&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2.400000e+01&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;7.133000e+01&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1.230000e+02&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;1.00&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;nodecov.Population&lt;/span&gt;      &lt;span class=&quot;m&quot;&gt;2.182382e+08&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;6.183536e+07&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2.176689e+08&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;3.786887e+08&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0.96&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;nodematch.Coastal.FALSE&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;3.200000e+01&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1.200000e+01&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;3.229000e+01&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;5.600000e+01&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0.94&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;gwesp.fixed.0.7&lt;/span&gt;         &lt;span class=&quot;m&quot;&gt;7.664400e+01&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;5.503415e+00&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;7.757239e+01&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1.734378e+02&lt;/span&gt;       &lt;span class=&quot;m&quot;&gt;0.92&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Bibliography&quot;&gt;Bibliography&lt;a class=&quot;anchor-link&quot; href=&quot;#Bibliography&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Gabor, Csardi. igraphdata: A Collection of Network Data Sets for the 'igraph' Package, 2015.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Erdos, P., Renyi, A. On Random Graphs, 1959.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;CRAN repository, igraphdata. 
&lt;a href=&quot;https://cran.r-project.org/web/packages/igraphdata/igraphdata.pdf&quot;&gt;https://cran.r-project.org/web/packages/igraphdata/igraphdata.pdf&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;2010 US Census,  &lt;a href=&quot;https://www.census.gov/programs-surveys/decennial-census/decade.2010.html&quot;&gt;https://www.census.gov/programs-surveys/decennial-census/decade.2010.html&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;RITA. The Research and Innovative Technology Administration, 2010. &lt;a href=&quot;http://www.rita.dot.gov/about_rita/&quot;&gt;http://www.rita.dot.gov/about_rita/&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><category term="Airline Industry" /><category term="Network Modeling" /><category term="Community Detection" /><category term="Visualization" /><category term="R" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/portfolio/images/net_fig9.png" /><media:content medium="image" url="/portfolio/images/net_fig9.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Using Visualization to Make an Effective Case for Using a Machine Translation Tool</title><link href="/portfolio/translation%20industry/visualization/python/seaborn/2019/02/12/VisualizationProject.html" rel="alternate" type="text/html" title="Using Visualization to Make an Effective Case for Using a Machine Translation Tool" /><published>2019-02-12T00:00:00-06:00</published><updated>2019-02-12T00:00:00-06:00</updated><id>/portfolio/translation%20industry/visualization/python/seaborn/2019/02/12/VisualizationProject</id><content type="html" xml:base="/portfolio/translation%20industry/visualization/python/seaborn/2019/02/12/VisualizationProject.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2019-02-12-VisualizationProject.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Background&quot;&gt;Background&lt;a class=&quot;anchor-link&quot; href=&quot;#Background&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;I am assisting a non-profit with a translation system that they use to translate documents into dozens of languages. The system presents a translator with a single sentence pair at a time. The top sentence is the source English sentence while the bottom sentence is a machine produced pre-translated sentence in the target language.  See Figure 1.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/vis_fig1.png&quot; alt=&quot;Figure 1  A single sentence pair is presented to the translator&quot; title=&quot;Figure 1  A single sentence pair is presented to the translator&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;This approach saves a lot of eye-wandering to keep finding the correct place in the text as a translator needs to jump back-and-forth between the original and translated text. It also focuses the translator’s attention on a single sentence at a time. Even so, a translator can simply hit the ‘+’ button to widen the context to as many sentences as needed.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The system breaks down a huge task into many small conquerable pieces (translation is a daunting task in general). By breaking the task down into sentence-sized pieces, it becomes less intimidating for a translator.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Having a pre-translated sentence saves a lot on typing and thinking time as all words/phrases are often present. The translator's main effort goes into rearranging words/phrases and adding/removing a word here and there. If the translator prefers to translate in the traditional way, he/she can simply click/tap the ‘Clear’ button and translate the sentence from scratch.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The system also allows for multiple translators to proofread a document and 'cast their votes' or make additional changes. In the end, the system knows which rendition of a sentence has the top number of votes and automatically includes the top-voted edit (which will be highlighted) in the output document. See Figure 2.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/vis_fig2.png&quot; alt=&quot;Figure 2  The system includes the edit with the most votes in the final document&quot; title=&quot;Figure 2  The system includes the edit with the most votes in the final document&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;If a translation engine for a specific language has not been trained yet, a translator can still get all the other benefits of the system by simply translating the presented English sentence from scratch, in other words without the benefit of a pre-translated sentence. In such cases the second sentence presented is just another copy of the English source sentence (which the translator clears and translates from scratch).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Pre-translations are generated by a neural machine translation algorithm. When a sufficient parallel corpus is available for a specific language an associated engine is created. The system also captures metrics on the performance of translators.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Visualization-story&quot;&gt;Visualization story&lt;a class=&quot;anchor-link&quot; href=&quot;#Visualization-story&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;I am sometimes met with skepticism about the value of having a pre-translation presented to a translator. The argument is that translators are then required to first read the pre-translated sentence, wasting time and energy in the process. It will take less effort to just allow translators to translate each sentence from scratch. Word artists as they are, they would prefer this creative freedom anyway.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;This sounds like a good argument. However, as outlined above, I maintain that providing a pre-translation has many benefits. From my point of view, providing a pre-translated sentence is the &lt;em&gt;key feature&lt;/em&gt; of the system. It is therefore important for me to have convincing evidence for the value provided by this functionality. In short, my main idea is to &lt;em&gt;convince&lt;/em&gt; the administrators at the non-profit that pre-translations add value to the efficiency of the translation process in general, and the performance of translators in particular. This situation defines the &lt;em&gt;contextual awareness&lt;/em&gt; of my visualization story.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The &lt;em&gt;independent&lt;/em&gt; variable in the story is the &lt;em&gt;BLEU score&lt;/em&gt; of a document. It is a measure of the quality of a machine-translated document (as evaluated against a human-translated version). The human-translated version that will be used to determine the BLEU score of each pre-translation, is simply the finalized document delivered by a translator. BLEU scores range from 0 to 100, which represents a perfect translation.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The first &lt;em&gt;dependent&lt;/em&gt; variable is the average number of &lt;em&gt;seconds needed to translate a sentence&lt;/em&gt;, for each completed document. The other dependent variable is the &lt;em&gt;percentage of sentences that needed to be changed&lt;/em&gt; during the translation of a document. These two variables function as performance metrics for the efficiency of the translation process. Understandably, this value varies widely due to a translator’s quality of work, level of dedication, brilliance, etc. Even so, my story will reveal that there is a gradual decline of mean time required per sentence as the BLEU score increases. It will also show that there is a progressive decline of percentage of sentences changed with increasing BLEU score.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The underlying rationale is therefore: The higher the BLEU score of a pre-translation, the less time a translator will spend on a sentence and the lower the percentage of changed sentences will be, for each document. I need to provide evidence of the correlation between providing a pre-translation (and the progressive increase in its quality, expressed as a BLEU score) and increased translator performance, expressed in the form of the dependent variables. This is supported by Tufte’s “Second principle for the analysis and presentation of data: Show causality, mechanism, explanation, systematic structure.” (Beautiful Evidence, E. R. Tufte, 2006).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The increase in the efficiency of the translation process in general, and translator performance in particular, leads to better productivity and reduced cost. Having pre-translations is therefore a valuable feature. My visualization story will be (in Berinato’s terms) &lt;em&gt;data-driven&lt;/em&gt; and &lt;em&gt;declarative&lt;/em&gt; (Good Charts - The HBR Guide to Making Smarter, More Persuasive Data Visualizations, Berinato, 2016).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Dataset&quot;&gt;Dataset&lt;a class=&quot;anchor-link&quot; href=&quot;#Dataset&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The dataset consists of 1935 records, each having 38 data fields. See Figure 3.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/vis_fig3.png&quot; alt=&quot;Figure 3  Summary of the dataset&quot; title=&quot;Figure 3  Summary of the dataset&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Source&quot;&gt;Source&lt;a class=&quot;anchor-link&quot; href=&quot;#Source&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The dataset belongs to the non-profit project known as “The Open Door” project. This is part of a larger missionary outreach initiative orchestrated by the client, a church in Canada. I am the custodian of the dataset and have permission to share it.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Row-Subset&quot;&gt;Row Subset&lt;a class=&quot;anchor-link&quot; href=&quot;#Row-Subset&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The subset of records that feature in my visualization story is a set of 60 records. Each of these records represent a translated document. This subset has been submitted with the rest of the deliverables and is called &lt;strong&gt;VisualizationProject.csv&lt;/strong&gt;. The remaining records relate to documents used for translation engine training, as well as documents in various stages of data preparation and could not be used for my purpose. From this subset, I had to throw out all records for which I did not have a BLEU score available. This left me with 48 records.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Column-Subset&quot;&gt;Column Subset&lt;a class=&quot;anchor-link&quot; href=&quot;#Column-Subset&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;From the 38 columns in the data set I selected a subset of 11 features to use in my visualization story. See Figure 4.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/vis_fig4.png&quot; alt=&quot;Figure 4  Summary of the subset of columns used&quot; title=&quot;Figure 4  Summary of the subset of columns used&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Other-preprocessing&quot;&gt;Other preprocessing&lt;a class=&quot;anchor-link&quot; href=&quot;#Other-preprocessing&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Next, I sorted the dataset by the BLEU column. After this, I partitioned the 11 variables into categorical and continuous variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Categorical:&lt;ul&gt;
&lt;li&gt;Descriptor, TE, Language, TotSent, ITS, Translator, todxx, todver&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Continuous:&lt;ul&gt;
&lt;li&gt;BLEU, secPerSen, percTopEdits&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Final-sub-selection-of-columns-for-my-visualization-story&quot;&gt;Final sub-selection of columns for my visualization story&lt;a class=&quot;anchor-link&quot; href=&quot;#Final-sub-selection-of-columns-for-my-visualization-story&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;After I finalized my visualization story, only the following columns remained:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;BLEU (called &lt;em&gt;quality of the pre-translation&lt;/em&gt; in the charts). This is the independent variable.&lt;/li&gt;
&lt;li&gt;secPerSen (called &lt;em&gt;seconds/sentence&lt;/em&gt; in the charts). This is the first dependent variable.&lt;/li&gt;
&lt;li&gt;percTopEdits (called &lt;em&gt;% sentences changed&lt;/em&gt; in the charts). &lt;em&gt;% sentences changed&lt;/em&gt; is presented as (1 – percTopEdits) to increase understanding. This is the second dependent variable.&lt;/li&gt;
&lt;li&gt;Translator (called &lt;em&gt;pre-translation type&lt;/em&gt; in the charts). There are three pre-translation types:&lt;ul&gt;
&lt;li&gt;No pre-translation&lt;/li&gt;
&lt;li&gt;Machine pre-translation&lt;/li&gt;
&lt;li&gt;Human pre-translation&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;A-note-on-pre-translation-type&quot;&gt;A note on pre-translation type&lt;a class=&quot;anchor-link&quot; href=&quot;#A-note-on-pre-translation-type&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;To tell my visualization story, I need sufficient context. I need to be able to answer the question “Compared to what?” This is in line with Tufte’s “First principle for the analysis and presentation of data: Show comparisons, contrasts, differences.” (E. R. Tufte, Beautiful Evidence, 2006). In effect, I need to put a “frame” of reference around the data that showcase the correlation between the quality of the machine pre-translations and the dependent variables. On the low side, I found a handy reference: I assigned a BLEU score of zero to all documents who did not have any pre-translation. It is fair to assign a zero score for such documents because the BLEU algorithm would not find any similarities between the original English document and the final translated document. In other words, the BLEU algorithm would have come up with a value of zero or a value close to zero.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;On the high side of the quality of pre-translations I was fortunate again to have some documents that had pre-translations that were provided by &lt;em&gt;humans&lt;/em&gt; (rather than machines). It so happened that a handful of documents had to be finalized by another proofreading. For these, the initial (human) translation was presented as if it were a machine pre-translation. Understandably, these pre-translations were of higher quality as they were deemed as final translations by the humans who translated them in the first place.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Tools&quot;&gt;Tools&lt;a class=&quot;anchor-link&quot; href=&quot;#Tools&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;I used python and a Jupyter notebook (called &lt;strong&gt;VisualizationProject.ipynb&lt;/strong&gt;) to prepare the visualizations. In addition, I used the following python libraries:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fast.ai&lt;/li&gt;
&lt;li&gt;Matplotlib&lt;/li&gt;
&lt;li&gt;Seaborn&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The dataset ingested by the Jupyter notebook is called &lt;strong&gt;VisualizationProject.csv&lt;/strong&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The notebook containing the complete development of the visualizations may be accessed here:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://nbviewer.jupyter.org/github/kobus78/VisualizationProject/blob/master/VisualizationProject.ipynb&quot;&gt;VisualizationProject.ipynb&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Principles-used-to-design-the-visualizations&quot;&gt;Principles used to design the visualizations&lt;a class=&quot;anchor-link&quot; href=&quot;#Principles-used-to-design-the-visualizations&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;I made use of some general principles throughout the design of the visualizations.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Layering-and-separation&quot;&gt;Layering and separation&lt;a class=&quot;anchor-link&quot; href=&quot;#Layering-and-separation&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Tufte makes the point:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;“Among the most powerful devices for reducing noise and enriching the content of displays is the technique of layering and separation, visually stratifying various aspects of the data.” And again: “What matters — inevitably, unrelentingly — is the proper relationship among information layers.” (E. R. Tufte, Envisioning Information, 1990, p. 53).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Valuing this advice, I ensured that the trend lines are on a higher or more prominent visual layer by making them more saturated and more intense. Sometimes I used color as well. The datapoints were made less saturated and intense, in effect placing them on the neutral “bottom” visual layer. The structural elements of the charts (axes, axes labels, and tick labels) were also demoted to the neutral layer.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Reduce-chart-junk-and-erase-non-data-ink&quot;&gt;Reduce chart junk and erase non-data-ink&lt;a class=&quot;anchor-link&quot; href=&quot;#Reduce-chart-junk-and-erase-non-data-ink&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;I tried to reduce chart junk and maximize data-ink ratio as much as possible by:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Removing all grid lines (the visualization story is not about the accurate location of values but rather about the trends).&lt;/li&gt;
&lt;li&gt;Removing all tick marks, for the same reason.&lt;/li&gt;
&lt;li&gt;Keeping x and y axes only on one side of the chart and using a neutral color (grey). There is no traditional frame around the chart.&lt;/li&gt;
&lt;li&gt;Not stating axes labels in the heading/sub-heading.&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Color&quot;&gt;Color&lt;a class=&quot;anchor-link&quot; href=&quot;#Color&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ul&gt;
&lt;li&gt;I used color sparingly. “Minimize the number of colors you use.” (Berinato)&lt;/li&gt;
&lt;li&gt;I generally used grey for the datapoints and structure elements, with a white background (Third rule of color in Tufte, Envisioning Information, 1990, p. 90).&lt;/li&gt;
&lt;li&gt;Where I do use color for datapoints (iterations 4, 5), I applied the fourth rule from Tufte, Envisioning Information, 1990, p. 90: “All colors of the main theme should be scattered like islands in the background color.”&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Other-best-practices&quot;&gt;Other best practices&lt;a class=&quot;anchor-link&quot; href=&quot;#Other-best-practices&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ul&gt;
&lt;li&gt;Simplicity. As Tufte puts it: “Graphical elegance is often found in simplicity of design and complexity of data.” (E. R. Tufte, The Visual Display of Quantitative Information, 2001, p. 177).&lt;/li&gt;
&lt;li&gt;Integrity guidelines (number representation proportional to numerical quantities, clear labeling, no design variation, dimensions on graphic should not exceed dimensions in data, graphics do not quote data out of context).&lt;/li&gt;
&lt;li&gt;Berinato’s four parts of a chart: Title, subtitle, visual field, source line.&lt;/li&gt;
&lt;li&gt;Multiple variables. I used at least three variables throughout. “Graphical excellence is nearly always multivariate.” (E. R. Tufte, The Visual Display of Quantitative Information, 2001, p. 51).&lt;/li&gt;
&lt;li&gt;“The Friendly Data Graphic” (E. R. Tufte, The Visual Display of Quantitative Information, 2001, p. 183).&lt;ul&gt;
&lt;li&gt;Words are spelled out (rather than abbreviations). I wanted to have the x-axis label “BLEU score” but decided instead that “Quality of pre-translation” would suffice and be more understandable.&lt;/li&gt;
&lt;li&gt;Words run from left to right (rather than vertically). I have all y-axis labels in horizontal format.&lt;/li&gt;
&lt;li&gt;Labels are placed on the graphic itself (rather than a legend). I have no legends.&lt;/li&gt;
&lt;li&gt;Cater for color-blindness. All graphics have support for color-blindness.&lt;/li&gt;
&lt;li&gt;Type is upper-and-lower case (rather than all capitals).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;“The principles for the analysis and presentation of data” (E. R. Tufte, Beautiful Evidence, 2006, p. 122).&lt;ul&gt;
&lt;li&gt;Show comparisons, contrasts, differences. In my final iteration, I show how &lt;em&gt;seconds/sentence&lt;/em&gt; and &lt;em&gt;% sentences changed&lt;/em&gt; varies with the quality of the pre-translation, compared among the cases &lt;em&gt;No pre-translation&lt;/em&gt;, &lt;em&gt;Machine pre-translation&lt;/em&gt;, and &lt;em&gt;Human pre-translation&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;Causality, Mechanism, Structure, Explanation. The trend lines (obtained by linear regression) in my visualizations clearly explain the mechanism and structure to achieve better translator performance: Increase the quality of the pre-translation. Increased translator performance is &lt;em&gt;caused&lt;/em&gt; by improving the quality of the pre-translation. They answer the question posed in the title: “Does it help to give translators a pre-translation?” The answer is clearly, “Yes, it does.”&lt;/li&gt;
&lt;li&gt;Show multivariate data. All the visualizations show multivariate data.&lt;/li&gt;
&lt;li&gt;Completely integrate words, numbers, images, diagrams. My graphics show labels next to the markers within the diagrams.&lt;/li&gt;
&lt;li&gt;Thoroughly describe the evidence. This document fulfills this requirement. I also included my name and the date on each visualization.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Iterations&quot;&gt;Iterations&lt;a class=&quot;anchor-link&quot; href=&quot;#Iterations&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;I had to go through five iterations before arriving at my final visualization.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Iteration-1:-Bubble-chart-scatter-plot&quot;&gt;Iteration 1: Bubble chart scatter plot&lt;a class=&quot;anchor-link&quot; href=&quot;#Iteration-1:-Bubble-chart-scatter-plot&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;According to Lane Harrison (and others) a scatter chart is the most effective chart type to show correlation (Good Charts - The HBR Guide to Making Smarter, More Persuasive Data Visualizations, Berinato, 2016). Please see Figure 5.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/vis_fig5.png&quot; alt=&quot;Figure 5  Bubble chart scatter plot&quot; title=&quot;Figure 5  Bubble chart scatter plot&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;Pros&quot;&gt;Pros&lt;a class=&quot;anchor-link&quot; href=&quot;#Pros&quot;&gt; &lt;/a&gt;&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ul&gt;
&lt;li&gt;Easy to understand, low clutter.&lt;/li&gt;
&lt;li&gt;Shows clear trend (intense green trend line color over neutral markers, green may also suggest “goodness” to some viewers which is part of the message). Exclude linear regression line’s parameters for the sake of simplicity.&lt;/li&gt;
&lt;li&gt;Multivariate (3 variables).&lt;/li&gt;
&lt;li&gt;Markers are, what Tufte calls, &lt;em&gt;multifunctioning&lt;/em&gt; graphical elements (E. R. Tufte, The Visual Display of Quantitative Information, 2001, p. 139). Each marker indicates 3 quantities.&lt;/li&gt;
&lt;li&gt;Low opacity allows good representation of the density of data points.&lt;/li&gt;
&lt;li&gt;Darker marker &lt;em&gt;edges&lt;/em&gt; make it easy to detect smaller markers (bottom right).&lt;/li&gt;
&lt;li&gt;Color palette allows for easy rendition on the printed page.
Color palette is sensitive to color blindness (red/green not used together).&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;Cons&quot;&gt;Cons&lt;a class=&quot;anchor-link&quot; href=&quot;#Cons&quot;&gt; &lt;/a&gt;&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ul&gt;
&lt;li&gt;% sentences changed is a 1-D quantity, but it is expressed by a 2-D circle area which is not ideal. In addition, the visual response to increases in circle size is not linear (Steven’s law).&lt;/li&gt;
&lt;li&gt;Large variation in marker size may allow the small markers (bottom right) to go unnoticed.&lt;/li&gt;
&lt;li&gt;Although simpler, this visualization does not use all the variables.&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Iteration-2:-Two-kernel-density-plots&quot;&gt;Iteration 2: Two kernel density plots&lt;a class=&quot;anchor-link&quot; href=&quot;#Iteration-2:-Two-kernel-density-plots&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Figure 6 shows a chart consisting of two kernel density plots.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/vis_fig6.png&quot; alt=&quot;Figure 6  Two kernel density plots!&quot; title=&quot;Figure 6  Two kernel density plots&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ul&gt;
&lt;li&gt;As pointed out by Berinato, the use of two y-axes on the same chart is not a good practice. Instead, I made separate charts – one for each dependent variable.&lt;/li&gt;
&lt;li&gt;I placed the charts below each other, rather than next to each other. This aligns with the principle that it is usually better to have the x-axis longer than the y-axis. (This is pointed out by Tufte when he compares it to what was called the golden rule during the middle-ages.) Berinato also makes a point of this and even suggests that it could be considered unethical to compress the x-axis too much to enhance a specific story.&lt;/li&gt;
&lt;li&gt;I decided to append the %-sign after each y-axis tick label to discourage a possible tendency to want to equate/compare the two vertical axes.&lt;/li&gt;
&lt;li&gt;I chose to not use blue as it may suggest the conventional “deepness” at the highest density points. Using red might have suggested “hotness” at these points, maybe not appropriate for my purpose either. Green looks pleasing and may suggest “goodness” to some viewers which is part of the message – the goodness of the falling trend.&lt;/li&gt;
&lt;li&gt;I made both charts the same color to minimize the number of colors used and to reduce cognitive burden.&lt;/li&gt;
&lt;li&gt;I restricted the number of density levels to 15 to reduce moiré vibration along the higher contour lines.&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;Pros&quot;&gt;Pros&lt;a class=&quot;anchor-link&quot; href=&quot;#Pros&quot;&gt; &lt;/a&gt;&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ul&gt;
&lt;li&gt;Shows clear trends. Exclude linear regression line’s parameters for the sake of simplicity.&lt;/li&gt;
&lt;li&gt;Might be more informative to technical viewers.&lt;/li&gt;
&lt;li&gt;Multivariate (3 variables).&lt;/li&gt;
&lt;li&gt;Color palette is sensitive to color blindness (red/green not used together).&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;Cons&quot;&gt;Cons&lt;a class=&quot;anchor-link&quot; href=&quot;#Cons&quot;&gt; &lt;/a&gt;&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ul&gt;
&lt;li&gt;Harder to understand for the average viewer due to the statistical principle of density of a distribution.&lt;/li&gt;
&lt;li&gt;Subtitle might not make sense to many readers.&lt;/li&gt;
&lt;li&gt;This kind of chart is ideal for larger datasets (my dataset is relatively small with 48 observations).&lt;/li&gt;
&lt;li&gt;Does not use all the variables.&lt;/li&gt;
&lt;li&gt;Not enough visual separation between trend lines and the data representation, especially at the highest density points.&lt;/li&gt;
&lt;li&gt;Some moiré vibration remains at the higher contour lines.&lt;/li&gt;
&lt;li&gt;Black color of trend lines converges with black coloration of highest density points introducing unnecessary weight (I could have used red but that would have introduced a color-blindness situation).&lt;/li&gt;
&lt;li&gt;Chart can benefit from what Tufte calls “subtraction of weight.”&lt;/li&gt;
&lt;li&gt;Edge fluting occurs between adjacent contours which is somewhat distracting.&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Iteration-3:-Two-scatter-plots&quot;&gt;Iteration 3: Two scatter plots&lt;a class=&quot;anchor-link&quot; href=&quot;#Iteration-3:-Two-scatter-plots&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Figure 7 shows a chart consisting of two scatter plots.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/vis_fig7.png&quot; alt=&quot;Figure 7  Two scatter plots&quot; title=&quot;Figure 7  Two scatter plots&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ul&gt;
&lt;li&gt;I picked the scatter plots over the density plots as a result of their relative advantages. Most viewers will find the scatter plots easier to understand.&lt;/li&gt;
&lt;li&gt;I again used two charts rather than two y-axes on the same chart for the reasons pointed out above.&lt;/li&gt;
&lt;li&gt;Again, I appended the %-sign after each y-axis tick label to discourage a possible tendency to want to equate/compare the two vertical axes.&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;Pros&quot;&gt;Pros&lt;a class=&quot;anchor-link&quot; href=&quot;#Pros&quot;&gt; &lt;/a&gt;&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ul&gt;
&lt;li&gt;Easy to understand scatter plots.&lt;/li&gt;
&lt;li&gt;Shows clear trends (intense green trend line color over neutral markers, green may also suggest “goodness” to some viewers which is part of the message). Exclude linear regression line’s parameters for the sake of simplicity.&lt;/li&gt;
&lt;li&gt;Multivariate (3 variables).&lt;/li&gt;
&lt;li&gt;Low opacity allows good representation of the density of data points.&lt;/li&gt;
&lt;li&gt;Color palette allows for easy rendition on the printed page.&lt;/li&gt;
&lt;li&gt;Color palette is sensitive to color blindness (red/green not used together).&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;Cons&quot;&gt;Cons&lt;a class=&quot;anchor-link&quot; href=&quot;#Cons&quot;&gt; &lt;/a&gt;&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ul&gt;
&lt;li&gt;Although simpler, does not use all the variables.&lt;/li&gt;
&lt;li&gt;Overlapping markers’ darker color competes with trend lines.&lt;/li&gt;
&lt;li&gt;Maybe marker size is too large?&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Iteration-4:-Two-scatter-plots-with-a-fourth-variable&quot;&gt;Iteration 4: Two scatter plots with a fourth variable&lt;a class=&quot;anchor-link&quot; href=&quot;#Iteration-4:-Two-scatter-plots-with-a-fourth-variable&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Figure 8 shows a chart consisting of two scatter plots and a fourth variable.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/vis_fig8.png&quot; alt=&quot;Figure 8  Two scatter plots with a fourth variable&quot; title=&quot;Figure 8  Two scatter plots with a fourth variable&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ul&gt;
&lt;li&gt;Introduce 4th variable (pre-translation type) by coloration of markers. This means I will use the &lt;em&gt;gestalt law of similarity&lt;/em&gt; to group translations by means of the similarity of their associated pre-translations. Using more variables is encouraged by Tufte’s “Third principle for the analysis and presentation of data: Show multivariate data.” (Beautiful Evidence, E. R. Tufte, 2006).&lt;/li&gt;
&lt;li&gt;Place comments/data labels close to markers instead of using a legend. This is suggested by Tufte’s “Fourth principle for the analysis and presentation of data: Completely integrate words, numbers, images, diagrams. (Beautiful Evidence, E. R. Tufte, 2006).&lt;/li&gt;
&lt;li&gt;Make opacity even lower to reduce the distracting effect of overlapping markers.&lt;/li&gt;
&lt;li&gt;Make marker size slightly smaller to reduce overlapping. Marker sizes cannot be too small, else color differences become harder to detect.&lt;/li&gt;
&lt;li&gt;Change color of trend line from green to black (green and red should not be used together for color-blindness problems).&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;Pros&quot;&gt;Pros&lt;a class=&quot;anchor-link&quot; href=&quot;#Pros&quot;&gt; &lt;/a&gt;&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ul&gt;
&lt;li&gt;Easy to understand scatter plots.&lt;/li&gt;
&lt;li&gt;Shows clear trends (intense trend line color over lighter markers; further decrease in opacity). Exclude linear regression line’s parameters for the sake of simplicity.&lt;/li&gt;
&lt;li&gt;Multivariate (now 4 variables) – one more variable, i.e. using all the relevant variables.&lt;/li&gt;
&lt;li&gt;Low opacity allows good representation of the density of data points.&lt;/li&gt;
&lt;li&gt;Color palette is sensitive to color blindness (red/green not used together).&lt;/li&gt;
&lt;li&gt;Provides good context, answers the question “Compared to what?”.&lt;/li&gt;
&lt;li&gt;Data labels close to markers (no legend). I considered spreading the labels over both graphics suggesting that they apply everywhere allowing me to only present them once. After trying this it seemed that it may cause confusion, so I added the three pre-translation type labels in &lt;em&gt;each&lt;/em&gt; scatter plot.&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;Cons&quot;&gt;Cons&lt;a class=&quot;anchor-link&quot; href=&quot;#Cons&quot;&gt; &lt;/a&gt;&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ul&gt;
&lt;li&gt;The use of colors maybe somewhat confusing, even though I have lowered the opacity. The varying overlaps between transparent markers lead to many shades of color. As Berinato says: “The more color differences they see, the more they have to work to figure out what the distinctions represent.”&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Iteration-5:-Two-scatter-plots-with-a-fourth-variable-and-better-coloration&quot;&gt;Iteration 5: Two scatter plots with a fourth variable and better coloration&lt;a class=&quot;anchor-link&quot; href=&quot;#Iteration-5:-Two-scatter-plots-with-a-fourth-variable-and-better-coloration&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Figure 9 shows a chart consisting of two scatter plots, a fourth variable and better coloration of the pre-translation types.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/vis_fig9.png&quot; alt=&quot;Figure 9  Two scatter plots with a fourth variable and better coloration&quot; title=&quot;Figure 9  Two scatter plots with a fourth variable and better coloration&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ul&gt;
&lt;li&gt;Make marker colors fully opaque. Now we don’t get the many shades due to overlapping of transparent markers, reducing the confusion.&lt;/li&gt;
&lt;li&gt;Make marker size even smaller to compensate for the increase in color intensity in order to maintain a proper weight balance between the trend line and the markers. Marker sizes cannot be too small, else color differences become harder to detect.&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;Pros&quot;&gt;Pros&lt;a class=&quot;anchor-link&quot; href=&quot;#Pros&quot;&gt; &lt;/a&gt;&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ul&gt;
&lt;li&gt;Easy to understand scatter plots.&lt;/li&gt;
&lt;li&gt;Shows clear trends (intense trend line color over lighter, less saturated markers and their labels). Exclude linear regression line’s parameters for the sake of simplicity.&lt;/li&gt;
&lt;li&gt;Multivariate (4 variables), using all the relevant variables.&lt;/li&gt;
&lt;li&gt;Color palette is sensitive to color blindness (red/green not used together).&lt;/li&gt;
&lt;li&gt;Provides good context, answers question “Compared to what?”&lt;/li&gt;
&lt;li&gt;Data labels close to markers (no legend).&lt;/li&gt;
&lt;li&gt;Good balance between trend lines and markers.&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;Cons&quot;&gt;Cons&lt;a class=&quot;anchor-link&quot; href=&quot;#Cons&quot;&gt; &lt;/a&gt;&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ul&gt;
&lt;li&gt;Density of data points is indicated less effectively. For example, it is not possible to show that all the &lt;em&gt;no pre-translation&lt;/em&gt; markers overlap (top left of bottom chart). Even so, this is my final choice for the visualization story.&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;REFERENCES&quot;&gt;REFERENCES&lt;a class=&quot;anchor-link&quot; href=&quot;#REFERENCES&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Berinato, S. (2016). Good Charts - The HBR Guide to Making Smarter, More Persuasive Data Visualizations.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Tufte, E. R. (1990). Envisioning Information.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Tufte, E. R. (2001). The Visual Display of Quantitative Information.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Tufte, E. R. (2006). Beautiful Evidence.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><category term="Translation Industry" /><category term="Visualization" /><category term="Python" /><category term="seaborn" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/portfolio/images/vis_fig9.png" /><media:content medium="image" url="/portfolio/images/vis_fig9.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Critique of a Data Product (Annual Report)</title><link href="/portfolio/translation%20industry/data%20product/visualization/2018/11/07/DataProductCritique.html" rel="alternate" type="text/html" title="Critique of a Data Product (Annual Report)" /><published>2018-11-07T00:00:00-06:00</published><updated>2018-11-07T00:00:00-06:00</updated><id>/portfolio/translation%20industry/data%20product/visualization/2018/11/07/DataProductCritique</id><content type="html" xml:base="/portfolio/translation%20industry/data%20product/visualization/2018/11/07/DataProductCritique.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2018-11-07-DataProductCritique.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Purpose&quot;&gt;Purpose&lt;a class=&quot;anchor-link&quot; href=&quot;#Purpose&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;This project involves the critique of the format and contents of the annual report of a non-profit. The annual report is about the operations of an initiative known as MissionTran. The need that the client has is to evaluate the current structure and contents of the annual report in the light of best-practice data communication standards, as well as obtain recommendations to improve the data communication aspects of it. A copy of the annual report may be found &lt;a href=&quot;/portfolio/AnnualReport2018.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Description-of-the-Data-Product&quot;&gt;Description of the Data Product&lt;a class=&quot;anchor-link&quot; href=&quot;#Description-of-the-Data-Product&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The MissionTran project uses a team of volunteers to proofread machine-translated sermons into a number of languages. Users proofread and correct machine-translated sentences one-by-one. Each proofreading effort to a sentence is called a &lt;em&gt;contribution&lt;/em&gt; and comes in the form of an &lt;em&gt;edit&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;There are two kinds of contributions: &lt;em&gt;Votes&lt;/em&gt; and &lt;em&gt;Creates&lt;/em&gt;. A vote contribution happens when the user decides to simply vote for the provided translation, i.e. the user deems the translation to be correct. A create contribution happens when the user edits the translation to correct it.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;There are a number of &lt;em&gt;bases&lt;/em&gt; upon which a contribution is made. For &lt;em&gt;vote&lt;/em&gt; contributions, the bases are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;By &lt;em&gt;accepting&lt;/em&gt; the machine-translated sentence as is (‘a’)&lt;/li&gt;
&lt;li&gt;By &lt;em&gt;creating&lt;/em&gt; a new edit (‘c’)&lt;/li&gt;
&lt;li&gt;By &lt;em&gt;topping&lt;/em&gt;, i.e. by voting for the current edit with the most votes (‘t’)&lt;/li&gt;
&lt;li&gt;By &lt;em&gt;picking&lt;/em&gt; another edit to vote for (‘p’)&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;For create contributions the bases are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;By &lt;em&gt;clearing&lt;/em&gt; (‘k’)&lt;/li&gt;
&lt;li&gt;By &lt;em&gt;modifying&lt;/em&gt; (‘m’)&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;A number of metrics are collected by the system:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;time used to make a vote contribution &lt;/li&gt;
&lt;li&gt;time used to make a create contribution &lt;/li&gt;
&lt;li&gt;vote time spent to proofread a complete translation (assignment)&lt;/li&gt;
&lt;li&gt;create time spent to proofread a complete translation (assignment)&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The components of the system may be summarized as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a Translation has many&lt;ul&gt;
&lt;li&gt;Sentences has many&lt;ul&gt;
&lt;li&gt;Edits has many&lt;ul&gt;
&lt;li&gt;Contributions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;a Translation has many&lt;ul&gt;
&lt;li&gt;Assignments has many&lt;ul&gt;
&lt;li&gt;Contributions&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;A User has many&lt;ul&gt;
&lt;li&gt;Assignments&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Finding-Your-Purpose-and-Message&quot;&gt;Finding Your Purpose and Message&lt;a class=&quot;anchor-link&quot; href=&quot;#Finding-Your-Purpose-and-Message&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Actionable-data&quot;&gt;Actionable data&lt;a class=&quot;anchor-link&quot; href=&quot;#Actionable-data&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The location of &lt;em&gt;actionable data&lt;/em&gt; in the annual report is not straightforward. Most charts seem to simply dump data. Only some charts seem to suggest its use as a trigger for action. One such chart is the report's Figure 12 which shows the number of contributions by username by contribution kind:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/dpc_fig12.png&quot; alt=&quot;&quot; title=&quot;Figure 12:  Number of contributions by username by contribution kind&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;This chart shows how productive users were by showing the total number of contributions each made. This number is further broken down into &lt;em&gt;vote&lt;/em&gt; contribution and &lt;em&gt;create&lt;/em&gt; contributions.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Balance&quot;&gt;Balance&lt;a class=&quot;anchor-link&quot; href=&quot;#Balance&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;There should be a balance between the representations for the &lt;em&gt;data&lt;/em&gt;, the &lt;em&gt;author&lt;/em&gt;, and the &lt;em&gt;audience&lt;/em&gt;. In the report, the balance is tilted strongly in the direction of the data. There is a strong impression that all available data has simply been dropped into the document. The consumer is then expected to pick out pieces for consumption.&lt;/p&gt;
&lt;p&gt;The document does not reveal much about the objectives of the &lt;em&gt;author&lt;/em&gt;, except maybe to communicate the progress of the translation team.&lt;/p&gt;
&lt;p&gt;There is an implication that the &lt;em&gt;audience’s&lt;/em&gt; need is simply to know the progress of the project.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Audience-factors&quot;&gt;Audience factors&lt;a class=&quot;anchor-link&quot; href=&quot;#Audience-factors&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The &lt;em&gt;role&lt;/em&gt; of the audience seems to be stakeholders interested in the status of the project. There is hardly any structure that would make it easy and direct to answer high-priority questions.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;workflow&lt;/em&gt; of the audience might be catered for as the information is presented in the form of charts that can be accessed easily in an office setting.&lt;/p&gt;
&lt;p&gt;There is no elaboration on the &lt;em&gt;data comfort and skills&lt;/em&gt; of the audience. The author does point out that “The workings of the translation system will not be presented here as most stakeholders are well aware of the details.” This suggests that the audience possess the &lt;em&gt;industry and data expertise&lt;/em&gt; and does not need embedded explanations.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Information-Discrimination&quot;&gt;Information Discrimination&lt;a class=&quot;anchor-link&quot; href=&quot;#Information-Discrimination&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;There is no identifiable core problem/theme. The report is simply a series of charts that try to convey the performance of the team. There is not a good separation of &lt;em&gt;reporting&lt;/em&gt; from &lt;em&gt;exploration&lt;/em&gt;. Some charts, like Figure 10, are totally unnecessary. This chart should have at least been pushed to an appendix but there is no appendix. Here is Figure 10:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/dpc_fig10.png&quot; alt=&quot;&quot; title=&quot;Figure 10:  Joint distribution of assignment total vote/create time&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Figure 10 seems much too technical and does not belong with the remaining charts.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;There is a section (at the end) on “Day of Week.” The chart in this section (Figure 13), breaks down contribution effort (in seconds) by day-of-week by contribution kind:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/dpc_fig13.png&quot; alt=&quot;&quot; title=&quot;Figure 13:  Distribution of effort by day-of-week by contribution kind&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;It seems unnatural to do this kind of breakdown. Why would distribution of effort per contribution vary with day of week. I cannot think of a fundamental reason why a user’s need for time to proofread a specific sentence would vary by day-of-week. This is not a meaningful chart. It should be thrown out.&lt;/p&gt;
&lt;p&gt;With a stretch of imagination one could think of the following scenario. Let us say on Mondays, there are always a series of celebrations in the large conference room next door. This might be distracting to the proofreaders. This kind of scenario, however, should be investigated in the 'backroom'. It should not be part of regular annual report.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;If the author happens to insist on keeping this chart, it should be changed considerably. Its current form hardly shows the required information. The presence of a few outliers compresses the real data so much that nothing is revealed. Here is a first suggestion. The weekdays are ordered, the axis labels are improved, and a measure of transparency is given to the markers. This helps to show how much overlap of points there are:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/dpc_fig13_replacement1.png&quot; alt=&quot;&quot; title=&quot;Figure 13-1:  Replacement suggestion 1&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Even better, we can provide a small amount of sideways random jitter to help reveal the overlap:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/dpc_fig13_replacement2.png&quot; alt=&quot;&quot; title=&quot;Figure 13-2:  Replacement suggestion 2&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;If we remove the outliers, the chart becomes more effective:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/dpc_fig13_replacement3.png&quot; alt=&quot;&quot; title=&quot;Figure 13-3:  Replacement suggestion 3&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We may also use a boxplot which is commonly used in a case like this:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/dpc_fig13_replacement4.png&quot; alt=&quot;&quot; title=&quot;Figure 13-4:  Replacement suggestion 4&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;A violin plot will also be effective:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/dpc_fig13_replacement5.png&quot; alt=&quot;&quot; title=&quot;Figure 13-5:  Replacement suggestion 5&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The latest plots all reveal that there is no real variation in contribution effort per sentence over day-of-week. This was certainly not evident from the original chart.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Defining-Meaningful-and-Actionable-Metrics&quot;&gt;Defining Meaningful and Actionable Metrics&lt;a class=&quot;anchor-link&quot; href=&quot;#Defining-Meaningful-and-Actionable-Metrics&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The area of metrics is the strong point of the document. Metrics are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;not too &lt;em&gt;simplistic&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;not overly &lt;em&gt;complex&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;not too &lt;em&gt;many&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;none are &lt;em&gt;vanity&lt;/em&gt; metrics&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The metrics have a &lt;em&gt;common interpretation&lt;/em&gt;, e.g.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;time used to make a vote contribution &lt;/li&gt;
&lt;li&gt;time used to make a create contribution &lt;/li&gt;
&lt;li&gt;vote time spent to proofread a complete translation (assignment)&lt;/li&gt;
&lt;li&gt;create time spent to proofread a complete translation (assignment)&lt;/li&gt;
&lt;li&gt;total vote time spent during an assignment.&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Metrics are mostly &lt;em&gt;actionable&lt;/em&gt;; e.g. how much time is spent per sentence.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Metrics are &lt;em&gt;accessible&lt;/em&gt; and also &lt;em&gt;transparent&lt;/em&gt; with simple calculations.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Creating-Structure-and-Flow-to-your-Data-Products&quot;&gt;Creating Structure and Flow to your Data Products&lt;a class=&quot;anchor-link&quot; href=&quot;#Creating-Structure-and-Flow-to-your-Data-Products&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;There is a simplistic logical structure and &lt;em&gt;no narrative&lt;/em&gt; at all – very poor!&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The author could have chosen one of many narrative flows, e.g. to&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;show how machine-translated assisted proofreading is more efficient than humans translating from scratch&lt;/li&gt;
&lt;li&gt;show how the productivity of users varies by language&lt;/li&gt;
&lt;li&gt;show the work patterns of users, i.e. a steady amount each workday, or only on one day per week, etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;There is no meaningful flow however. There is no notion of the “guided safari” storytelling, or even of a traditional story. This is a glaring weakness in the data product.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Designing-Attractive,-Easy-to-understand-Data-Products&quot;&gt;Designing Attractive, Easy-to-understand Data Products&lt;a class=&quot;anchor-link&quot; href=&quot;#Designing-Attractive,-Easy-to-understand-Data-Products&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The presentation looks rough and unfinished (low aesthetic value). Given the author’s comment about the stakeholders’ understanding of the workings of the system, this might be overlooked somewhat.&lt;/p&gt;
&lt;p&gt;There is no need for &lt;em&gt;connectivity&lt;/em&gt;, &lt;em&gt;data detail&lt;/em&gt;, &lt;em&gt;interactivity&lt;/em&gt;, and &lt;em&gt;mobility&lt;/em&gt; as this is an annual report.&lt;/p&gt;
&lt;p&gt;The use of &lt;em&gt;color&lt;/em&gt; in charts was mostly acceptable.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The author often did not choose the most appropriate chart type, e.g. in Figures 1 and 2 a line chart should have been used instead of the heat maps:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/dpc_fig1.png&quot; alt=&quot;&quot; title=&quot;Figure 1:  Contributions&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;If the author insists on having a heat map, it could have been made simpler and more effective:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/dpc_fig1_replacement.png&quot; alt=&quot;&quot; title=&quot;Figure 1-1:  Replacement suggestion&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The chart type for Figure 6 was chosen poorly and the presence of a few outliers hide most of the real data:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/dpc_fig6.png&quot; alt=&quot;&quot; title=&quot;Figure 6:  Distribution of effort by contribution kind by sentence type&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The following chart would have been more effective (after removing the outliers):&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/dpc_fig6_replacement.png&quot; alt=&quot;&quot; title=&quot;Figure 6-1:  Replacement suggestion&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Creating-Dialogue-with-Your-Data-Products&quot;&gt;Creating Dialogue with Your Data Products&lt;a class=&quot;anchor-link&quot; href=&quot;#Creating-Dialogue-with-Your-Data-Products&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In general, there is not a lot of &lt;em&gt;chart junk&lt;/em&gt;. The author mostly used sufficient &lt;em&gt;contrast&lt;/em&gt;. &lt;em&gt;Readability of labels&lt;/em&gt; are generally bad. A serious omission is that no chart has a heading! For example, Figure 11:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/dpc_fig11.png&quot; alt=&quot;&quot; title=&quot;Figure 11:  Distribution of effort&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The time units are always in seconds which is often inappropriate. Although the meaning of variables is explained, it is bothersome that they appear in their “software” form on axes, as in Figure 8:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/dpc_fig8.png&quot; alt=&quot;&quot; title=&quot;Figure 8:  Assignment total create time versus translation sentence count&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Sentence types are not defined anywhere.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;em&gt;Sorting for comprehension&lt;/em&gt; could have been done in Figure 5 and 12:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/dpc_fig5.png&quot; alt=&quot;&quot; title=&quot;Figure 5:  Distribution of effort by language by sentence type&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/dpc_fig12.png&quot; alt=&quot;&quot; title=&quot;Figure 12:  Number of contributions by username by contribution kind&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In addition, monotone &lt;em&gt;color variants&lt;/em&gt; could have been used in Figure 12.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Summary&quot;&gt;Summary&lt;a class=&quot;anchor-link&quot; href=&quot;#Summary&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The annual report showcases a lot of data. The disappointing part is that it has been done ineffectively and unprofessionally.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The main weaknesses are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;No purpose or message&lt;/li&gt;
&lt;li&gt;Use of data indiscriminately&lt;/li&gt;
&lt;li&gt;Weak structure and no narrative flow &lt;/li&gt;
&lt;li&gt;No mentionable design&lt;/li&gt;
&lt;li&gt;Will not trigger conversation and dialogue in a straightforward way&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The one strength:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Meaningful metrics&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><category term="Translation Industry" /><category term="Data Product" /><category term="Visualization" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/portfolio/images/dpc_fig1_replacement.png" /><media:content medium="image" url="/portfolio/images/dpc_fig1_replacement.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Time-series Regression (Deep Learning to Detect Change Points)</title><link href="/portfolio/deep%20learning/time%20series/image%20regression/cnn/rnn/python/fastai/2017/07/03/TimeSeriesRegression.html" rel="alternate" type="text/html" title="Time-series Regression (Deep Learning to Detect Change Points)" /><published>2017-07-03T00:00:00-05:00</published><updated>2017-07-03T00:00:00-05:00</updated><id>/portfolio/deep%20learning/time%20series/image%20regression/cnn/rnn/python/fastai/2017/07/03/TimeSeriesRegression</id><content type="html" xml:base="/portfolio/deep%20learning/time%20series/image%20regression/cnn/rnn/python/fastai/2017/07/03/TimeSeriesRegression.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2017-07-03-TimeSeriesRegression.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Purpose&quot;&gt;Purpose&lt;a class=&quot;anchor-link&quot; href=&quot;#Purpose&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;This project investigates whether it is feasible to use Convolutional Neural Networks (CNNs) to perform time-series regression, rather than more traditional methods. Before a CNN can be used, the time-series has to be converted to a spatial signal point, i.e. an image.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Problem&quot;&gt;Problem&lt;a class=&quot;anchor-link&quot; href=&quot;#Problem&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The problem for our purpose is the following: Given a time-series, detect the points in time where changes occur in the value of the time-series. The formal name for this is &lt;em&gt;change point detection&lt;/em&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Change-Point-Detection&quot;&gt;Change Point Detection&lt;a class=&quot;anchor-link&quot; href=&quot;#Change-Point-Detection&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In Python, a promising package for the detection of change points, is the &lt;a href=&quot;https://github.com/deepcharles/ruptures&quot;&gt;ruptures&lt;/a&gt; package by Truong, Oudre and Vayatis (2020). This package includes a number of implemented algorithms which are covered in the associated publication called &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0165168419303494?via%3Dihub&quot;&gt;&quot;Selective review of offline change point detection methods&quot;&lt;/a&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The authors provide the following three highlights:&lt;/p&gt;
&lt;blockquote&gt;&lt;ul&gt;
&lt;li&gt;A structured and didactic review of more than 140 articles related to offline change point detection. Thanks to the methodological framework proposed in this survey, all methods are presented as the combination of three functional blocks, which facilitates comparison between the different approaches.&lt;/li&gt;
&lt;li&gt;The survey provides details on mathematical as well as algorithmic aspects such as complexity, asymptotic consistency, estimation of the number of changes, calibration, etc.&lt;/li&gt;
&lt;li&gt;The review is linked to a Python package that includes most of the pre- sented methods, and allows the user to perform experiments and bench- marks.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;... and this abstract:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;This article presents a selective survey of algorithms for the offline detection of multiple change points in multivariate time series. A general yet structuring methodological strategy is adopted to organize this vast body of work. More precisely, detection algorithms considered in this review are characterized by three elements:a cost function, a search method and a constraint on the number of changes. Each of those elements is described, reviewed and discussed separately. Implementations of the main algorithms described in this article are provided within a Python package called ruptures.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;This summary comes from the GitHub site:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;strong&gt;ruptures&lt;/strong&gt; is a Python library for off-line change point detection. This package provides methods for the analysis and segmentation of non-stationary signals. Implemented algorithms include exact and approximate detection for various parametric and non-parametric models. ruptures focuses on ease of use by providing a well-documented and consistent interface. In addition, thanks to its modular structure, different algorithms and models can be connected and extended within this package.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Here is an example of how the ruptures package might be used. The following code generates a noisy piecewise constant signal. Then it performs a penalized kernel change point detection and displays the results. Alternating colors outline the true regimes and vertical dashed lines mark the detected change points.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;ruptures&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;rpt&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# generate signal&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;n_bkps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# number of breakpoints&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;signal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bkps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rpt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pw_constant&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_samples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_bkps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;noise_std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# detection&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;algo&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rpt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Pelt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;rbf&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;signal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;algo&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pen&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# display&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rpt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;display&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;signal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bkps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/tsr_fig1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Novel-approach-to-Change-Point-Detection&quot;&gt;Novel approach to Change Point Detection&lt;a class=&quot;anchor-link&quot; href=&quot;#Novel-approach-to-Change-Point-Detection&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In the approach of this project, we do not have access to first principles models (i.e. white-box models). Instead we rely on a series of convolution neural networks (CNNs) to learn an empirical model (i.e. black-box model).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The &lt;em&gt;predictor points&lt;/em&gt; for our problem are not structured vectors as is common in the case of structured data analysis. Here we have to use a time-series or sequence of scalar-valued predictor points and have the model learn the associated &lt;em&gt;target point&lt;/em&gt; which is a vector of scalars that identify the times of the change points in each case.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In the deep learning subfield, it is common to use a &lt;em&gt;Recurrent Neural Network&lt;/em&gt; (RNN) for this kind of problem. Examples of this, but for the case of time-series classification, rather than regression, are Hüsken and Stagge (2003), and also Sun, Di, and Fang (2019). However, the training of an RNN can be challenging due to high demands on computing resources including processing power, processing time, and memory. There is also the vanishing/exploding gradients problem, addressed in various ways, but is often still lurking in the background.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Consider how easy it is for the human visual system to handle this problem, and in a fraction of a second. In fact, this is exactly how analysts sometimes do their change detections manually. This suggests that we might benefit from tapping into the biological mechanisms for analyzing visual data (i.e. images). Recently, some researchers started adopting this insight. See, for example, Wang and Oates (2015a, 2015b, 2015c) and Wang, Zan and Oates (2017). The essence of this line of thought is the following: Instead of analyzing a sequence of 1-D or scalar-valued &lt;em&gt;temporal&lt;/em&gt; data points, we transform them into a single 2-D or matrix-valued &lt;em&gt;spatial&lt;/em&gt; data point. The spatial data point is simply an image which means the time-series signal has been transformed into an image. This allows for the application of a large body of relatively well-developed computer vision techniques to the above-stated problem. Most of these techniques center around the &lt;em&gt;Convolutional Neural Network&lt;/em&gt; (CNN). In summary, the &lt;em&gt;time-series regression&lt;/em&gt; problem has been converted to an &lt;em&gt;image regression&lt;/em&gt; problem.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We will use a simple transformation technique to transform the time-series into an image. After transformation of the synthetic training dataset of time-series, a CNN will be trained which will serve as a regressor. &lt;em&gt;Transfer learning&lt;/em&gt; will be used to speed up the training of the CNNs.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Value-Proposition&quot;&gt;Value Proposition&lt;a class=&quot;anchor-link&quot; href=&quot;#Value-Proposition&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;This project seeks to provide value in a number of ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Demonstrates how synthetic time-series can be turned into images for more effective regression&lt;/li&gt;
&lt;li&gt;Demonstrates how transfer learning greatly speedup the time to train a CNN neural network for the regression of time-series&lt;/li&gt;
&lt;li&gt;Satisfies my personal interest to think &quot;outside the box&quot; and apply existing technology in novel ways&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Data-Source&quot;&gt;Data Source&lt;a class=&quot;anchor-link&quot; href=&quot;#Data-Source&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;This project makes use of synthetic data. Each time-series is generated randomly and has the following properties:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Consists of two changes, called actions&lt;/li&gt;
&lt;li&gt;Starts at a time (beg_t=0) at a random level (beg_l)&lt;/li&gt;
&lt;li&gt;The first action starts at a later random time (actn1_t) at a random level (actn1_l=beg_l)&lt;/li&gt;
&lt;li&gt;The first action ends at a later random time (comp1_t) at a random level (comp1_l)&lt;/li&gt;
&lt;li&gt;The second action starts at a later random time (actn2_t) at a random level (actn1_l)&lt;/li&gt;
&lt;li&gt;The second action ends at a later random time (comp1_t) at a random level (comp2_l)&lt;/li&gt;
&lt;li&gt;Ends at a time (end_t=0) at a random level (comp2_l=end_l)&lt;/li&gt;
&lt;li&gt;Is captured in a .png file&lt;ul&gt;
&lt;li&gt;file naming uses the same names as those used in the &lt;a href=&quot;https://closedloopai.github.io/portfolio/2019/09/15/TimeSeriesClassification.html&quot;&gt;Time Series Classification Project&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;this is just a convenience and no data from the other project is used&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Figure 2 shows examples of randomly generated synthetic time-series.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/tsr_fig2.png&quot; alt=&quot;Figure 2: Examples of synthetic time-series&quot; title=&quot;Figure 2: Examples of synthetic time-series&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Modeling&quot;&gt;Modeling&lt;a class=&quot;anchor-link&quot; href=&quot;#Modeling&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In this section, we will look at the important concept of time-series regression and how it relates to two of the most important deep learning architectures: Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;There seem to be advantages to the use of deep learning to perform regression on time-series. One specific advantage is the ability to detect time invariant characteristics. This is similar to how spatially invariant filters detect patterns in images.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Recurrent-Neural-Networks-(RNNs)&quot;&gt;Recurrent Neural Networks (RNNs)&lt;a class=&quot;anchor-link&quot; href=&quot;#Recurrent-Neural-Networks-(RNNs)&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Recurrent layers in RNNs are described by the equations:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
$$ \Large
\begin{align}
\mathbf{a}^{&amp;lt;t&amp;gt;} &amp;amp;= g(\mathbf{W}_{aa} \mathbf{a}^{&amp;lt;t-1&amp;gt;} + \mathbf{W}_{ax} \mathbf{x}^{&amp;lt;t&amp;gt;} + \mathbf{b}_a) \\
\hat{\mathbf{y}}^{&amp;lt;t&amp;gt;}  &amp;amp;= g(\mathbf W_{ya}\mathbf{a}^{&amp;lt;t&amp;gt;} + \mathbf{b_y})
\end{align}
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The parameters or weights that undergo training are captured in a number of &lt;em&gt;filters&lt;/em&gt; or &lt;em&gt;kernels&lt;/em&gt;. The &lt;em&gt;feedback&lt;/em&gt; filter is $\mathbf{W}_{aa}$, the &lt;em&gt;input&lt;/em&gt; filter $\mathbf{W}_{ax}$, and the &lt;em&gt;output&lt;/em&gt; filter $\mathbf{W}_{ya}$. The &lt;em&gt;signal&lt;/em&gt; is the data that are used as examples during training. The symbols $\mathbf{x}^{&amp;lt;t&amp;gt;}$ and $\mathbf{\hat{y}}^{&amp;lt;t&amp;gt;}$ represent the input and output signals respectively. The hidden state, or internal signal, is given by $\mathbf{a}^{&amp;lt;t&amp;gt;}$. The filters are matrices while the signals are vector-valued. There is often a single layer in an RNN. Note, however, that this architecture is recursive. This means that each time-step could be considered a separate layer in time.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In the context of our regression problem, the input is a sequence of scalar-valued continuous predictor points. The output (target point) is a vector of scalar-valued continuous values (after being processed by a sigmoid function). The values of a target point are the two time values where changes occur in the level of the time-series. This type of RNN is also known as a &lt;em&gt;many-to-one&lt;/em&gt; RNN because a series of input data points leads to a single output datapoint.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Disadvantages-of-RNNs&quot;&gt;Disadvantages of RNNs&lt;a class=&quot;anchor-link&quot; href=&quot;#Disadvantages-of-RNNs&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In 2015 RNNs made a dramatic come-back (Karpathy, 2015). A year or two after this the &lt;em&gt;ResNet&lt;/em&gt; (He, Zhang, Ren &amp;amp; Sun, 2016) and the &lt;em&gt;attention&lt;/em&gt; mechanism (Xu et al., 2015) were invented. This provided an expanded context for the evaluation of RNNs and the Long Short Term Memory (LSTM). A further two years later, arguably, saw the beginning of a measure of decline of the popularity of the RNN and the LSTM in some disciplines.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Culurciell (2018) points out some shortcomings of RNNs in his article “The fall of RNN / LSTM.” In this regard, he mentions the problem of vanishing gradients and that RNNs are not hardware friendly. RNNs are also harder to train and parallelize.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Time-series-transformation-to-images&quot;&gt;Time-series transformation to images&lt;a class=&quot;anchor-link&quot; href=&quot;#Time-series-transformation-to-images&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Before we look at Convolutional Neural Networks (CNNs), we will mention how a time-series can be transformed into an image. The purpose of this transformation is to enable computers to “visually” recognize and perform regression on the time-series signal. By doing this transformation we can take advantage of the impressive successes of deep learning architectures (using CNNs) in computer vision. This allows us to identify the structure of a time-series, leading to, hopefully, more effective regression.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;To create this spatial (or image) representation of the time-series, we will use a dark pen on a light background. No graphical annotations will be included, i.e. graphic frame, tick marks, tick labels, axis labels, and heading. Annotations will make the learning process unnecessarily complex.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Convolutional-Neural-Networks-(CNNs)&quot;&gt;Convolutional Neural Networks (CNNs)&lt;a class=&quot;anchor-link&quot; href=&quot;#Convolutional-Neural-Networks-(CNNs)&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Convolutional Neural Networks are structured in a way that enables them to exploit translational invariance. They extract features through receptive fields. Significant weight sharing drastically reduces the number of parameters involved in training them. They are the state-of-the-art architecture in the handling of computer vision tasks.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;A CNN consists of different types of layers. The most important ones are &lt;em&gt;convolution&lt;/em&gt;, &lt;em&gt;pooling&lt;/em&gt;, and &lt;em&gt;dense&lt;/em&gt; layers. Convolution and pooling layers often alternate during the initial layers. Near the end, a number of dense layers usually occurs which often ends with a sigmoid or softmax layer in the case of a classification CNN.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;Convolution-layers&quot;&gt;Convolution layers&lt;a class=&quot;anchor-link&quot; href=&quot;#Convolution-layers&quot;&gt; &lt;/a&gt;&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Convolution layers are described by the equations:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
$$ \Large
\begin{align}
\mathbf{Z}^{[l]} &amp;amp;= \mathbf{W}^{[l]} \ast \mathbf{A}^{[l-1]} + \mathbf{b}^{[l]} \qquad(1) \\
\mathbf{A}^{[l]} &amp;amp;= g^{[l]}(\mathbf{Z}^{[l]}) \qquad\qquad\qquad\qquad\,(2)
\end{align}
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The &lt;em&gt;filter&lt;/em&gt; (also called &lt;em&gt;kernel&lt;/em&gt;) of a convolution layer is indicated by $\mathbf{W}^{[l]}$ where $l$ is the index of the layer. $\mathbf{W}^{[l]}$ is tensor-valued with each element $w_{ijk}^{[l]} \in \mathbb{R}$. The values of $w_{ijk}^{[l]}$ are learned by the training process. The &lt;em&gt;dimensions&lt;/em&gt; (or &lt;em&gt;shape&lt;/em&gt;) of $\mathbf{W}^{[l]}$ are $n_C^{[l-1]} \times f^{[l]} \times f^{[l]}$ where $n_C^{[l-1]}$ is the &lt;em&gt;number of filters&lt;/em&gt; (also the &lt;em&gt;number of channels&lt;/em&gt;) in the previous layer. The filter size is indicated by $f^{[l]}$. If we have multiple filters in layer $l$, the dimensions of $\mathbf{W}^{[l]}$ expand to $n_C^{[l]} \times n_C^{[l-1]} \times f^{[l]} \times f^{[l]}$ making $\mathbf{W}^{[l]}$ a vector of tensors.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The &lt;em&gt;activations&lt;/em&gt; in layer $l$ are represented by another tensor $\mathbf{A}^{[l]}$. For each layer, we distinguish between the &lt;em&gt;input&lt;/em&gt; activations, $\mathbf{A}^{[l-1]}$, and &lt;em&gt;output&lt;/em&gt; activations, $\mathbf{A}^{[l]}$. The dimensions of $\mathbf{A}^{[l-1]}$ are $n_C^{[l-1]} \times n_H^{[l-1]} \times n_W^{[l-1]}$ where $n_C^{[l-1]}$ is the number of channels in the previous layer, $n_H^{[l-1]}$ the &lt;em&gt;height&lt;/em&gt; of the image in the previous layer, and $n_W^{[l-1]}$ the &lt;em&gt;width&lt;/em&gt; of the image in the previous layer.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The dimensions of $\mathbf{A}^{[l]}$ are $n_C^{[l]} \times n_H^{[l]} \times n_W^{[l]}$  where&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
$$ \Large
n_H^{[l]}=\Bigg \lfloor \frac{n_H^{[l-1]}+2p^{[l]}-f^{[l]}}{s^{[l]}} + 1 \Bigg \rfloor\qquad\qquad\qquad\qquad(3)
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
$$ \Large
n_W^{[l]}=\Bigg \lfloor \frac{n_W^{[l-1]}+2p^{[l]}-f^{[l]}}{s^{[l]}} + 1 \Bigg \rfloor\qquad\qquad\qquad\qquad(4)
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The &lt;em&gt;padding&lt;/em&gt; size in layer $l$ is indicated by $p^{[l]}$. The &lt;em&gt;stride&lt;/em&gt; is represented by $s^{[l]}$. If we make use of mini-batch training, the dimensions of $\mathbf{A}^{[l]}$ will expand to $n_C^{[l]} \times n_H^{[l]} \times n_W^{[l]}$ where $m$ is the mini-batch size. In this case $\mathbf{A}^{[l]}$ becomes a vector of tensors.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The &lt;em&gt;bias&lt;/em&gt; vector in layer $l$ is indicated by $\mathbf{b}^{[l]}$. The &lt;em&gt;convolution operation&lt;/em&gt; is indicated by the symbol $\ast$. Equation 1 describes the &lt;em&gt;linear&lt;/em&gt; part of the convolution. The &lt;em&gt;activation&lt;/em&gt; part is captured by Equation 2. The &lt;em&gt;activation function&lt;/em&gt;, $g^{[l]}$, is often a rectified linear unit (ReLU).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Equations 1 and 2 can be combined into a single equation:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
$$ \Large
\mathbf{A}^{[l]} = g^{[l]}(\mathbf{W}^{[l]} \ast \mathbf{A}^{[l-1]} + \mathbf{b}^{[l]}) \qquad\qquad\qquad\qquad\,(5)
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://towardsdatascience.com/gentle-dive-into-math-behind-convolutional-neural-networks-79a07dd44cf9&quot;&gt;Piotr  Skalski&lt;/a&gt;
 (2019) has a blog that makes the operation of CNNs easy to understand. It includes a number of animations that provide valuable insight.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;Pooling-layers&quot;&gt;Pooling layers&lt;a class=&quot;anchor-link&quot; href=&quot;#Pooling-layers&quot;&gt; &lt;/a&gt;&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The filter of a pooling layer has no weights that need to be trained. It has a filter size $f^{[l]}$, and a stride $s^{[l]}$. The value for padding is almost always zero, $p^{[l]}=0$. The filter performs an aggregation operation as it slides over the activation signal. This operation is performed on each of the input channels independently. Types of aggregation operations are &lt;em&gt;maximum&lt;/em&gt; and &lt;em&gt;average&lt;/em&gt;. The most common type is the &lt;em&gt;max-pool&lt;/em&gt; layer. As the $f \times f$ filter slides across the image, it picks the maximum activation for each position and sends that to the output image, thereby reducing the size of the input image according to equations (3) and (4). There is also no activation function. This means pooling layers are described by the following equation:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
$$ \Large
\mathbf{A}^{[l]}=\text{max}(\mathbf{A}^{[l-1]}) \qquad\qquad\qquad(6)
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;Dense-layers&quot;&gt;Dense layers&lt;a class=&quot;anchor-link&quot; href=&quot;#Dense-layers&quot;&gt; &lt;/a&gt;&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;em&gt;Dense layers&lt;/em&gt; always form the last few layers of a CNN that performs classification. These layers are also called &lt;em&gt;fully connected&lt;/em&gt; layers. Dense layers are described by Equations 7 and 8:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
$$ \Large
\begin{align}
\mathbf{z}^{[l]} &amp;amp;= \mathbf{W}^{[l]}\mathbf{a}^{[l-1]}+\mathbf{b}^{[l]}\qquad\qquad (7)\\
\mathbf{a}^{[l]} &amp;amp;= g^{[l]}(\mathbf{z}^{[l]})\qquad\qquad\qquad\qquad\quad(8)
\end{align}
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Notice the absence of the convolution operator, which have been replaced by matrix multiplication.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The &lt;em&gt;filter&lt;/em&gt; of a dense layer is indicated by $\mathbf{W}^{[l]}$ where $l$ is the index of the layer. $\mathbf{W}^{[l]}$ is matrix-valued with each element $w_{ij}^{[l]}\in\mathbb{R}$. The values of $w_{ij}^{[l]}$ are learned by the training process. The &lt;em&gt;dimensions&lt;/em&gt; of $\mathbf{W}^{[l]}$ are $n^{[l]} \times n^{[l-1]}$ where $n^{[l-1]}$ is the &lt;em&gt;number of input features&lt;/em&gt; (also the number of output features in the previous layer), and $n^{[l]}$ is the &lt;em&gt;number of output features&lt;/em&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The &lt;em&gt;activations&lt;/em&gt; in layer $l$ are represented by a vector $\mathbf{a}^{[l]}$. For each layer, we distinguish between the &lt;em&gt;input&lt;/em&gt; activations, $\mathbf{a}^{[l-1]}$, and &lt;em&gt;output&lt;/em&gt; activations, $\mathbf{a}^{[l]}$. The dimension of $\mathbf{a}^{[l-1]}$ is $n^{[l-1]}$ where $n^{[l-1]}$ is the number of neurons or hidden units in the previous layer. The dimension of $\mathbf{a}^{[l]}$ is $n^{[l]}$ where $n^{[l]}$ is the number of units in the current layer.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;There is no concept of padding nor of stride. The &lt;em&gt;bias&lt;/em&gt; vector in layer $l$ is indicated by $\mathbf{b}^{[l]}$. Equation 7 describes the &lt;em&gt;linear&lt;/em&gt; part of the filter. The &lt;em&gt;activation&lt;/em&gt; part is captured by Equation 8. The &lt;em&gt;activation function&lt;/em&gt;, $g^{[l]}$, is often a ReLU.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;As before, we can combine Equations 7 and 8 into a single equation (see Equation 9).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
$$ \Large
\mathbf{a}^{[l]} = g^{[l]}(\mathbf{W}^{[l]} \mathbf{a}^{[l-1]} + \mathbf{b}^{[l]}) \qquad\qquad\qquad\qquad\,(9)
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;If we make use of mini-batch training, the dimensions of $\mathbf{a}^{[l]}$ will expand to $n^{[l]} \times m$ where $m$ is the mini-batch size. In this case $\mathbf{a}^{[l]}$ becomes a matrix $\mathbf{A}^{[l]}$ so that we have&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
$$ \Large
\mathbf{A}^{[l]} = g^{[l]}(\mathbf{W}^{[l]} \mathbf{A}^{[l-1]} + \mathbf{b}^{[l]}) \qquad\qquad\qquad\qquad(10)
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h4 id=&quot;Residual-Networks&quot;&gt;Residual Networks&lt;a class=&quot;anchor-link&quot; href=&quot;#Residual-Networks&quot;&gt; &lt;/a&gt;&lt;/h4&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;He et al. (2016) provides an impressive and state-of-the-art architecture to improve the performance of CNNs even more. This architecture is called a &lt;em&gt;residual network&lt;/em&gt;, or a &lt;em&gt;ResNet&lt;/em&gt;. &lt;a href=&quot;https://towardsdatascience.com/review-resnet-winner-of-ilsvrc-2015-image-classification-localization-detection-e39402bfa5d8&quot;&gt;Sik-Ho Tsang&lt;/a&gt; (2018) has a blog that presents some of the details in a more digestible form.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;A ResNet layer or block is developed as follows. As before, for a convolution layer, we have:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
$$ \Large
\begin{align}
\mathbf{Z}^{[l]} &amp;amp;= \mathbf{W}^{[l]} \ast \mathbf{A}^{[l-1]} + \mathbf{b}^{[l]} \\
\mathbf{A}^{[l]} &amp;amp;= g^{[l]}(\mathbf{Z}^{[l]})
\end{align}
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Let’s add 1 to the index values for the purpose of deriving:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
$$ \Large
\begin{align}
\mathbf{Z}^{[l+1]} &amp;amp;= \mathbf{W}^{[l+1]} \ast \mathbf{A}^{[l]} + \mathbf{b}^{[l+1]} \qquad(11) \\
\mathbf{A}^{[l+1]} &amp;amp;= g^{[l+1]}(\mathbf{Z}^{[l+1]}) \qquad\qquad\qquad\qquad\,(12)
\end{align}
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The next layer will then be:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
$$ \Large
\begin{align}
\mathbf{Z}^{[l+2]} &amp;amp;= \mathbf{W}^{[l+2]} \ast \mathbf{A}^{[l+1]} + \mathbf{b}^{[l+2]} \qquad(13) \\
\mathbf{A}^{[l+2]} &amp;amp;= g^{[l+2]}(\mathbf{Z}^{[l+2]}) \qquad\qquad\qquad\qquad\quad(14)
\end{align}
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Now comes the crucial step. We feed the activations $\mathbf{A}^{[l]}$ in Equation 11 &lt;em&gt;forward&lt;/em&gt; by means of a &lt;em&gt;skip-connection&lt;/em&gt; (also called a &lt;em&gt;short-circuit-connection&lt;/em&gt;) and add them to $\mathbf{Z}^{[l+2]}$ in Equation 13. This means Equation 14 now becomes:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
$$ \Large
\mathbf{A}^{[l+2]} = g^{[l+2]}(\mathbf{Z}^{[l+2]} + \mathbf{A}^{[l]})
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The ResNet block is therefore described by the equations:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
$$ \Large
\begin{align}
\mathbf{Z}^{[l+2]} &amp;amp;= \mathbf{W}^{[l+2]} \ast \mathbf{A}^{[l+1]} + \mathbf{b}^{[l+2]} \qquad(15) \\
\mathbf{A}^{[l+2]} &amp;amp;= g^{[l+2]}(\mathbf{Z}^{[l+2]} + \mathbf{A}^{[l]}) \qquad\qquad\quad(16)
\end{align}
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Expressed as a single equation we have:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
$$ \Large
\mathbf{A}^{[l+2]} = g^{[l+2]}(\mathbf{W}^{[l+2]} \ast \mathbf{A}^{[l+1]} + \mathbf{b}^{[l+2]} + \mathbf{A}^{[l]})\quad(17)
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Adjusting the indexes again, we have&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
$$ \Large
\mathbf{A}^{[l]} = g^{[l]}(\mathbf{W}^{[l]} \ast \mathbf{A}^{[l-1]} + \mathbf{b}^{[l]} + \mathbf{A}^{[l-2]})\qquad(18)
$$
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The ResNet architecture allows for much deeper networks than before by stacking ResNet blocks as deep as needed. In theory, as the number of layers in a traditional deep neural network increases, the training error should keep on decreasing. The reality is that when the network gets too deep, the training error actually starts to increase again. ResNets rescue this situation, allowing the training error to keep on falling even with the number of layers approaching one thousand.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Next we will discuss all the classification models used in this paper. All of them make use of a 50-layer ResNet architecture.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Regression-Model&quot;&gt;Regression Model&lt;a class=&quot;anchor-link&quot; href=&quot;#Regression-Model&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The input datapoints for this model are all the synthetic time-series images. In the software, this model is called &lt;em&gt;mod9&lt;/em&gt;. The purpose of the model is to predict the two time points at which changes occur.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The labeled training data appears in the file synthetic.csv:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ul&gt;
&lt;li&gt;dashlink_regression&lt;ul&gt;
&lt;li&gt;Tail_687_1&lt;ul&gt;
&lt;li&gt;png9&lt;ul&gt;
&lt;li&gt;synthetic.csv&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Figure 3 shows the first few records of synthetic.csv.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/tsr_fig3.png&quot; alt=&quot;Figure 3: First few records of synthetic.csv&quot; title=&quot;Figure 3: First few records of synthetic.csv&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The code for the model is present in Python notebook:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://nbviewer.jupyter.org/github/kobus78/dashlink_regression/blob/master/30_mod9_canon.ipynb&quot;&gt;30_mod9_canon.ipynb&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The model’s notebook performs the following steps:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ul&gt;
&lt;li&gt;Ingest data&lt;ul&gt;
&lt;li&gt;Form item list&lt;/li&gt;
&lt;li&gt;Form train and validation item lists&lt;ul&gt;
&lt;li&gt;Train list is 80% of data&lt;/li&gt;
&lt;li&gt;Validation list is 20% of data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Form label lists&lt;ul&gt;
&lt;li&gt;The labels come from synthetic.csv&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Normalize data using the ImageNet statistics (for transfer learning)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Train model&lt;ul&gt;
&lt;li&gt;Create a learner with the data and a ResNet-18 architecture&lt;/li&gt;
&lt;li&gt;Plot the Loss vs the Learning Rate that allows selection of the frozen learning rate by inspection &lt;/li&gt;
&lt;li&gt;Fit the learner’s model to the data using the frozen learning rate&lt;/li&gt;
&lt;li&gt;Plot the Train and Validation Loss vs the number of Batches&lt;/li&gt;
&lt;li&gt;Save the frozen model and iterate if necessary&lt;/li&gt;
&lt;li&gt;Unfreeze the model&lt;/li&gt;
&lt;li&gt;Plot the Loss vs the Learning Rate that allows selection of the unfrozen learning rate by inspection &lt;/li&gt;
&lt;li&gt;Fit the learner’s model to the data using the unfrozen learning rate and 10% of the frozen learning rate&lt;/li&gt;
&lt;li&gt;Plot the Train and Validation Loss vs the number of Batches&lt;/li&gt;
&lt;li&gt;Save the unfrozen model and iterate if necessary&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Test Inference/Production (on train data)&lt;ul&gt;
&lt;li&gt;Export the trained, unfrozen model&lt;/li&gt;
&lt;li&gt;Loop through all the data and submit each image to the trained model to predict the locations of the changes&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Figure 4 shows an example of predicted location of changes (green dotted lines).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/tsr_fig4.png&quot; alt=&quot;Figure 4: Example of predicted location of changes (green dotted lines)&quot; title=&quot;Figure 4: Example of predicted location of changes (green dotted lines)&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;This brings us to the end of the modeling section. We looked at the concept of time-series regression and two deep learning architectures that can be used for this.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Inference&quot;&gt;Inference&lt;a class=&quot;anchor-link&quot; href=&quot;#Inference&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We have finally come to the application of the models developed above. During inference (also called testing in this paper) the trained models are presented with previously unseen data points. This is the situation in production when the developed models are called upon to provide value for users.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Test-Dataset&quot;&gt;Test Dataset&lt;a class=&quot;anchor-link&quot; href=&quot;#Test-Dataset&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The dataset for testing/inference will be prepared in the same way as the dataset for training. Note that the datapoints are not labeled for inference. It is a simple matter to generate additional random data and submit it to the trained model and assess the performance. This work is still outstanding.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Conclusions-&amp;amp;-Recommendations&quot;&gt;Conclusions &amp;amp; Recommendations&lt;a class=&quot;anchor-link&quot; href=&quot;#Conclusions-&amp;amp;-Recommendations&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We have demonstrated how randomly generated time-series can be turned into images for a novel way of performing time-series regression. Using transfer learning, we showed how quickly a deep learning model could be trained.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We recommend that interested analysts take this work as a starting point and adapt it to suit their needs. This may even involve changing the technology stack, for example, making use of other deep learning libraries and a different programming language. Here we used the fastai Python library (built on top of PyTorch) and the Python language. There are a number of other useful technology environments, e.g. Java, TensorFlow, Julia, and MATLAB.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We suggest that analysts need not shy away from the use of deep learning for analyses that conventionally make use of traditional approaches. The use of transfer learning makes the training of deep learning models very tractable. In our case, transfer learning was based on the ImageNet model which was trained on over 14 million images to classify them into more than 20,000 categories. There are many cloud providers offering the use of GPUs (Graphical Processing Units), ideal for the training process. GPUs are not necessary for inference. Even without access to a GPU, the training process is still tractable on an ordinary laptop. This is the beauty of transfer learning.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Further-Experimentation&quot;&gt;Further Experimentation&lt;a class=&quot;anchor-link&quot; href=&quot;#Further-Experimentation&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;There are a good number of hyper-parameters that may be adjusted leading to further experiments, for example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fraction of train data dedicated for validation (20% here)&lt;/li&gt;
&lt;li&gt;The batch size during training&lt;/li&gt;
&lt;li&gt;A technique that holds promise is to first down-sample images drastically, say to 32 x 32. Then, after training, transfer learning is used while progressively up-sampling again.&lt;/li&gt;
&lt;li&gt;Learning rates. This is arguably the most influential hyper-parameter during training. It may be worthwhile to adjust the used learning rates, (both for the frozen learning rate, &lt;em&gt;lrf&lt;/em&gt;, as well as the unfrozen learning rate, &lt;em&gt;lru&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We used data that always contains exactly two changes. Future experiments can make the occurrance of change points more flexible.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Our synthetic data do not include noise. Future experiments can investigate the performance of the model in the presence of noise.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The CNN architecture used was ResNet-18. We believe ResNet is the current state-of-the-art but there are other promising architectures, i.e. the Inception network.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;REFERENCES&quot;&gt;REFERENCES&lt;a class=&quot;anchor-link&quot; href=&quot;#REFERENCES&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Culurciell, E. (2018). The fall of RNN / LSTM. [Weblog]. Retrieved from 
&lt;a href=&quot;https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0&quot;&gt;https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;He, K., Zhang, X., Ren, S., &amp;amp; Sun, J. (2016). Deep Residual Learning for Image Recognition. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, 2016, pp. 770-778. doi: 10.1109/CVPR.2016.90&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Hüsken, M., &amp;amp; Stagge, P. (2003). Recurrent neural networks for time series classification. Neurocomputing, 50, 223-235. Retrieved from &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S0925231201007068?via=ihub&quot;&gt;https://www.sciencedirect.com/science/article/pii/S0925231201007068?via=ihub&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Karpathy, A. (2015). The Unreasonable Effectiveness of Recurrent Neural Networks. [Weblog]. Retrieved from &lt;a href=&quot;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&quot;&gt;http://karpathy.github.io/2015/05/21/rnn-effectiveness/&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Sun, Z., Di, L. &amp;amp; Fang, H. (2019). Using long short-term memory recurrent neural network in land cover classification on Landsat and Cropland data layer time series. International Journal of Remote Sensing, 40(2), 593-614. DOI: 10.1080/01431161.2018.1516313. Retrieved from &lt;a href=&quot;https://www.tandfonline.com/doi/abs/10.1080/01431161.2018.1516313&quot;&gt;https://www.tandfonline.com/doi/abs/10.1080/01431161.2018.1516313&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Truong, C., Oudre, L., &amp;amp; Vayatis, N. (2020). Selective review of offline change point detection methods. Signal Processing, 167:107299, 2020.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Wang, Z., Yan, W., &amp;amp; Oates, T. (2017). Time series classification from scratch with deep neural networks: A strong baseline. 2017 International Joint Conference on Neural Networks (IJCNN), 1578-1585.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Wang, Z., &amp;amp; Oates, T. (2015a). Encoding Time Series as Images for Visual Inspection and Classification Using Tiled Convolutional Neural Networks. Trajectory-Based Behavior Analytics: Papers from the 2015 AAAI Workshop. Retrieved from &lt;a href=&quot;https://aaai.org/ocs/index.php/WS/AAAIW15/paper/viewFile/10179/10251&quot;&gt;https://aaai.org/ocs/index.php/WS/AAAIW15/paper/viewFile/10179/10251&lt;/a&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Wang, Z., &amp;amp; Oates, T. (2015b). Imaging time-series to improve classification and imputation. International Conference on Artificial Intelligence, pp 3939-3945.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Wang, Z., &amp;amp; Oates, T. (2015c). Spatially Encoding Temporal Correlations to Classify Temporal Data Using Convolutional Neural Networks. ArXiv, abs/1509.07481.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., … Bengio, Y. (2015). Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. Proceedings of the 32nd International Conference on Machine Learning, in PMLR, 37, 2048-2057.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><category term="Deep Learning" /><category term="Time series" /><category term="Image Regression" /><category term="CNN" /><category term="RNN" /><category term="Python" /><category term="fastai" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/portfolio/images/tsr_fig4.png" /><media:content medium="image" url="/portfolio/images/tsr_fig4.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Sentiment Analysis</title><link href="/portfolio/api/nlp/python/nltk/r/2017/04/15/SentimentAnalysis.html" rel="alternate" type="text/html" title="Sentiment Analysis" /><published>2017-04-15T00:00:00-05:00</published><updated>2017-04-15T00:00:00-05:00</updated><id>/portfolio/api/nlp/python/nltk/r/2017/04/15/SentimentAnalysis</id><content type="html" xml:base="/portfolio/api/nlp/python/nltk/r/2017/04/15/SentimentAnalysis.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2017-04-15-SentimentAnalysis.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;PURPOSE&quot;&gt;PURPOSE&lt;a class=&quot;anchor-link&quot; href=&quot;#PURPOSE&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;The purpose of this project is to demonstrate how social media data can be used to capture public sentiment.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;SUMMARY&quot;&gt;SUMMARY&lt;a class=&quot;anchor-link&quot; href=&quot;#SUMMARY&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;President Trump recently authorized a missile strike on Syria in response to an alleged chemical weapon attack by the president of Syria on members of his own population on April 4 2017. This led to the beginning of heated debates about the appropriateness and consequences of this drastic, largely unexpected, and seemingly impulsive act. The presidential election afforded Trump a popular vote of 46%. One might argue that the country’s approval of the missile strike might roughly align with this partition given the recentness of the election. This investigation is about whether this is a valid assumption. My prediction is that the missile strike approval partition will be &lt;em&gt;different&lt;/em&gt; from the popular vote partition. This is an interesting question for the American public as this incident of military action represents a dramatic change in direction relative to the president’s campaign stance in this regard.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The &lt;em&gt;Twitter Streaming API&lt;/em&gt; was used to collect 5,000 tweets on April 11 2017, 4 days after the missile strike. It was important to collect this information as soon as possible after the event. Only tweets that contain &lt;em&gt;all&lt;/em&gt; of the words 'trump syria strike' were collected (the word order did not matter). So-called &lt;em&gt;retweets&lt;/em&gt; were also collected, as these were taken to indicate that the person sending the retweet agrees with the originator of the tweet. &lt;em&gt;Sentiment analysis&lt;/em&gt; techniques were used. Features were extracted from the tweets in the form of words that convey &lt;em&gt;positive&lt;/em&gt; sentiment (interpreted as &lt;em&gt;approval&lt;/em&gt; for the strike) or &lt;em&gt;negative&lt;/em&gt; sentiment (interpreted as &lt;em&gt;disapproval&lt;/em&gt;). A &lt;em&gt;classifier&lt;/em&gt; was then trained (using a different &lt;em&gt;labeled&lt;/em&gt; data set of 100,000 tweets) to associate patterns of words with the appropriate sentiment. After training, the classifier was used to estimate the sentiment of each of the 5,000 collected tweets on the Syria missile strike. The accuracy of the classifier was found to be 75% (which means it got 75% of classifications correct on the test data).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The analysis found that, out of the 4,999 usable tweets that were collected, 2,918 were estimated to have &lt;em&gt;positive&lt;/em&gt; sentiment. These were interpreted as conveying &lt;em&gt;approval&lt;/em&gt; for the Syria missile strike. This number represents a 58% approval rating. Compare this with the 46% popularity rating during the presidential election (see the blue columns in Figure 1). Using a statistical proportion test, the assumption that the strike approval rating would align with the popular vote partition, was &lt;em&gt;rejected&lt;/em&gt;. Strong evidence was found, therefore, that there is a clear willingness on the part of the American public to support a dramatic change of course undertaken by the president relative to his campaign stance on matters of this sort.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/portfolio/images/copied_from_nb/../images/sa_fig1.png&quot; alt=&quot;&quot; title=&quot;Figure 1 President Trump&amp;#39;s Syria Strike Approval&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;CODE&quot;&gt;CODE&lt;a class=&quot;anchor-link&quot; href=&quot;#CODE&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Part-1.-Capture-training-data-and-train-a-sentiment-classifier&quot;&gt;Part 1. Capture training data and train a sentiment classifier&lt;a class=&quot;anchor-link&quot; href=&quot;#Part-1.-Capture-training-data-and-train-a-sentiment-classifier&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;IMPORTANT-NOTE:&quot;&gt;IMPORTANT NOTE:&lt;a class=&quot;anchor-link&quot; href=&quot;#IMPORTANT-NOTE:&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Please note that the code of a few cells have been commented out. This was done to prevent accidental triggering of longer-running pieces of code (e.g. it takes about 10 minutes to train the classifier). If desired, it is easy to remove the comments to verify this code.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;A Naive Bayes classifier will be trained on a labeled data set. This data set will also come from a body of tweets to ensure similarity to the style of text associated with tweets (i.e. to-the-point-ness, conciseness, use of exclamation marks, emoticons, etc.). An example of such a labeled data set is &lt;a href=&quot;http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/&quot;&gt;http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/&lt;/a&gt;. I have selected the first 100,000 rows of this data set for training.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nltk&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pickle&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pd&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;1.1-Read-in-and-prepare-training-data&quot;&gt;1.1 Read in and prepare training data&lt;a class=&quot;anchor-link&quot; href=&quot;#1.1-Read-in-and-prepare-training-data&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filetext&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;SentimentAnalysisDataset.csv&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;filetext&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text_file&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;filetext&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filetext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&amp;quot;&amp;quot; Brokeback Mountain &amp;quot;&amp;quot;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;Brokeback Mountain&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;filetext&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filetext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&amp;quot; Brokeback Mountain &amp;quot;&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;Brokeback Mountain&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;filetext&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filetext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&amp;quot; brokeback mountain was terrible.&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;brokeback mountain was terrible.&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;SentimentAnalysisDataset_fixed.csv&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;w&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filetext&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;del&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filetext&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Note that a positive sentiment is indicated by a &amp;#39;1&amp;#39;, and a negative sentiment by a &amp;#39;0&amp;#39; in the &amp;#39;Sentiment&amp;#39; column&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;SentimentAnalysisDataset_fixed.csv&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nrows&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;splitpoint&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;80000&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#train/test: 80%/20%&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;drop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;SentimentSource&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;


&lt;div class=&quot;output_html rendered_html output_subarea output_execute_result&quot;&gt;
&lt;div&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;ItemID&lt;/th&gt;
      &lt;th&gt;Sentiment&lt;/th&gt;
      &lt;th&gt;SentimentText&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;is so sad for my APL frie...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;I missed the New Moon trail...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;omg its already 7:30 :O&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;.. Omgaga. Im sooo  im gunna CRy. I'...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;i think mi bf is cheating on me!!!   ...&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;word_tokenize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tweet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;Breaks a string up into words.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    This function uses the NLTK package&amp;#39;s word_tokenize function to split a string up&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    into word tokens.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    &lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    Parameters&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    ----------&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    tweet : str&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        The text of a tweet.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    &lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    Returns&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    -------&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    list&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        A list consisting of the words in the input tweet string.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nltk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word_tokenize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tweet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;1.2-Gather-all-words-in-a-bag-and-form-tuples-of-(tokenized-tweets,-sentiment)&quot;&gt;1.2 Gather all words in a bag and form tuples of (tokenized-tweets, sentiment)&lt;a class=&quot;anchor-link&quot; href=&quot;#1.2-Gather-all-words-in-a-bag-and-form-tuples-of-(tokenized-tweets,-sentiment)&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tqdm&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tqdm&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#for progress bar&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;all_words&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tweet_tuples&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tqdm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iterrows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tweet_words&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word_tokenize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;SentimentText&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tweet_words&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lower&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tweet_words&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tweet_tuples&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tweet_words&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Sentiment&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tweet_words&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;all_words&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Number of words: &amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;all_words&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stderr output_text&quot;&gt;
&lt;pre&gt;100000it [00:34, 2870.05it/s]&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Number of words:  1719434
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stderr output_text&quot;&gt;
&lt;pre&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;del&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;del&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;del&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;del&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tweet_words&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;del&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;1.3-Get-frequency-distribution-of-the-words&quot;&gt;1.3 Get frequency distribution of the words&lt;a class=&quot;anchor-link&quot; href=&quot;#1.3-Get-frequency-distribution-of-the-words&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;all_words&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nltk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FreqDist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;all_words&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# inspect the frequency distribution of the words&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;The 30 most common words are: &amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;all_words&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;most_common&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;all_words&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cumulative&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;The 30 most common words are:  [(&amp;#39;@&amp;#39;, 90794), (&amp;#39;i&amp;#39;, 62578), (&amp;#39;!&amp;#39;, 57937), (&amp;#39;.&amp;#39;, 51116), (&amp;#39;,&amp;#39;, 32555), (&amp;#39;the&amp;#39;, 29914), (&amp;#39;you&amp;#39;, 29844), (&amp;#39;to&amp;#39;, 29561), (&amp;#39;?&amp;#39;, 25284), (&amp;#39;a&amp;#39;, 22062), (&amp;#39;it&amp;#39;, 22041), (&amp;#39;...&amp;#39;, 18448), (&amp;#39;and&amp;#39;, 16303), (&amp;#39;;&amp;#39;, 16178), (&amp;#39;&amp;amp;&amp;#39;, 14440), (&amp;#39;my&amp;#39;, 13790), (&amp;#39;that&amp;#39;, 13587), (&amp;#39;for&amp;#39;, 12553), (&amp;#39;is&amp;#39;, 12534), (&amp;#34;&amp;#39;s&amp;#34;, 11912), (&amp;#39;in&amp;#39;, 11822), (&amp;#34;n&amp;#39;t&amp;#34;, 11791), (&amp;#39;me&amp;#39;, 11025), (&amp;#39;of&amp;#39;, 10606), (&amp;#39;have&amp;#39;, 10531), (&amp;#39;on&amp;#39;, 9618), (&amp;#39;quot&amp;#39;, 9153), (&amp;#39;so&amp;#39;, 9106), (&amp;#39;but&amp;#39;, 8956), (&amp;#34;&amp;#39;m&amp;#34;, 8500)]
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_png output_subarea &quot;&gt;
&lt;img src=&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAjQAAAGNCAYAAAAGpusjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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&quot; /&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;1.4-Select-the-top-5000-most-common-words-as-features&quot;&gt;1.4 Select the top 5000 most common words as features&lt;a class=&quot;anchor-link&quot; href=&quot;#1.4-Select-the-top-5000-most-common-words-as-features&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;freq_tuples&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;all_words&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;most_common&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;1.5-Remove-the-30-most-common-words-again&quot;&gt;1.5 Remove the 30 most common words again&lt;a class=&quot;anchor-link&quot; href=&quot;#1.5-Remove-the-30-most-common-words-again&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;The most common words often have less 'signal' in them and may be ignored.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;freq_tuples&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;freq_tuples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#remove the 30 most common words&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;1.6-Prepare-the-word-features&quot;&gt;1.6 Prepare the word features&lt;a class=&quot;anchor-link&quot; href=&quot;#1.6-Prepare-the-word-features&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word_features&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;freq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;freq_tuples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;1.7-Save-the-word_features&quot;&gt;1.7 Save the word_features&lt;a class=&quot;anchor-link&quot; href=&quot;#1.7-Save-the-word_features&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;word_features.pkl&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;wb&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pickle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dump&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;del&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;all_words&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;del&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;freq_tuples&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sentiment_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tweet_words&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;Extract sentiment features from a tweet.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    This function extracts sentiment features from the words of a tweet.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    &lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    Parameters&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    ----------&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    tweet_words : list of words&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        The text of a tweet, tokenized into words.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    &lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    Returns&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    -------&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    dict&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        A dictionary that has a key for each word in the list of word features (word_features).&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        The value of each entry indicates whether that specific word is present in this tweet.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;uniq_words&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tweet_words&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#keep unique words only&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;features&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;uniq_words&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;1.8-Prepare-featuresets-for-the-complete-classifier-data-set&quot;&gt;1.8 Prepare featuresets for the complete classifier data set&lt;a class=&quot;anchor-link&quot; href=&quot;#1.8-Prepare-featuresets-for-the-complete-classifier-data-set&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tqdm&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tqdm&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;featuresets&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sentiment_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tweet_words&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sentiment&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tweet_words&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sentiment&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tqdm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tweet_tuples&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Number of featuresets: &amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;featuresets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stderr output_text&quot;&gt;
&lt;pre&gt;100%|██████████| 100000/100000 [01:29&amp;lt;00:00, 1119.71it/s]&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Number of featuresets:  100000
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stderr output_text&quot;&gt;
&lt;pre&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;del&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tweet_tuples&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;1.9-Split-the-data-set-for-training-and-testing&quot;&gt;1.9 Split the data set for training and testing&lt;a class=&quot;anchor-link&quot; href=&quot;#1.9-Split-the-data-set-for-training-and-testing&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_set&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;featuresets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;splitpoint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;testing_set&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;featuresets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;splitpoint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Length of featuresets: &amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;featuresets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Length of training_set: &amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Length of testing_set: &amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;testing_set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Length of featuresets:  100000
Length of training_set:  80000
Length of testing_set:  20000
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;del&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;featuresets&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;1.10-Train-classifier&quot;&gt;1.10 Train classifier&lt;a class=&quot;anchor-link&quot; href=&quot;#1.10-Train-classifier&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;commented out to protect&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;classifier = nltk.NaiveBayesClassifier.train(tqdm(training_set))&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stderr output_text&quot;&gt;
&lt;pre&gt;100%|██████████| 80000/80000 [09:46&amp;lt;00:00, 136.45it/s]
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;1.11-Score-classifier&quot;&gt;1.11 Score classifier&lt;a class=&quot;anchor-link&quot; href=&quot;#1.11-Score-classifier&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;commented out to protect&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;accuracy = nltk.classify.accuracy(classifier, testing_set)&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;print(&amp;#39;Accuracy: &amp;#39;, accuracy)&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Accuracy:  0.75205
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;commented out to protect&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;classifier.show_most_informative_features(40)&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Most Informative Features
               cancelled = True                0 : 1      =     29.0 : 1.0
                   ak618 = True                0 : 1      =     29.0 : 1.0
             appreciated = True                1 : 0      =     26.2 : 1.0
            aaroncarter7 = True                0 : 1      =     24.0 : 1.0
                 *cries* = True                0 : 1      =     24.0 : 1.0
                 hurting = True                0 : 1      =     21.4 : 1.0
                    grrr = True                0 : 1      =     21.4 : 1.0
                     sad = True                0 : 1      =     21.1 : 1.0
                canceled = True                0 : 1      =     19.8 : 1.0
             musicmonday = True                1 : 0      =     19.7 : 1.0
          recommendation = True                1 : 0      =     18.2 : 1.0
                 sadness = True                0 : 1      =     18.1 : 1.0
                  gutted = True                0 : 1      =     17.5 : 1.0
                 blessed = True                1 : 0      =     16.7 : 1.0
                 swollen = True                0 : 1      =     16.4 : 1.0
            followfriday = True                1 : 0      =     16.2 : 1.0
           disappointing = True                0 : 1      =     15.6 : 1.0
                   r.i.p = True                0 : 1      =     15.6 : 1.0
                 *sniff* = True                0 : 1      =     15.6 : 1.0
                    booo = True                0 : 1      =     15.4 : 1.0
               3hotwords = True                1 : 0      =     15.1 : 1.0
                   sadly = True                0 : 1      =     14.4 : 1.0
                   sucks = True                0 : 1      =     14.2 : 1.0
                  bummed = True                0 : 1      =     13.9 : 1.0
             frustrating = True                0 : 1      =     13.9 : 1.0
                 surgery = True                0 : 1      =     13.9 : 1.0
          noundiessunday = True                1 : 0      =     13.5 : 1.0
                    argh = True                0 : 1      =     13.3 : 1.0
                  bummer = True                0 : 1      =     12.7 : 1.0
                   blows = True                0 : 1      =     12.4 : 1.0
                   ughhh = True                0 : 1      =     12.2 : 1.0
         congratulations = True                1 : 0      =     11.9 : 1.0
                     sux = True                0 : 1      =     11.7 : 1.0
                downside = True                0 : 1      =     11.4 : 1.0
              depressing = True                0 : 1      =     11.4 : 1.0
                    leak = True                0 : 1      =     11.4 : 1.0
               emotional = True                0 : 1      =     11.4 : 1.0
                      /3 = True                0 : 1      =     11.4 : 1.0
                  cramps = True                0 : 1      =     11.4 : 1.0
                    poor = True                0 : 1      =     11.2 : 1.0
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;1.12-Define-a-function-to-estimate-the-sentiment-of-a-tweet&quot;&gt;1.12 Define a function to estimate the sentiment of a tweet&lt;a class=&quot;anchor-link&quot; href=&quot;#1.12-Define-a-function-to-estimate-the-sentiment-of-a-tweet&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;estimate_sentiment&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tweet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;Estimate the sentiment of a tweet.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    This function estimates the sentiment of a tweet.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    &lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    Parameters&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    ----------&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    tweet : str&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        The text of a tweet.&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    &lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    Returns&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    -------&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    int&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        A value that indicates the sentiment of the tweet: &lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        1 for positive, and&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;        0 for negative&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;classifier&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;classify&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sentiment_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word_tokenize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tweet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;1.13-Hand-test-classifier&quot;&gt;1.13 Hand-test classifier&lt;a class=&quot;anchor-link&quot; href=&quot;#1.13-Hand-test-classifier&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;estimate_sentiment&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;I feel fine today!&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;estimate_sentiment&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;I feel horrible today!&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;estimate_sentiment&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;This is good news.&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;estimate_sentiment&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;This was a mistake&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;estimate_sentiment&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;He had to do it&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;estimate_sentiment&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Not fair to Syria&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;1
0
1
0
1
0
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;1.14-Save-classifier&quot;&gt;1.14 Save classifier&lt;a class=&quot;anchor-link&quot; href=&quot;#1.14-Save-classifier&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;commented out to protect&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;f = open(&amp;#39;tweepy_naivebayes.pkl&amp;#39;, &amp;#39;wb&amp;#39;)&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;pickle.dump(classifier, f)&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;f.close()&lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Part-2.-Capture-twitter-data-on-Trump-Syria-Strike&quot;&gt;Part 2. Capture twitter data on Trump Syria Strike&lt;a class=&quot;anchor-link&quot; href=&quot;#Part-2.-Capture-twitter-data-on-Trump-Syria-Strike&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tweepy&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;time&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;datetime&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;file_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;trump_syria_strike15.txt&amp;#39;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;track_string&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;trump syria strike&amp;#39;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;WritingTxtStreamListener&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tweepy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;StreamListener&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;sd&quot;&gt;&amp;quot;&amp;quot;&amp;quot;Sets up and captures a stream of tweets.    &lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    This class sets up and captures a stream of tweets from the Twitter Streaming API.    &lt;/span&gt;
&lt;span class=&quot;sd&quot;&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;__count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;#captures the json of a tweet&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;on_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;file_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;a&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__count&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
            &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;now&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;: &amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;except&lt;/span&gt; &lt;span class=&quot;ne&quot;&gt;BaseException&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;ERROR: &amp;#39;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sleep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;#disconnect the stream if we receive an error message indicating we are overloading Twitter&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;on_error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;status_code&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;ERROR: status_code = &amp;#39;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;status_code&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;status_code&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;420&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;#returning False in on_data disconnects the stream&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;False&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;con_key&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;con_secret&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;acc_token&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;acc_secret&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;auth&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tweepy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;OAuthHandler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;consumer_key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;con_key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;consumer_secret&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;con_secret&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;auth&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_access_token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;acc_token&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;acc_secret&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;api&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tweepy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;API&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;auth&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wait_on_rate_limit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wait_on_rate_limit_notify&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;retry_count&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;retry_delay&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;retry_errors&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;         
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;my_stream_listener&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;WritingTxtStreamListener&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;my_stream&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tweepy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Stream&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;auth&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;api&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;auth&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;listener&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;my_stream_listener&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;my_stream&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;track&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;track_string&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;languages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;en&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;my_stream&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;disconnect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;3.-Analyze-twitter-data-on-Trump-Syria-Strike&quot;&gt;3. Analyze twitter data on Trump Syria Strike&lt;a class=&quot;anchor-link&quot; href=&quot;#3.-Analyze-twitter-data-on-Trump-Syria-Strike&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;json&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pd&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;3.1-Load-the-json-study-data&quot;&gt;3.1 Load the json study data&lt;a class=&quot;anchor-link&quot; href=&quot;#3.1-Load-the-json-study-data&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tweets_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tweets_file&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;trump_syria_strike15.txt&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;r&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tweets_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;tweet_json&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;tweets_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tweet_json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;except&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;continue&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tweets_file&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;Number of tweets: &amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tweets_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Number of tweets:  5000
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tweets&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tweets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;text&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;text&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tweets_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tweets&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;


&lt;div class=&quot;output_html rendered_html output_subarea output_execute_result&quot;&gt;
&lt;div&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;text&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;RT @WilliamAder: Most daughters would be happy...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;RT @TheEconomist: Listen: We discuss what Dona...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;RT @davebernstein: Translation:\n\nIntent of S...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;RT @JoeMyGod: Ann Coulter Denounces Trump Over...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;RT @ddale8: Can you even imagine if a Hillary ...&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;re&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;suppress_newlines&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tweet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;re&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sub&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;\n&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tweet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tweets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;text&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tweets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;text&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;apply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;suppress_newlines&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;3.2-Load-the-sentiment-classifier&quot;&gt;3.2 Load the sentiment classifier&lt;a class=&quot;anchor-link&quot; href=&quot;#3.2-Load-the-sentiment-classifier&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pickle&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;tweepy_naivebayes.pkl&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;rb&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;classifier&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pickle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;3.3-Load-the-word-features-(from-word_features)&quot;&gt;3.3 Load the word features (from word_features)&lt;a class=&quot;anchor-link&quot; href=&quot;#3.3-Load-the-word-features-(from-word_features)&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;word_features.pkl&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;rb&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;word_features&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pickle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;3.4-Estimate-the-sentiment-for-each-tweet-in-the-data-on-Trump-Syria-Strike&quot;&gt;3.4 Estimate the sentiment for each tweet in the data on Trump Syria Strike&lt;a class=&quot;anchor-link&quot; href=&quot;#3.4-Estimate-the-sentiment-for-each-tweet-in-the-data-on-Trump-Syria-Strike&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;#from tqdm import tqdm, tqdm_pandas&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#tqdm_pandas(tqdm())&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tweets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;est_sentiment&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#tweets[&amp;#39;est_sentiment&amp;#39;] = tweets[&amp;#39;text&amp;#39;].progress_apply(estimate_sentiment)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tweets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;est_sentiment&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tweets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;text&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;apply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;estimate_sentiment&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tweets&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;


&lt;div class=&quot;output_html rendered_html output_subarea output_execute_result&quot;&gt;
&lt;div&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;text&lt;/th&gt;
      &lt;th&gt;est_sentiment&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;RT @WilliamAder: Most daughters would be happy...&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;RT @TheEconomist: Listen: We discuss what Dona...&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;RT @davebernstein: Translation:  Intent of Syr...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;RT @JoeMyGod: Ann Coulter Denounces Trump Over...&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;RT @ddale8: Can you even imagine if a Hillary ...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;3.5-Save-analysis-in-updated-dataframe&quot;&gt;3.5 Save analysis in updated dataframe&lt;a class=&quot;anchor-link&quot; href=&quot;#3.5-Save-analysis-in-updated-dataframe&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tweets&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;trump_syria_strike15.csv&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;utf-8&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sep&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;|&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;4.-Statistical-proportion-test&quot;&gt;4. Statistical proportion test&lt;a class=&quot;anchor-link&quot; href=&quot;#4.-Statistical-proportion-test&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;h3 id=&quot;(R-Language)&quot;&gt;(R Language)&lt;a class=&quot;anchor-link&quot; href=&quot;#(R-Language)&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## Load sentiment data on Trump Syria Strike&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;read.csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;trump_syria_strike15.csv&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sep&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;|&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stringsAsFactors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;FALSE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;head&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## Count the tweets with positive sentiment, disregard a handful of rows that have NAs&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;total.evals&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;complete.cases&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;total.evals&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;total.approvals&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;est_sentiment&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;na.rm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;total.approvals&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fraction.approvals&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total.approvals&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total.evals&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fraction.approvals&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## Perform a hypothesis test of proportions&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;### Setup hypotheses&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;The&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hypotheses&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;will&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;be&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;H0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;H1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p0&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;where&lt;/span&gt; 

&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;true&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;proportion&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;American&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;that&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;approves&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;strike&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0.46&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;.&lt;/span&gt; 

&lt;span class=&quot;n&quot;&gt;The&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hypothesis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;therefore&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;states&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;that&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;proportion&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;that&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;approves&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;strike &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;associated&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;positive&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sentiment&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tweets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;same&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;proportion&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;popular&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;votes &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;46&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%) during the presidential election. &lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;The *alternative* hypothesis, states that the proportion of the public that approves the strike is *different* from the proportion of popular votes. The type of hypothesis test will be a *one-sample test for a proportion*. The test will be *two-sided*.&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;### Perform hypothesis test&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;prop.test(total.approvals, n = total.evals, p = 0.46, alternative = &amp;#39;two.sided&amp;#39;)&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;### Conclusion&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;The p-value is much smaller than 0.05. At the 0.05 significance level, we *reject* the null hypothesis. There is enough evidence to claim that the proportion of the American public that *approves* the missile strike is *different* from 46%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;The&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;proportion&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;that&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;approves&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;strike&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;58&lt;/span&gt;%&lt;span class=&quot;n&quot;&gt;.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name></name></author><category term="API" /><category term="NLP" /><category term="Python" /><category term="nltk" /><category term="R" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/portfolio/images/sa_fig1.png" /><media:content medium="image" url="/portfolio/images/sa_fig1.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>